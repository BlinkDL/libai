
RWKV_HEAD_QK_DIM 256

RWKV_HEAD_QK_DIM 256


[32m[08/24 03:15:58 libai]: [0mRank of current process: 0. World size: 2
[32m[08/24 03:15:58 libai]: [0mCommand line arguments: Namespace(config_file='projects/RWKV_v4/configs/config_test.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[32m[08/24 03:15:58 libai]: [0mContents of args.config_file=projects/RWKV_v4/configs/config_test.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15momegaconf[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mOmegaConf[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mtokenizer[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mGPT2Tokenizer[39m
[38;5;242m# 配置 dataloader `build_image_train_loader` 和 `build_image_test_loader` 是 LiBai 提供的用于创建图像数据的训练集和测试集 DataLoader 的两个函数[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbuild[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mbuild_nlp_test_loader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mbuild_nlp_train_loader[39m
[38;5;197mimport[39m[38;5;15m [39m[38;5;15moneflow[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mflow[39m

[38;5;242m# 配置 model[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mprojects[39m[38;5;15m.[39m[38;5;15mRWKV_v4[39m[38;5;15m.[39m[38;5;15mmodeling[39m[38;5;15m.[39m[38;5;15mmodel[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mGPT[39m[38;5;15m [39m[38;5;15m,[39m[38;5;15mGPTConfig[39m
[38;5;242m# 导入自定义的 dataset[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mprojects[39m[38;5;15m.[39m[38;5;15mRWKV_v4[39m[38;5;15m.[39m[38;5;15mdataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mRWKVDataset[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mprojects[39m[38;5;15m.[39m[38;5;15mRWKV_v4[39m[38;5;15m.[39m[38;5;15mutils[39m[38;5;15m.[39m[38;5;15mconfig_optimizer[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mget_RWKV_v4_config_optim[39m


[38;5;15mtest[39m[38;5;197m=[39m[38;5;15mOmegaConf[39m[38;5;197m.[39m[38;5;15mcreate[39m[38;5;15m([39m[38;5;15m)[39m
[38;5;15mtest[39m[38;5;197m.[39m[38;5;15menable[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15mtest[39m[38;5;197m.[39m[38;5;15mweight_style[39m[38;5;197m=[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;186m"[39m[38;5;186mpytorch[39m[38;5;186m"[39m
[38;5;15m)[39m
[38;5;15mtest[39m[38;5;197m.[39m[38;5;15mpath[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186m/home/zhangxiaoyu/RWKV-LM/RWKV-v4/for_load.pth[39m[38;5;186m"[39m

[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/models/graph.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mgraph[39m

[38;5;15mgraph[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m
[38;5;15mgraph[39m[38;5;197m.[39m[38;5;15mdebug[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;242m# optim = get_config("common/optim.py").optim[39m
[38;5;15moptim[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mflow[39m[38;5;197m.[39m[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mAdam[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mparams[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mget_RWKV_v4_config_optim[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mlr[39m[38;5;197m=[39m[38;5;141m8e-4[39m[38;5;15m,[39m
[38;5;15m)[39m


[38;5;242m# 配置model[39m
[38;5;15mmodel[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mGPT[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mvocab_size[39m[38;5;197m=[39m[38;5;141m6064[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mctx_len[39m[38;5;197m=[39m[38;5;141m1024[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmodel_type[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mRWKV[39m[38;5;186m'[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mn_layer[39m[38;5;197m=[39m[38;5;141m6[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mn_embd[39m[38;5;197m=[39m[38;5;141m1024[39m
[38;5;15m)[39m

[38;5;242m# 训练过程[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mtrain[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minput_placement_device[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcpu[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mflow[39m[38;5;197m.[39m[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr_scheduler[39m[38;5;197m.[39m[38;5;15mStepLR[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mstep_size[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m[38;5;15m [39m
[38;5;15m        [39m[38;5;15mgamma[39m[38;5;197m=[39m[38;5;141m1.0[39m
[38;5;15m)[39m[38;5;15m [39m

[38;5;242m# false = fp32[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m

[38;5;15mdatafile[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186m/home/zhangxiaoyu/RWKV-LM/data/enwik8[39m[38;5;186m"[39m
[38;5;242m# 获得一个 DataLoader 的配置对象[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mOmegaConf[39m[38;5;197m.[39m[38;5;15mcreate[39m[38;5;15m([39m[38;5;15m)[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mbuild_nlp_train_loader[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mdataset[39m[38;5;197m=[39m[38;5;15m[[39m
[38;5;15m        [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mRWKVDataset[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m            [39m[38;5;15mdata_dir[39m[38;5;197m=[39m[38;5;15mdatafile[39m[38;5;15m,[39m
[38;5;15m            [39m[38;5;15mctx_len[39m[38;5;197m=[39m[38;5;141m1024[39m[38;5;15m,[39m
[38;5;15m            [39m[38;5;15mepoch_length_fixed[39m[38;5;197m=[39m[38;5;141m9996[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m][39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_workers[39m[38;5;197m=[39m[38;5;141m4[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# train.train_iter=3[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;197m=[39m[38;5;141m1[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186moutput/rwkv_output_loss_compare[39m[38;5;186m"[39m
[38;5;242m# train.load_weight = "/home/zhangxiaoyu/RWKV-LM/libai/projects/RWKV_v4/model/output_model/" # 采用同一个model进行初始化[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mrdma_enabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m

[38;5;242m# model.cfg.hidden_dropout_prob= 0.0 # 关闭所有的dropout[39m
[38;5;242m# model.cfg.attention_probs_dropout_prob= 0.0[39m
[38;5;242m# model.cfg.bias_dropout_fusion= False[39m

[38;5;242m# train.dist.pipeline_parallel_size=2[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mevaluation[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m

[38;5;242m# train.train_iter=5[39m
[38;5;242m# train.dist.tensor_parallel_size = 2  # 并行度为 4 的模型并行[39m
[38;5;242m# train.dist.tensor_parallel_size = 4  # 并行度为 4 的模型并行[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mactivation_checkpoint[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m [39m

[32m[08/24 03:15:58 libai]: [0mFull config saved to output/rwkv_output_loss_compare/config.yaml
[32m[08/24 03:15:58 lb.engine.default]: [0m> compiling dataset index builder ...
make: Entering directory '/home/zhangxiaoyu/libai/libai/data/data_utils'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/zhangxiaoyu/libai/libai/data/data_utils'
[32m[08/24 03:15:58 lb.engine.default]: [0m>>> done with dataset index builder. Compilation time: 0.052 seconds
[32m[08/24 03:15:58 lb.engine.default]: [0m>>> done with compiling. Compilation time: 0.054 seconds
[32m[08/24 03:15:58 lb.engine.default]: [0mPrepare training, validating, testing set
building token list... building token list... data has 99621832 tokens, 6064 unique.
data has 99621832 tokens, 6064 unique.
[32m[08/24 03:16:00 lb.engine.default]: [0mAuto-scaling the config to train.train_iter=10000, train.warmup_iter=0
[32m[08/24 03:16:04 lb.engine.default]: [0mModel:
GPT(
  (emb): VocabEmbedding(num_embeddings=6064, embedding_dim=1024)
  (blocks): Sequential(
    (0): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (1): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (2): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (3): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (4): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (5): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
  )
  (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=1024, out_features=6064, bias=False, parallel=row)
  (head_q): Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)
  (head_k): Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)
)
[32m[08/24 03:16:04 lb.engine.default]: [0mGraph debug mode on, automatically output debug info.
[32m[08/24 03:16:04 lb.engine.default]: [0mGraph debug mode on, automatically output debug info.
[32m[08/24 03:16:05 lb.engine.trainer]: [0mStarting training from iteration 0
(GRAPH:GraphBase_0:GraphBase) start building graph.
(GRAPH:GraphBase_0:GraphBase) start building graph builders of parameters and buffers.
(GRAPH:GraphBase_0:GraphBase) end building graph builders of parameters and buffers.
(GRAPH:GraphBase_0:GraphBase) start building graph inputs.
I20220824 03:16:05.794450 1458836 lazy_op_interpreter.cpp:403] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "_GraphBase_0_input.1.0_idx"
device_tag: "cpu"
scope_symbol_id: 131
input_conf {
  out: "out"
  blob_conf {
    shape {
      dim: 8
      dim: 1024
    }
    data_type: kInt64
    is_dynamic: false
    nd_sbp {
      sbp_parallel {
        split_parallel {
          axis: 0
        }
      }
    }
  }
}

I20220824 03:16:05.794909 1458836 lazy_op_interpreter.cpp:406] Lazy nn.Graph name GraphBase_0 add op : 
_GraphBase_0_input.1.0_idx
(INPUT:_GraphBase_0_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
I20220824 03:16:05.795282 1458836 lazy_op_interpreter.cpp:403] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "_GraphBase_0_input.1.1_targets"
device_tag: "cpu"
scope_symbol_id: 131
input_conf {
  out: "out"
  blob_conf {
    shape {
      dim: 8
      dim: 1024
    }
    data_type: kInt64
    is_dynamic: false
    nd_sbp {
      sbp_parallel {
        split_parallel {
          axis: 0
        }
      }
    }
  }
}

I20220824 03:16:05.795433 1458836 lazy_op_interpreter.cpp:406] Lazy nn.Graph name GraphBase_0 add op : 
_GraphBase_0_input.1.1_targets
(INPUT:_GraphBase_0_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
(GRAPH:GraphBase_0:GraphBase) end building graph inputs.
(GRAPH:GraphBase_0:GraphBase) start building graph modules.
[32m[08/24 03:16:05 lb.models.utils.graph_base]: [0mStart compling the train graph which may take some time. Please wait for a moment ...
(MODULE:model:GPT())
(INPUT:_model_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
       dtype=oneflow.int64))
(INPUT:_model_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
       dtype=oneflow.int64))
(BUFFER:model.copy_mask:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32))
I20220824 03:16:05.797786 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.emb.weight"
device_tag: "cuda"
scope_symbol_id: 139
variable_conf {
  out: "out"
  shape {
    dim: 6064
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.798072 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.emb.weight
(MODULE:model.emb:VocabEmbedding(num_embeddings=6064, embedding_dim=1024))
(INPUT:_model.emb_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
       dtype=oneflow.int64))
(PARAMETER:model.emb.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
       grad_fn=<accumulate_grad>))
I20220824 03:16:05.799885 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.emb-gather-0"
device_tag: "cuda"
scope_symbol_id: 143
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 309; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/embedding.py\': line 160; ... 7 more"
user_conf {
  op_type_name: "gather"
  input {
    key: "in"
    value {
      s: "model.emb.weight/out"
    }
  }
  input {
    key: "indices"
    value {
      s: "_GraphBase_0_input.1.0_idx/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  input_order: "indices"
  output_order: "out"
}

I20220824 03:16:05.800290 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.emb-gather-0
(OUTPUT:_model.emb_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(MODULE:model.blocks:Sequential())
(INPUT:_model.blocks_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(MODULE:model.blocks.0:Block())
(INPUT:_model.blocks.0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(MODULE:model.blocks.0.ln0:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.0.ln0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(PARAMETER:model.blocks.0.ln0.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ln0.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.805598 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln0.weight"
device_tag: "cuda"
scope_symbol_id: 150
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.805761 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0.weight
I20220824 03:16:05.806691 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln0.bias"
device_tag: "cuda"
scope_symbol_id: 154
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.806849 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0.bias
I20220824 03:16:05.807180 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-layer_norm-1"
device_tag: "cuda"
scope_symbol_id: 157
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 215; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.0.ln0.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln0.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.807729 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-layer_norm-1
(OUTPUT:_model.blocks.0.ln0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.0.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.0.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.0.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.809617 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 162
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.809767 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1.weight
I20220824 03:16:05.810623 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 166
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.810765 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1.bias
I20220824 03:16:05.811015 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-layer_norm-2"
device_tag: "cuda"
scope_symbol_id: 169
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.0.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.811522 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-layer_norm-2
(OUTPUT:_model.blocks.0.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.0.att:RWKV_TimeMix())
(INPUT:_model.blocks.0.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.0.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.0.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.0.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.813743 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.time_shift-pad-3"
device_tag: "cuda"
scope_symbol_id: 174
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.814080 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_shift-pad-3
(OUTPUT:_model.blocks.0.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.815105 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 178
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.815251 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_mix_k
I20220824 03:16:05.815500 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-4"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-4/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.815838 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-4
I20220824 03:16:05.816083 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-5"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-5/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.816411 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-5
I20220824 03:16:05.816581 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_add-6"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-5/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_add-6/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.816875 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_add-6
I20220824 03:16:05.817067 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-7"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-6/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-7/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.817415 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-7
I20220824 03:16:05.817617 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-8"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-4/z_0"
      s: "model.blocks.0.att-broadcast_mul-7/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-8/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.817937 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-8
I20220824 03:16:05.818840 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 197
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.818989 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_mix_v
I20220824 03:16:05.819195 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-9"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-9/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.819514 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-9
I20220824 03:16:05.819730 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-10"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-10/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.820039 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-10
I20220824 03:16:05.820200 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_add-11"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-10/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_add-11/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.820495 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_add-11
I20220824 03:16:05.820674 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-12"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-11/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-12/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.820984 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-12
I20220824 03:16:05.821171 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-13"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-9/z_0"
      s: "model.blocks.0.att-broadcast_mul-12/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-13/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.821482 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-13
I20220824 03:16:05.822360 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 216
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.822506 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_mix_r
I20220824 03:16:05.822719 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-14"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-14/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.823029 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-14
I20220824 03:16:05.823237 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-15"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-15/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.823560 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-15
I20220824 03:16:05.823736 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_add-16"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-15/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.824044 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_add-16
I20220824 03:16:05.824229 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-17"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-17/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.824534 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-17
I20220824 03:16:05.824738 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-18"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-14/z_0"
      s: "model.blocks.0.att-broadcast_mul-17/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-18/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.825042 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-18
(MODULE:model.blocks.0.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.826709 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 236
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.826853 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key.weight
I20220824 03:16:05.827445 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.key-hierarchical_parallel_cast-19"
device_tag: "cuda"
scope_symbol_id: 239
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-add_n-8/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.827733 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key-hierarchical_parallel_cast-19
I20220824 03:16:05.828048 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.key-broadcast_matmul-20"
device_tag: "cuda"
scope_symbol_id: 239
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.key-broadcast_matmul-20/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.828418 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key-broadcast_matmul-20
(OUTPUT:_model.blocks.0.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.0.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.830139 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 247
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.830286 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value.weight
I20220824 03:16:05.830824 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.value-hierarchical_parallel_cast-21"
device_tag: "cuda"
scope_symbol_id: 250
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-add_n-13/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.831104 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value-hierarchical_parallel_cast-21
I20220824 03:16:05.831324 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.value-broadcast_matmul-22"
device_tag: "cuda"
scope_symbol_id: 250
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.value-broadcast_matmul-22/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.831707 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value-broadcast_matmul-22
(OUTPUT:_model.blocks.0.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.0.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.833439 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 258
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.833585 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance.weight
I20220824 03:16:05.834167 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23"
device_tag: "cuda"
scope_symbol_id: 261
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-add_n-18/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.834448 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-hierarchical_parallel_cast-23
I20220824 03:16:05.834669 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-broadcast_matmul-24"
device_tag: "cuda"
scope_symbol_id: 261
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-24/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.835040 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-broadcast_matmul-24
(OUTPUT:_model.blocks.0.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.835455 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-sigmoid_v2-25"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-24/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-sigmoid_v2-25/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.835736 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-sigmoid_v2-25
(MODULE:model.blocks.0.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.0.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
(PARAMETER:model.blocks.0.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.837273 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 272
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.837416 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output.weight
I20220824 03:16:05.837949 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-hierarchical_parallel_cast-26"
device_tag: "cuda"
scope_symbol_id: 275
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-sigmoid_v2-25/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-hierarchical_parallel_cast-26/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.838222 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-hierarchical_parallel_cast-26
I20220824 03:16:05.838444 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-broadcast_matmul-27"
device_tag: "cuda"
scope_symbol_id: 275
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.output-hierarchical_parallel_cast-26/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul-27/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.838814 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-broadcast_matmul-27
(OUTPUT:_model.blocks.0.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.0.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.839331 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0-add_n-28"
device_tag: "cuda"
scope_symbol_id: 281
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
      s: "model.blocks.0.att.output-broadcast_matmul-27/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0-add_n-28/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.839711 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0-add_n-28
(MODULE:model.blocks.0.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.0.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.841383 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 286
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.841537 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2.weight
I20220824 03:16:05.842355 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 290
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.842495 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2.bias
I20220824 03:16:05.842733 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-layer_norm-29"
device_tag: "cuda"
scope_symbol_id: 293
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.0.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-28/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.843221 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-layer_norm-29
(OUTPUT:_model.blocks.0.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.0.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.0.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.0.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.0.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.0.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.844894 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.time_shift-pad-30"
device_tag: "cuda"
scope_symbol_id: 298
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-30/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.845134 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_shift-pad-30
(OUTPUT:_model.blocks.0.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.845880 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 302
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.845986 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_mix_k
I20220824 03:16:05.846119 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-31"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-31/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.846321 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-31
I20220824 03:16:05.846463 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-32"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-32/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.846678 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-32
I20220824 03:16:05.846812 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_add-33"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-32/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_add-33/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.847473 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_add-33
I20220824 03:16:05.847606 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-34"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-30/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-33/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-34/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.847838 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-34
I20220824 03:16:05.847970 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-35"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-31/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-34/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-35/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.848170 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-35
I20220824 03:16:05.862097 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 321
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.862229 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_mix_r
I20220824 03:16:05.862373 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-36"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-36/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.862583 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-36
I20220824 03:16:05.862721 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-37"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-37/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.862926 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-37
I20220824 03:16:05.863024 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_add-38"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-37/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_add-38/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.863211 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_add-38
I20220824 03:16:05.863327 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-39"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-30/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-38/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-39/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.863520 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-39
I20220824 03:16:05.863643 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-40"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-36/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-39/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-40/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.863853 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-40
(MODULE:model.blocks.0.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.0.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.865191 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 341
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.865304 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key.weight
I20220824 03:16:05.865680 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-hierarchical_parallel_cast-41"
device_tag: "cuda"
scope_symbol_id: 344
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-add_n-35/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-hierarchical_parallel_cast-41/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.865861 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-hierarchical_parallel_cast-41
I20220824 03:16:05.866009 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-broadcast_matmul-42"
device_tag: "cuda"
scope_symbol_id: 344
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn.key-hierarchical_parallel_cast-41/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-42/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.866268 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-broadcast_matmul-42
(OUTPUT:_model.blocks.0.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.866580 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-relu-43"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-42/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-relu-43/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.866762 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-relu-43
I20220824 03:16:05.866909 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-square-44"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-relu-43/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-square-44/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.867074 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-square-44
(MODULE:model.blocks.0.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.0.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.0.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.868225 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 358
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.868330 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value.weight
I20220824 03:16:05.868680 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-hierarchical_parallel_cast-45"
device_tag: "cuda"
scope_symbol_id: 361
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-square-44/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-hierarchical_parallel_cast-45/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.868866 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-hierarchical_parallel_cast-45
I20220824 03:16:05.869012 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-broadcast_matmul-46"
device_tag: "cuda"
scope_symbol_id: 361
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn.value-hierarchical_parallel_cast-45/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-46/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.869271 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-broadcast_matmul-46
(OUTPUT:_model.blocks.0.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.0.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.870457 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 369
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.870569 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance.weight
I20220824 03:16:05.870888 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-47"
device_tag: "cuda"
scope_symbol_id: 372
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-add_n-40/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-47/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.871068 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-hierarchical_parallel_cast-47
I20220824 03:16:05.871212 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-broadcast_matmul-48"
device_tag: "cuda"
scope_symbol_id: 372
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-47/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-48/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.871472 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-broadcast_matmul-48
(OUTPUT:_model.blocks.0.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.871742 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-sigmoid_v2-49"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-48/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2-49/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.871935 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-sigmoid_v2-49
I20220824 03:16:05.872056 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-50"
device_tag: "cuda"
scope_symbol_id: 305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2-49/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-46/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-50/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.872270 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-50
(OUTPUT:_model.blocks.0.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 03:16:05.872509 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0-add_n-51"
device_tag: "cuda"
scope_symbol_id: 281
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0-add_n-28/out_0"
      s: "model.blocks.0.ffn-broadcast_mul-50/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0-add_n-51/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.872723 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0-add_n-51
(OUTPUT:_model.blocks.0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.1:Block())
(INPUT:_model.blocks.1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.1.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.1.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.874413 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 390
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.874516 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1.weight
I20220824 03:16:05.875089 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 394
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.875190 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1.bias
I20220824 03:16:05.875352 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-layer_norm-52"
device_tag: "cuda"
scope_symbol_id: 397
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.1.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-51/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.875669 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-layer_norm-52
(OUTPUT:_model.blocks.1.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.1.att:RWKV_TimeMix())
(INPUT:_model.blocks.1.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.1.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.1.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.1.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.877164 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.time_shift-pad-53"
device_tag: "cuda"
scope_symbol_id: 402
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-53/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.877398 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_shift-pad-53
(OUTPUT:_model.blocks.1.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.878067 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 406
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.878172 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_mix_k
I20220824 03:16:05.878300 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-54"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-54/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.878504 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-54
I20220824 03:16:05.878654 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-55"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-55/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.878868 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-55
I20220824 03:16:05.878988 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_add-56"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-55/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_add-56/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.879187 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_add-56
I20220824 03:16:05.879317 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-57"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.time_shift-pad-53/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-56/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-57/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.879539 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-57
I20220824 03:16:05.879668 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-58"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-54/z_0"
      s: "model.blocks.1.att-broadcast_mul-57/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-58/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.879870 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-58
I20220824 03:16:05.880434 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 425
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.880538 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_mix_v
I20220824 03:16:05.880693 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-59"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-59/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.880896 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-59
I20220824 03:16:05.881045 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-60"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-60/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.881261 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-60
I20220824 03:16:05.881377 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_add-61"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-60/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_add-61/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.881567 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_add-61
I20220824 03:16:05.881695 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-62"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.time_shift-pad-53/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-61/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-62/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.881922 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-62
I20220824 03:16:05.882051 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-63"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-59/z_0"
      s: "model.blocks.1.att-broadcast_mul-62/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-63/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.882251 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-63
I20220824 03:16:05.882810 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 444
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.882917 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_mix_r
I20220824 03:16:05.883060 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-64"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-64/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.883257 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-64
I20220824 03:16:05.883404 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-65"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-65/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.883605 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-65
I20220824 03:16:05.883721 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_add-66"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-65/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_add-66/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.883919 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_add-66
I20220824 03:16:05.884034 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-67"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.time_shift-pad-53/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-66/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-67/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.884258 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-67
I20220824 03:16:05.884389 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-68"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-64/z_0"
      s: "model.blocks.1.att-broadcast_mul-67/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-68/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.884588 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-68
(MODULE:model.blocks.1.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.885651 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 464
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.885753 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key.weight
I20220824 03:16:05.886174 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.key-hierarchical_parallel_cast-69"
device_tag: "cuda"
scope_symbol_id: 467
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-add_n-58/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.key-hierarchical_parallel_cast-69/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.886364 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key-hierarchical_parallel_cast-69
I20220824 03:16:05.886516 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.key-broadcast_matmul-70"
device_tag: "cuda"
scope_symbol_id: 467
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.key-hierarchical_parallel_cast-69/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-70/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.886759 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key-broadcast_matmul-70
(OUTPUT:_model.blocks.1.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.1.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.887900 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 475
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.888013 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value.weight
I20220824 03:16:05.888365 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.value-hierarchical_parallel_cast-71"
device_tag: "cuda"
scope_symbol_id: 478
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-add_n-63/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.value-hierarchical_parallel_cast-71/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.888550 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value-hierarchical_parallel_cast-71
I20220824 03:16:05.888695 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.value-broadcast_matmul-72"
device_tag: "cuda"
scope_symbol_id: 478
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.value-hierarchical_parallel_cast-71/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-72/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.888947 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value-broadcast_matmul-72
(OUTPUT:_model.blocks.1.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.1.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.890076 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 486
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.890184 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance.weight
I20220824 03:16:05.890516 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-hierarchical_parallel_cast-73"
device_tag: "cuda"
scope_symbol_id: 489
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-add_n-68/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-hierarchical_parallel_cast-73/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.890698 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-hierarchical_parallel_cast-73
I20220824 03:16:05.890854 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-broadcast_matmul-74"
device_tag: "cuda"
scope_symbol_id: 489
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.receptance-hierarchical_parallel_cast-73/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-74/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.891113 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-broadcast_matmul-74
(OUTPUT:_model.blocks.1.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.891408 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-sigmoid_v2-75"
device_tag: "cuda"
scope_symbol_id: 409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-74/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-sigmoid_v2-75/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.891582 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-sigmoid_v2-75
(MODULE:model.blocks.1.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.1.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
(PARAMETER:model.blocks.1.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.892642 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 500
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.892745 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output.weight
I20220824 03:16:05.893075 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-hierarchical_parallel_cast-76"
device_tag: "cuda"
scope_symbol_id: 503
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-sigmoid_v2-75/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-hierarchical_parallel_cast-76/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.893254 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-hierarchical_parallel_cast-76
I20220824 03:16:05.893411 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-broadcast_matmul-77"
device_tag: "cuda"
scope_symbol_id: 503
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.output-hierarchical_parallel_cast-76/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul-77/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.893671 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-broadcast_matmul-77
(OUTPUT:_model.blocks.1.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.1.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.894129 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1-add_n-78"
device_tag: "cuda"
scope_symbol_id: 509
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0-add_n-51/out_0"
      s: "model.blocks.1.att.output-broadcast_matmul-77/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1-add_n-78/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.895004 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1-add_n-78
(MODULE:model.blocks.1.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.1.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.898667 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 514
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.899008 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2.weight
I20220824 03:16:05.900694 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 518
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.901001 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2.bias
I20220824 03:16:05.901479 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-layer_norm-79"
device_tag: "cuda"
scope_symbol_id: 521
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.1.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-78/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.902559 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-layer_norm-79
(OUTPUT:_model.blocks.1.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.1.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.1.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.1.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.1.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.1.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.905164 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.time_shift-pad-80"
device_tag: "cuda"
scope_symbol_id: 526
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-80/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.905441 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_shift-pad-80
(OUTPUT:_model.blocks.1.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.906208 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 530
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.906330 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_mix_k
I20220824 03:16:05.906494 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-81"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-81/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.906739 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-81
I20220824 03:16:05.906913 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-82"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-82/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.907161 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-82
I20220824 03:16:05.907300 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_add-83"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-82/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_add-83/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.907538 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_add-83
I20220824 03:16:05.907691 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-84"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-80/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-83/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-84/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.907989 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-84
I20220824 03:16:05.908130 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-85"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-81/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-84/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-85/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.908360 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-85
I20220824 03:16:05.909036 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 549
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.909154 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_mix_r
I20220824 03:16:05.909320 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-86"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-86/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.909559 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-86
I20220824 03:16:05.909729 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-87"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-87/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.909976 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-87
I20220824 03:16:05.910094 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_add-88"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-87/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_add-88/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.910305 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_add-88
I20220824 03:16:05.910454 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-89"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-80/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-88/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-89/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.910688 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-89
I20220824 03:16:05.910835 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-90"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-86/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-89/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-90/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.911064 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-90
(MODULE:model.blocks.1.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.1.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.912389 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 569
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.912520 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key.weight
I20220824 03:16:05.912918 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-hierarchical_parallel_cast-91"
device_tag: "cuda"
scope_symbol_id: 572
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-add_n-85/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-hierarchical_parallel_cast-91/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.913125 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-hierarchical_parallel_cast-91
I20220824 03:16:05.913309 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-broadcast_matmul-92"
device_tag: "cuda"
scope_symbol_id: 572
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn.key-hierarchical_parallel_cast-91/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-92/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.913612 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-broadcast_matmul-92
(OUTPUT:_model.blocks.1.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.913918 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-relu-93"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-92/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-relu-93/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.914139 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-relu-93
I20220824 03:16:05.914292 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-square-94"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-relu-93/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-square-94/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.914502 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-square-94
(MODULE:model.blocks.1.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.1.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.1.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.915735 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 586
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.915843 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value.weight
I20220824 03:16:05.916224 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-hierarchical_parallel_cast-95"
device_tag: "cuda"
scope_symbol_id: 589
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-square-94/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-hierarchical_parallel_cast-95/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.916410 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-hierarchical_parallel_cast-95
I20220824 03:16:05.916569 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-broadcast_matmul-96"
device_tag: "cuda"
scope_symbol_id: 589
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn.value-hierarchical_parallel_cast-95/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-96/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.916816 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-broadcast_matmul-96
(OUTPUT:_model.blocks.1.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.1.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.918174 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 597
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.918283 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance.weight
I20220824 03:16:05.918651 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-97"
device_tag: "cuda"
scope_symbol_id: 600
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-add_n-90/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-97/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.918869 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-hierarchical_parallel_cast-97
I20220824 03:16:05.919031 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-broadcast_matmul-98"
device_tag: "cuda"
scope_symbol_id: 600
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-97/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-98/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.919299 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-broadcast_matmul-98
(OUTPUT:_model.blocks.1.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.919551 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-sigmoid_v2-99"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-98/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2-99/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.919749 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-sigmoid_v2-99
I20220824 03:16:05.919872 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-100"
device_tag: "cuda"
scope_symbol_id: 533
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2-99/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-96/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-100/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.920083 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-100
(OUTPUT:_model.blocks.1.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 03:16:05.920331 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1-add_n-101"
device_tag: "cuda"
scope_symbol_id: 509
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1-add_n-78/out_0"
      s: "model.blocks.1.ffn-broadcast_mul-100/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1-add_n-101/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.920552 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1-add_n-101
(OUTPUT:_model.blocks.1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.2:Block())
(INPUT:_model.blocks.2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.2.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.2.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.922281 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 618
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.922387 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1.weight
I20220824 03:16:05.922983 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 622
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.923084 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1.bias
I20220824 03:16:05.923249 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-layer_norm-102"
device_tag: "cuda"
scope_symbol_id: 625
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.2.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-101/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.923584 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-layer_norm-102
(OUTPUT:_model.blocks.2.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.2.att:RWKV_TimeMix())
(INPUT:_model.blocks.2.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.2.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.2.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.2.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.925061 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.time_shift-pad-103"
device_tag: "cuda"
scope_symbol_id: 630
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-103/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.925318 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_shift-pad-103
(OUTPUT:_model.blocks.2.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.926005 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 634
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.926120 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_mix_k
I20220824 03:16:05.926252 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-104"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-104/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.926462 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-104
I20220824 03:16:05.926620 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-105"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-105/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.926831 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-105
I20220824 03:16:05.926957 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_add-106"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-105/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_add-106/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.927160 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_add-106
I20220824 03:16:05.927289 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-107"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.time_shift-pad-103/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-106/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-107/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.927520 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-107
I20220824 03:16:05.927639 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-108"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-104/z_0"
      s: "model.blocks.2.att-broadcast_mul-107/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-108/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.927841 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-108
I20220824 03:16:05.928462 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 653
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.928567 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_mix_v
I20220824 03:16:05.928714 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-109"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-109/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.928913 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-109
I20220824 03:16:05.929061 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-110"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-110/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.929273 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-110
I20220824 03:16:05.929389 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_add-111"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-110/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_add-111/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.929586 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_add-111
I20220824 03:16:05.929703 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-112"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.time_shift-pad-103/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-111/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-112/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.929919 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-112
I20220824 03:16:05.930034 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-113"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-109/z_0"
      s: "model.blocks.2.att-broadcast_mul-112/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-113/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.930227 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-113
I20220824 03:16:05.930836 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 672
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.930948 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_mix_r
I20220824 03:16:05.931092 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-114"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-114/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.931298 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-114
I20220824 03:16:05.931443 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-115"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-115/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.931644 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-115
I20220824 03:16:05.931753 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_add-116"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-115/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_add-116/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.931938 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_add-116
I20220824 03:16:05.932052 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-117"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.time_shift-pad-103/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-116/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-117/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.932238 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-117
I20220824 03:16:05.932353 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-118"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-114/z_0"
      s: "model.blocks.2.att-broadcast_mul-117/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-118/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.932554 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-118
(MODULE:model.blocks.2.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.933733 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 692
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.933841 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key.weight
I20220824 03:16:05.934331 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.key-hierarchical_parallel_cast-119"
device_tag: "cuda"
scope_symbol_id: 695
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-add_n-108/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.key-hierarchical_parallel_cast-119/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.934520 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key-hierarchical_parallel_cast-119
I20220824 03:16:05.934684 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.key-broadcast_matmul-120"
device_tag: "cuda"
scope_symbol_id: 695
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.key-hierarchical_parallel_cast-119/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-120/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.934950 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key-broadcast_matmul-120
(OUTPUT:_model.blocks.2.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.2.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.936139 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 703
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.936249 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value.weight
I20220824 03:16:05.936658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.value-hierarchical_parallel_cast-121"
device_tag: "cuda"
scope_symbol_id: 706
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-add_n-113/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.value-hierarchical_parallel_cast-121/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.936851 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value-hierarchical_parallel_cast-121
I20220824 03:16:05.937011 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.value-broadcast_matmul-122"
device_tag: "cuda"
scope_symbol_id: 706
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.value-hierarchical_parallel_cast-121/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-122/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.937273 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value-broadcast_matmul-122
(OUTPUT:_model.blocks.2.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.2.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.938482 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 714
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.938588 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance.weight
I20220824 03:16:05.939002 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-hierarchical_parallel_cast-123"
device_tag: "cuda"
scope_symbol_id: 717
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-add_n-118/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-hierarchical_parallel_cast-123/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.939188 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-hierarchical_parallel_cast-123
I20220824 03:16:05.939354 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-broadcast_matmul-124"
device_tag: "cuda"
scope_symbol_id: 717
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.receptance-hierarchical_parallel_cast-123/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-124/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.939606 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-broadcast_matmul-124
(OUTPUT:_model.blocks.2.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.939884 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-sigmoid_v2-125"
device_tag: "cuda"
scope_symbol_id: 637
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-124/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-sigmoid_v2-125/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.940083 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-sigmoid_v2-125
(MODULE:model.blocks.2.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.2.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
(PARAMETER:model.blocks.2.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.941205 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 728
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.941318 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output.weight
I20220824 03:16:05.941726 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-hierarchical_parallel_cast-126"
device_tag: "cuda"
scope_symbol_id: 731
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-sigmoid_v2-125/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-hierarchical_parallel_cast-126/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.941912 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-hierarchical_parallel_cast-126
I20220824 03:16:05.942068 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-broadcast_matmul-127"
device_tag: "cuda"
scope_symbol_id: 731
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.output-hierarchical_parallel_cast-126/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul-127/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.942320 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-broadcast_matmul-127
(OUTPUT:_model.blocks.2.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.2.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.942692 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2-add_n-128"
device_tag: "cuda"
scope_symbol_id: 737
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1-add_n-101/out_0"
      s: "model.blocks.2.att.output-broadcast_matmul-127/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2-add_n-128/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.942981 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2-add_n-128
(MODULE:model.blocks.2.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.2.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.944141 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 742
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.944252 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2.weight
I20220824 03:16:05.944800 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 746
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.944900 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2.bias
I20220824 03:16:05.945055 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-layer_norm-129"
device_tag: "cuda"
scope_symbol_id: 749
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.2.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-128/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.945384 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-layer_norm-129
(OUTPUT:_model.blocks.2.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.2.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.2.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.2.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.2.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.2.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.946697 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.time_shift-pad-130"
device_tag: "cuda"
scope_symbol_id: 754
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-130/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.947430 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_shift-pad-130
(OUTPUT:_model.blocks.2.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.948105 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 758
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.948213 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_mix_k
I20220824 03:16:05.948348 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-131"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-131/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.948560 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-131
I20220824 03:16:05.948700 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-132"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-132/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.948910 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-132
I20220824 03:16:05.949029 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_add-133"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-132/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_add-133/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.949225 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_add-133
I20220824 03:16:05.949353 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-134"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-130/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-133/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-134/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.949575 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-134
I20220824 03:16:05.949707 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-135"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-131/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-134/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-135/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.949914 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-135
I20220824 03:16:05.950480 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 777
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.950585 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_mix_r
I20220824 03:16:05.950731 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-136"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-136/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.950928 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-136
I20220824 03:16:05.951074 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-137"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-137/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.951285 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-137
I20220824 03:16:05.951386 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_add-138"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-137/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_add-138/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.951579 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_add-138
I20220824 03:16:05.951702 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-139"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-130/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-138/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-139/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.951920 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-139
I20220824 03:16:05.952034 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-140"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-136/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-139/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-140/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.952713 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-140
(MODULE:model.blocks.2.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.2.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.953794 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 797
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.953896 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key.weight
I20220824 03:16:05.954319 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-hierarchical_parallel_cast-141"
device_tag: "cuda"
scope_symbol_id: 800
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-add_n-135/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-hierarchical_parallel_cast-141/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.954506 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-hierarchical_parallel_cast-141
I20220824 03:16:05.954658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-broadcast_matmul-142"
device_tag: "cuda"
scope_symbol_id: 800
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn.key-hierarchical_parallel_cast-141/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-142/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.954903 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-broadcast_matmul-142
(OUTPUT:_model.blocks.2.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.955157 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-relu-143"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-142/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-relu-143/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.955343 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-relu-143
I20220824 03:16:05.955480 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-square-144"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-relu-143/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-square-144/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.955646 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-square-144
(MODULE:model.blocks.2.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.2.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.2.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.956719 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 814
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.956823 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value.weight
I20220824 03:16:05.957180 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-hierarchical_parallel_cast-145"
device_tag: "cuda"
scope_symbol_id: 817
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-square-144/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-hierarchical_parallel_cast-145/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.957365 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-hierarchical_parallel_cast-145
I20220824 03:16:05.957523 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-broadcast_matmul-146"
device_tag: "cuda"
scope_symbol_id: 817
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn.value-hierarchical_parallel_cast-145/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-146/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.957769 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-broadcast_matmul-146
(OUTPUT:_model.blocks.2.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.2.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.958880 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 825
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.958987 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance.weight
I20220824 03:16:05.959324 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-147"
device_tag: "cuda"
scope_symbol_id: 828
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-add_n-140/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-147/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.959513 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-hierarchical_parallel_cast-147
I20220824 03:16:05.959657 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-broadcast_matmul-148"
device_tag: "cuda"
scope_symbol_id: 828
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-147/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-148/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.959937 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-broadcast_matmul-148
(OUTPUT:_model.blocks.2.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.960193 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-sigmoid_v2-149"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-148/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2-149/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.960387 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-sigmoid_v2-149
I20220824 03:16:05.960520 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-150"
device_tag: "cuda"
scope_symbol_id: 761
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2-149/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-146/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-150/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.960722 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-150
(OUTPUT:_model.blocks.2.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 03:16:05.960969 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2-add_n-151"
device_tag: "cuda"
scope_symbol_id: 737
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2-add_n-128/out_0"
      s: "model.blocks.2.ffn-broadcast_mul-150/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2-add_n-151/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.961186 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2-add_n-151
(OUTPUT:_model.blocks.2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.3:Block())
(INPUT:_model.blocks.3_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.3.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.3.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.962922 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 846
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.963027 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1.weight
I20220824 03:16:05.963598 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 850
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.963713 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1.bias
I20220824 03:16:05.963884 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-layer_norm-152"
device_tag: "cuda"
scope_symbol_id: 853
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.3.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-151/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.964219 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-layer_norm-152
(OUTPUT:_model.blocks.3.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.3.att:RWKV_TimeMix())
(INPUT:_model.blocks.3.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.3.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.3.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.3.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.965605 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.time_shift-pad-153"
device_tag: "cuda"
scope_symbol_id: 858
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-153/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.965837 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_shift-pad-153
(OUTPUT:_model.blocks.3.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.966521 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 862
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.966631 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_mix_k
I20220824 03:16:05.966764 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-154"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-154/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.966979 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-154
I20220824 03:16:05.967119 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-155"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-155/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.967329 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-155
I20220824 03:16:05.967545 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_add-156"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-155/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_add-156/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.967732 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_add-156
I20220824 03:16:05.967862 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-157"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.time_shift-pad-153/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-156/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-157/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.968091 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-157
I20220824 03:16:05.968223 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-158"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-154/z_0"
      s: "model.blocks.3.att-broadcast_mul-157/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-158/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.968428 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-158
I20220824 03:16:05.969002 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 881
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.969105 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_mix_v
I20220824 03:16:05.969250 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-159"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-159/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.969447 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-159
I20220824 03:16:05.969594 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-160"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-160/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.969802 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-160
I20220824 03:16:05.969925 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_add-161"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-160/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_add-161/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.970125 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_add-161
I20220824 03:16:05.970242 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-162"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.time_shift-pad-153/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-161/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-162/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.970458 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-162
I20220824 03:16:05.970587 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-163"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-159/z_0"
      s: "model.blocks.3.att-broadcast_mul-162/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-163/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.970783 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-163
I20220824 03:16:05.971370 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 900
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.971477 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_mix_r
I20220824 03:16:05.971616 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-164"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-164/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.971830 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-164
I20220824 03:16:05.971963 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-165"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-165/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.972172 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-165
I20220824 03:16:05.972275 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_add-166"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-165/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_add-166/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.972473 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_add-166
I20220824 03:16:05.972606 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-167"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.time_shift-pad-153/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-166/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-167/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.972826 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-167
I20220824 03:16:05.972957 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-168"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-164/z_0"
      s: "model.blocks.3.att-broadcast_mul-167/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-168/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.973161 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-168
(MODULE:model.blocks.3.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.974260 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 920
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.974368 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key.weight
I20220824 03:16:05.974778 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.key-hierarchical_parallel_cast-169"
device_tag: "cuda"
scope_symbol_id: 923
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-add_n-158/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.key-hierarchical_parallel_cast-169/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.974961 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key-hierarchical_parallel_cast-169
I20220824 03:16:05.975124 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.key-broadcast_matmul-170"
device_tag: "cuda"
scope_symbol_id: 923
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.key-hierarchical_parallel_cast-169/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-170/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.975374 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key-broadcast_matmul-170
(OUTPUT:_model.blocks.3.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.3.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.976609 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 931
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.976714 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value.weight
I20220824 03:16:05.977082 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.value-hierarchical_parallel_cast-171"
device_tag: "cuda"
scope_symbol_id: 934
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-add_n-163/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.value-hierarchical_parallel_cast-171/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.977260 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value-hierarchical_parallel_cast-171
I20220824 03:16:05.977403 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.value-broadcast_matmul-172"
device_tag: "cuda"
scope_symbol_id: 934
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.value-hierarchical_parallel_cast-171/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-172/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.977651 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value-broadcast_matmul-172
(OUTPUT:_model.blocks.3.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.3.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.978879 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 942
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.978992 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance.weight
I20220824 03:16:05.979338 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-hierarchical_parallel_cast-173"
device_tag: "cuda"
scope_symbol_id: 945
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-add_n-168/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-hierarchical_parallel_cast-173/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.979519 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-hierarchical_parallel_cast-173
I20220824 03:16:05.979671 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-broadcast_matmul-174"
device_tag: "cuda"
scope_symbol_id: 945
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.receptance-hierarchical_parallel_cast-173/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-174/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.979930 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-broadcast_matmul-174
(OUTPUT:_model.blocks.3.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.980176 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-sigmoid_v2-175"
device_tag: "cuda"
scope_symbol_id: 865
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-174/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-sigmoid_v2-175/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.980370 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-sigmoid_v2-175
(MODULE:model.blocks.3.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.3.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
(PARAMETER:model.blocks.3.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.981509 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 956
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.981616 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output.weight
I20220824 03:16:05.981972 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-hierarchical_parallel_cast-176"
device_tag: "cuda"
scope_symbol_id: 959
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-sigmoid_v2-175/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-hierarchical_parallel_cast-176/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.982158 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-hierarchical_parallel_cast-176
I20220824 03:16:05.982316 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-broadcast_matmul-177"
device_tag: "cuda"
scope_symbol_id: 959
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.output-hierarchical_parallel_cast-176/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul-177/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.982565 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-broadcast_matmul-177
(OUTPUT:_model.blocks.3.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.3.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.982919 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3-add_n-178"
device_tag: "cuda"
scope_symbol_id: 965
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2-add_n-151/out_0"
      s: "model.blocks.3.att.output-broadcast_matmul-177/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3-add_n-178/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.983201 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3-add_n-178
(MODULE:model.blocks.3.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.3.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.984464 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 970
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.984628 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2.weight
I20220824 03:16:05.985273 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 974
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.985381 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2.bias
I20220824 03:16:05.985548 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-layer_norm-179"
device_tag: "cuda"
scope_symbol_id: 977
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.3.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-178/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:05.985872 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-layer_norm-179
(OUTPUT:_model.blocks.3.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.3.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.3.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.3.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.3.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.3.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:05.987185 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.time_shift-pad-180"
device_tag: "cuda"
scope_symbol_id: 982
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-180/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.987416 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_shift-pad-180
(OUTPUT:_model.blocks.3.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:05.988113 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 986
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.988220 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_mix_k
I20220824 03:16:05.988370 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-181"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-181/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.988579 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-181
I20220824 03:16:05.988729 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-182"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-182/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.988945 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-182
I20220824 03:16:05.989063 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_add-183"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-182/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_add-183/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.989253 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_add-183
I20220824 03:16:05.989387 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-184"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-180/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-183/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-184/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.989583 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-184
I20220824 03:16:05.989717 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-185"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-181/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-184/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-185/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.989917 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-185
I20220824 03:16:05.990501 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1005
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.990615 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_mix_r
I20220824 03:16:05.990761 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-186"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-186/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.990963 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-186
I20220824 03:16:05.991111 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-187"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-187/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.991318 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-187
I20220824 03:16:05.991436 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_add-188"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-187/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_add-188/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.991626 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_add-188
I20220824 03:16:05.991750 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-189"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-180/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-188/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-189/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:05.991937 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-189
I20220824 03:16:05.992051 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-190"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-186/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-189/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-190/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.992245 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-190
(MODULE:model.blocks.3.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.3.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.993337 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 1025
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.993465 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key.weight
I20220824 03:16:05.993894 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-hierarchical_parallel_cast-191"
device_tag: "cuda"
scope_symbol_id: 1028
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-add_n-185/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-hierarchical_parallel_cast-191/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.994081 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-hierarchical_parallel_cast-191
I20220824 03:16:05.994232 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-broadcast_matmul-192"
device_tag: "cuda"
scope_symbol_id: 1028
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn.key-hierarchical_parallel_cast-191/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-192/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.994493 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-broadcast_matmul-192
(OUTPUT:_model.blocks.3.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.994752 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-relu-193"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-192/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-relu-193/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.994946 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-relu-193
I20220824 03:16:05.995077 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-square-194"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-relu-193/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-square-194/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:05.995246 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-square-194
(MODULE:model.blocks.3.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.3.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.3.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.996323 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 1042
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.996431 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value.weight
I20220824 03:16:05.996773 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-hierarchical_parallel_cast-195"
device_tag: "cuda"
scope_symbol_id: 1045
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-square-194/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-hierarchical_parallel_cast-195/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.996963 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-hierarchical_parallel_cast-195
I20220824 03:16:05.997133 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-broadcast_matmul-196"
device_tag: "cuda"
scope_symbol_id: 1045
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn.value-hierarchical_parallel_cast-195/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-196/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.997383 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-broadcast_matmul-196
(OUTPUT:_model.blocks.3.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.3.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:05.998515 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1053
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:05.998621 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance.weight
I20220824 03:16:05.998980 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-197"
device_tag: "cuda"
scope_symbol_id: 1056
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-add_n-190/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-197/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:05.999167 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-hierarchical_parallel_cast-197
I20220824 03:16:05.999346 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-broadcast_matmul-198"
device_tag: "cuda"
scope_symbol_id: 1056
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-197/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-198/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:05.999646 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-broadcast_matmul-198
(OUTPUT:_model.blocks.3.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:05.999925 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-sigmoid_v2-199"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-198/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2-199/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.000149 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-sigmoid_v2-199
I20220824 03:16:06.000277 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-200"
device_tag: "cuda"
scope_symbol_id: 989
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2-199/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-196/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-200/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.000484 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-200
(OUTPUT:_model.blocks.3.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 03:16:06.000723 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3-add_n-201"
device_tag: "cuda"
scope_symbol_id: 965
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3-add_n-178/out_0"
      s: "model.blocks.3.ffn-broadcast_mul-200/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3-add_n-201/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.000944 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3-add_n-201
(OUTPUT:_model.blocks.3_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.4:Block())
(INPUT:_model.blocks.4_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.4.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.4.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.003077 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 1074
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.003226 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1.weight
I20220824 03:16:06.003862 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 1078
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.003964 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1.bias
I20220824 03:16:06.004127 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-layer_norm-202"
device_tag: "cuda"
scope_symbol_id: 1081
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.4.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-201/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:06.004469 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-layer_norm-202
(OUTPUT:_model.blocks.4.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.4.att:RWKV_TimeMix())
(INPUT:_model.blocks.4.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.4.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.4.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.4.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:06.006424 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.time_shift-pad-203"
device_tag: "cuda"
scope_symbol_id: 1086
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-203/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.006650 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_shift-pad-203
(OUTPUT:_model.blocks.4.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:06.007571 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1090
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.007702 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_mix_k
I20220824 03:16:06.007854 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-204"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-204/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.008122 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-204
I20220824 03:16:06.008291 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-205"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-205/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.008563 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-205
I20220824 03:16:06.008713 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_add-206"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-205/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_add-206/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.008985 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_add-206
I20220824 03:16:06.009146 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-207"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.time_shift-pad-203/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-206/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-207/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.009457 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-207
I20220824 03:16:06.009609 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-208"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-204/z_0"
      s: "model.blocks.4.att-broadcast_mul-207/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-208/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.009902 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-208
I20220824 03:16:06.010763 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 1109
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.010900 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_mix_v
I20220824 03:16:06.011065 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-209"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-209/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.011346 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-209
I20220824 03:16:06.011523 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-210"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-210/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.011790 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-210
I20220824 03:16:06.011917 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_add-211"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-210/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_add-211/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.012173 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_add-211
I20220824 03:16:06.012317 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-212"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.time_shift-pad-203/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-211/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-212/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.012634 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-212
I20220824 03:16:06.012787 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-213"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-209/z_0"
      s: "model.blocks.4.att-broadcast_mul-212/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-213/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.013062 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-213
I20220824 03:16:06.013800 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1128
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.013928 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_mix_r
I20220824 03:16:06.014094 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-214"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-214/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.014374 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-214
I20220824 03:16:06.014542 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-215"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-215/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.014829 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-215
I20220824 03:16:06.014955 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_add-216"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-215/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_add-216/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.015215 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_add-216
I20220824 03:16:06.015355 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-217"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.time_shift-pad-203/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-216/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-217/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.015640 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-217
I20220824 03:16:06.015786 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-218"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-214/z_0"
      s: "model.blocks.4.att-broadcast_mul-217/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-218/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.016057 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-218
(MODULE:model.blocks.4.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.017423 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 1148
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.017565 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key.weight
I20220824 03:16:06.018040 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.key-hierarchical_parallel_cast-219"
device_tag: "cuda"
scope_symbol_id: 1151
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-add_n-208/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.key-hierarchical_parallel_cast-219/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.018316 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key-hierarchical_parallel_cast-219
I20220824 03:16:06.018491 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.key-broadcast_matmul-220"
device_tag: "cuda"
scope_symbol_id: 1151
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.key-hierarchical_parallel_cast-219/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-220/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.018826 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key-broadcast_matmul-220
(OUTPUT:_model.blocks.4.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.4.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.020221 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 1159
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.020339 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value.weight
I20220824 03:16:06.020713 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.value-hierarchical_parallel_cast-221"
device_tag: "cuda"
scope_symbol_id: 1162
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-add_n-213/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.value-hierarchical_parallel_cast-221/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.020939 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value-hierarchical_parallel_cast-221
I20220824 03:16:06.021090 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.value-broadcast_matmul-222"
device_tag: "cuda"
scope_symbol_id: 1162
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.value-hierarchical_parallel_cast-221/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-222/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.021426 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value-broadcast_matmul-222
(OUTPUT:_model.blocks.4.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.4.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.022753 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1170
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.022871 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance.weight
I20220824 03:16:06.023253 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-hierarchical_parallel_cast-223"
device_tag: "cuda"
scope_symbol_id: 1173
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-add_n-218/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-hierarchical_parallel_cast-223/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.023486 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-hierarchical_parallel_cast-223
I20220824 03:16:06.023635 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-broadcast_matmul-224"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.receptance-hierarchical_parallel_cast-223/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-224/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.023942 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-broadcast_matmul-224
(OUTPUT:_model.blocks.4.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.024230 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-sigmoid_v2-225"
device_tag: "cuda"
scope_symbol_id: 1093
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-224/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-sigmoid_v2-225/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.024444 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-sigmoid_v2-225
(MODULE:model.blocks.4.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.4.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
(PARAMETER:model.blocks.4.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.025616 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 1184
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.025738 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output.weight
I20220824 03:16:06.026118 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-hierarchical_parallel_cast-226"
device_tag: "cuda"
scope_symbol_id: 1187
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-sigmoid_v2-225/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-hierarchical_parallel_cast-226/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.026345 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-hierarchical_parallel_cast-226
I20220824 03:16:06.026500 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-broadcast_matmul-227"
device_tag: "cuda"
scope_symbol_id: 1187
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.output-hierarchical_parallel_cast-226/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul-227/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.026796 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-broadcast_matmul-227
(OUTPUT:_model.blocks.4.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.4.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.027140 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4-add_n-228"
device_tag: "cuda"
scope_symbol_id: 1193
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3-add_n-201/out_0"
      s: "model.blocks.4.att.output-broadcast_matmul-227/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4-add_n-228/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.027438 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4-add_n-228
(MODULE:model.blocks.4.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.4.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.028811 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 1198
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.028928 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2.weight
I20220824 03:16:06.029548 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 1202
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.029664 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2.bias
I20220824 03:16:06.029836 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-layer_norm-229"
device_tag: "cuda"
scope_symbol_id: 1205
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.4.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-228/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:06.030231 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-layer_norm-229
(OUTPUT:_model.blocks.4.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.4.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.4.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.4.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.4.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.4.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:06.031598 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.time_shift-pad-230"
device_tag: "cuda"
scope_symbol_id: 1210
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-230/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.031857 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_shift-pad-230
(OUTPUT:_model.blocks.4.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:06.032586 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1214
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.032702 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_mix_k
I20220824 03:16:06.032850 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-231"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-231/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.033103 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-231
I20220824 03:16:06.033259 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-232"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-232/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.033489 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-232
I20220824 03:16:06.033597 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_add-233"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-232/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_add-233/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.033846 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_add-233
I20220824 03:16:06.033972 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-234"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-230/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-233/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-234/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.034205 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-234
I20220824 03:16:06.034332 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-235"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-231/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-234/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-235/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.034580 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-235
I20220824 03:16:06.035214 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1233
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.035331 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_mix_r
I20220824 03:16:06.035483 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-236"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-236/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.035741 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-236
I20220824 03:16:06.035895 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-237"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-237/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.036136 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-237
I20220824 03:16:06.036260 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_add-238"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-237/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_add-238/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.036499 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_add-238
I20220824 03:16:06.036631 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-239"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-230/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-238/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-239/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.036892 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-239
I20220824 03:16:06.037025 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-240"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-236/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-239/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-240/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.037261 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-240
(MODULE:model.blocks.4.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.4.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.038461 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 1253
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.038583 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key.weight
I20220824 03:16:06.038998 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-hierarchical_parallel_cast-241"
device_tag: "cuda"
scope_symbol_id: 1256
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-add_n-235/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-hierarchical_parallel_cast-241/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.039243 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-hierarchical_parallel_cast-241
I20220824 03:16:06.039407 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-broadcast_matmul-242"
device_tag: "cuda"
scope_symbol_id: 1256
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn.key-hierarchical_parallel_cast-241/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-242/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.039705 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-broadcast_matmul-242
(OUTPUT:_model.blocks.4.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.039952 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-relu-243"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-242/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-relu-243/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.040170 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-relu-243
I20220824 03:16:06.040289 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-square-244"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-relu-243/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-square-244/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.040486 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-square-244
(MODULE:model.blocks.4.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.4.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.4.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.041694 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 1270
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.041846 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value.weight
I20220824 03:16:06.042315 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-hierarchical_parallel_cast-245"
device_tag: "cuda"
scope_symbol_id: 1273
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-square-244/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-hierarchical_parallel_cast-245/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.042595 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-hierarchical_parallel_cast-245
I20220824 03:16:06.042806 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-broadcast_matmul-246"
device_tag: "cuda"
scope_symbol_id: 1273
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn.value-hierarchical_parallel_cast-245/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-246/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.043200 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-broadcast_matmul-246
(OUTPUT:_model.blocks.4.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.4.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.045331 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1281
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.045483 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance.weight
I20220824 03:16:06.046084 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-247"
device_tag: "cuda"
scope_symbol_id: 1284
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-add_n-240/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-247/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.046368 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-hierarchical_parallel_cast-247
I20220824 03:16:06.046582 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-broadcast_matmul-248"
device_tag: "cuda"
scope_symbol_id: 1284
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-247/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-248/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.046954 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-broadcast_matmul-248
(OUTPUT:_model.blocks.4.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.047286 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-sigmoid_v2-249"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-248/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2-249/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.047559 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-sigmoid_v2-249
I20220824 03:16:06.047730 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-250"
device_tag: "cuda"
scope_symbol_id: 1217
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2-249/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-246/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-250/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.048054 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-250
(OUTPUT:_model.blocks.4.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 03:16:06.048375 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4-add_n-251"
device_tag: "cuda"
scope_symbol_id: 1193
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4-add_n-228/out_0"
      s: "model.blocks.4.ffn-broadcast_mul-250/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4-add_n-251/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.048692 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4-add_n-251
(OUTPUT:_model.blocks.4_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.5:Block())
(INPUT:_model.blocks.5_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.5.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.5.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.051056 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 1302
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.051208 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1.weight
I20220824 03:16:06.052199 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 1306
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.052461 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1.bias
I20220824 03:16:06.052831 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-layer_norm-252"
device_tag: "cuda"
scope_symbol_id: 1309
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.5.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-251/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:06.053674 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-layer_norm-252
(OUTPUT:_model.blocks.5.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.5.att:RWKV_TimeMix())
(INPUT:_model.blocks.5.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.5.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.5.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.5.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:06.056896 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.time_shift-pad-253"
device_tag: "cuda"
scope_symbol_id: 1314
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-253/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.057394 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_shift-pad-253
(OUTPUT:_model.blocks.5.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:06.058746 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1318
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.058992 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_mix_k
I20220824 03:16:06.059275 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-254"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-254/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.059789 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-254
I20220824 03:16:06.060007 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-255"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-255/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.060292 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-255
I20220824 03:16:06.060425 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_add-256"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-255/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_add-256/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.060680 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_add-256
I20220824 03:16:06.060827 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-257"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.time_shift-pad-253/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-256/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-257/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.061142 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-257
I20220824 03:16:06.061388 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-258"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-254/z_0"
      s: "model.blocks.5.att-broadcast_mul-257/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-258/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.061789 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-258
I20220824 03:16:06.062773 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 1337
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.062950 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_mix_v
I20220824 03:16:06.063194 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-259"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-259/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.063742 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-259
I20220824 03:16:06.064028 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-260"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-260/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.064378 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-260
I20220824 03:16:06.064579 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_add-261"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-260/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_add-261/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.064939 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_add-261
I20220824 03:16:06.065171 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-262"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.time_shift-pad-253/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-261/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-262/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.065922 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-262
I20220824 03:16:06.066148 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-263"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-259/z_0"
      s: "model.blocks.5.att-broadcast_mul-262/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-263/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.066514 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-263
I20220824 03:16:06.068044 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1356
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.068220 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_mix_r
I20220824 03:16:06.068451 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-264"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-264/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.068874 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-264
I20220824 03:16:06.069084 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-265"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-265/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.069494 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-265
I20220824 03:16:06.069865 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_add-266"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-265/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_add-266/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.070410 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_add-266
I20220824 03:16:06.070618 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-267"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.time_shift-pad-253/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-266/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-267/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.071005 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-267
I20220824 03:16:06.071188 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-268"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-264/z_0"
      s: "model.blocks.5.att-broadcast_mul-267/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-268/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.071485 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-268
(MODULE:model.blocks.5.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.073032 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 1376
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.073184 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key.weight
I20220824 03:16:06.073634 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.key-hierarchical_parallel_cast-269"
device_tag: "cuda"
scope_symbol_id: 1379
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-add_n-258/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.key-hierarchical_parallel_cast-269/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.073935 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key-hierarchical_parallel_cast-269
I20220824 03:16:06.074160 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.key-broadcast_matmul-270"
device_tag: "cuda"
scope_symbol_id: 1379
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.key-hierarchical_parallel_cast-269/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-270/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.074513 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key-broadcast_matmul-270
(OUTPUT:_model.blocks.5.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.5.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.075807 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 1387
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.076390 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value.weight
I20220824 03:16:06.076754 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.value-hierarchical_parallel_cast-271"
device_tag: "cuda"
scope_symbol_id: 1390
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-add_n-263/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.value-hierarchical_parallel_cast-271/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.076942 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value-hierarchical_parallel_cast-271
I20220824 03:16:06.077100 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.value-broadcast_matmul-272"
device_tag: "cuda"
scope_symbol_id: 1390
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.value-hierarchical_parallel_cast-271/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-272/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.077351 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value-broadcast_matmul-272
(OUTPUT:_model.blocks.5.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.5.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.078670 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1398
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.078780 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance.weight
I20220824 03:16:06.079166 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-hierarchical_parallel_cast-273"
device_tag: "cuda"
scope_symbol_id: 1401
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-add_n-268/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-hierarchical_parallel_cast-273/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.079360 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-hierarchical_parallel_cast-273
I20220824 03:16:06.079507 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-broadcast_matmul-274"
device_tag: "cuda"
scope_symbol_id: 1401
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.receptance-hierarchical_parallel_cast-273/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-274/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.079782 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-broadcast_matmul-274
(OUTPUT:_model.blocks.5.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.080034 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-sigmoid_v2-275"
device_tag: "cuda"
scope_symbol_id: 1321
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-274/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-sigmoid_v2-275/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.080225 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-sigmoid_v2-275
(MODULE:model.blocks.5.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.5.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
(PARAMETER:model.blocks.5.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.081400 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 1412
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.081506 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output.weight
I20220824 03:16:06.081866 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-hierarchical_parallel_cast-276"
device_tag: "cuda"
scope_symbol_id: 1415
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-sigmoid_v2-275/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-hierarchical_parallel_cast-276/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.082058 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-hierarchical_parallel_cast-276
I20220824 03:16:06.082203 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-broadcast_matmul-277"
device_tag: "cuda"
scope_symbol_id: 1415
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.output-hierarchical_parallel_cast-276/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul-277/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.082458 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-broadcast_matmul-277
(OUTPUT:_model.blocks.5.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.5.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.082803 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5-add_n-278"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4-add_n-251/out_0"
      s: "model.blocks.5.att.output-broadcast_matmul-277/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5-add_n-278/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.083092 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5-add_n-278
(MODULE:model.blocks.5.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.5.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.084388 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 1426
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.084497 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2.weight
I20220824 03:16:06.085155 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 1430
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.085260 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2.bias
I20220824 03:16:06.085426 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-layer_norm-279"
device_tag: "cuda"
scope_symbol_id: 1433
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.5.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-278/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:06.085760 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-layer_norm-279
(OUTPUT:_model.blocks.5.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.5.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.5.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.5.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.5.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.5.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 03:16:06.087234 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.time_shift-pad-280"
device_tag: "cuda"
scope_symbol_id: 1438
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-280/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.087479 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_shift-pad-280
(OUTPUT:_model.blocks.5.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 03:16:06.088245 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1442
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.088353 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_mix_k
I20220824 03:16:06.088487 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-281"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-281/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.088692 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-281
I20220824 03:16:06.088833 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-282"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-282/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.089079 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-282
I20220824 03:16:06.089228 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_add-283"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-282/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_add-283/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.089473 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_add-283
I20220824 03:16:06.089615 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-284"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-280/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-283/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-284/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.089808 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-284
I20220824 03:16:06.089929 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-285"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-281/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-284/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-285/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.090167 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-285
I20220824 03:16:06.090956 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1461
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.091113 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_mix_r
I20220824 03:16:06.091321 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-286"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-286/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.091626 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-286
I20220824 03:16:06.091848 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-287"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-287/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.092173 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-287
I20220824 03:16:06.092362 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_add-288"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-287/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_add-288/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.092653 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_add-288
I20220824 03:16:06.092835 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-289"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-280/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-288/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-289/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.093180 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-289
I20220824 03:16:06.093376 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-290"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-286/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-289/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-290/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.093695 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-290
(MODULE:model.blocks.5.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.5.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.095150 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 1481
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.095299 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key.weight
I20220824 03:16:06.095666 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-hierarchical_parallel_cast-291"
device_tag: "cuda"
scope_symbol_id: 1484
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-add_n-285/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-hierarchical_parallel_cast-291/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.095875 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-hierarchical_parallel_cast-291
I20220824 03:16:06.096042 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-broadcast_matmul-292"
device_tag: "cuda"
scope_symbol_id: 1484
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn.key-hierarchical_parallel_cast-291/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-292/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.096300 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-broadcast_matmul-292
(OUTPUT:_model.blocks.5.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.096555 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-relu-293"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-292/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-relu-293/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.096761 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-relu-293
I20220824 03:16:06.096880 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-square-294"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-relu-293/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-square-294/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.097060 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-square-294
(MODULE:model.blocks.5.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.5.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.5.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.098526 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 1498
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.098637 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value.weight
I20220824 03:16:06.099118 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-hierarchical_parallel_cast-295"
device_tag: "cuda"
scope_symbol_id: 1501
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-square-294/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-hierarchical_parallel_cast-295/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.099314 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-hierarchical_parallel_cast-295
I20220824 03:16:06.099483 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-broadcast_matmul-296"
device_tag: "cuda"
scope_symbol_id: 1501
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn.value-hierarchical_parallel_cast-295/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-296/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.099750 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-broadcast_matmul-296
(OUTPUT:_model.blocks.5.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.5.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.101599 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1509
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.101713 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance.weight
I20220824 03:16:06.102058 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-297"
device_tag: "cuda"
scope_symbol_id: 1512
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-add_n-290/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-297/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.102247 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-hierarchical_parallel_cast-297
I20220824 03:16:06.102394 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-broadcast_matmul-298"
device_tag: "cuda"
scope_symbol_id: 1512
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-297/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-298/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.102648 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-broadcast_matmul-298
(OUTPUT:_model.blocks.5.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.102895 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-sigmoid_v2-299"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-298/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2-299/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.103097 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-sigmoid_v2-299
I20220824 03:16:06.103217 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-300"
device_tag: "cuda"
scope_symbol_id: 1445
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2-299/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-296/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-300/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.103428 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-300
(OUTPUT:_model.blocks.5.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 03:16:06.103667 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5-add_n-301"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5-add_n-278/out_0"
      s: "model.blocks.5.ffn-broadcast_mul-300/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5-add_n-301/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.103897 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5-add_n-301
(OUTPUT:_model.blocks.5_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(OUTPUT:_model.blocks_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.ln_out:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.ln_out_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.ln_out.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.ln_out.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.105510 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.ln_out.weight"
device_tag: "cuda"
scope_symbol_id: 1529
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.105621 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out.weight
I20220824 03:16:06.106326 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.ln_out.bias"
device_tag: "cuda"
scope_symbol_id: 1533
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.106434 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out.bias
I20220824 03:16:06.106595 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-layer_norm-302"
device_tag: "cuda"
scope_symbol_id: 1536
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 313; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 7 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.ln_out.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.ln_out.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-301/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.ln_out-layer_norm-302/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.ln_out-layer_norm-302/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.ln_out-layer_norm-302/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 03:16:06.106916 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-layer_norm-302
(OUTPUT:_model.ln_out_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.head_q:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col))
(INPUT:_model.head_q_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.head_q.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.108444 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.head_q.weight"
device_tag: "cuda"
scope_symbol_id: 1541
variable_conf {
  out: "out"
  shape {
    dim: 256
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.108561 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q.weight
I20220824 03:16:06.108932 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-hierarchical_parallel_cast-303"
device_tag: "cuda"
scope_symbol_id: 1544
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.ln_out-layer_norm-302/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-hierarchical_parallel_cast-303/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.109122 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-hierarchical_parallel_cast-303
I20220824 03:16:06.109288 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-broadcast_matmul-304"
device_tag: "cuda"
scope_symbol_id: 1544
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 317; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 7 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.head_q-hierarchical_parallel_cast-303/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_q.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-broadcast_matmul-304/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.109625 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-broadcast_matmul-304
(OUTPUT:_model.head_q_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 256),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.head_k:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col))
(INPUT:_model.head_k_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.head_k.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.111150 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.head_k.weight"
device_tag: "cuda"
scope_symbol_id: 1552
variable_conf {
  out: "out"
  shape {
    dim: 256
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.111255 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k.weight
I20220824 03:16:06.111593 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-hierarchical_parallel_cast-305"
device_tag: "cuda"
scope_symbol_id: 1555
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.ln_out-layer_norm-302/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-hierarchical_parallel_cast-305/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.111786 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-hierarchical_parallel_cast-305
I20220824 03:16:06.111930 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-broadcast_matmul-306"
device_tag: "cuda"
scope_symbol_id: 1555
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 318; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 7 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.head_k-hierarchical_parallel_cast-305/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_k.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-broadcast_matmul-306/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.112190 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-broadcast_matmul-306
(OUTPUT:_model.head_k_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 256),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.112464 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-307"
device_tag: "cuda"
scope_symbol_id: 1561
loc: ""
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model.head_k-broadcast_matmul-306/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-307/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 2
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 03:16:06.112679 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-307
I20220824 03:16:06.112829 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-308"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 319; ... 6 more"
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model.head_q-broadcast_matmul-304/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-transpose-307/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-308/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.113068 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-308
I20220824 03:16:06.113207 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-scalar_mul-309"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 319; ... 6 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model-batch_matmul-308/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-scalar_mul-309/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0.00390625
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.113420 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-scalar_mul-309
I20220824 03:16:06.113613 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-one_hot-310"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 322; ... 6 more"
user_conf {
  op_type_name: "one_hot"
  input {
    key: "indices"
    value {
      s: "_GraphBase_0_input.1.0_idx/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model-one_hot-310/out_0"
    }
  }
  attr {
    key: "depth"
    value {
      at_int64: 6064
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kInt64
    }
  }
  attr {
    key: "floating_off_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "floating_on_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_off_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "integer_on_value"
    value {
      at_int64: 1
    }
  }
  input_order: "indices"
  output_order: "out"
}

I20220824 03:16:06.113844 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-one_hot-310
I20220824 03:16:06.113943 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-cast-311"
device_tag: "cuda"
scope_symbol_id: 1561
loc: ""
user_conf {
  op_type_name: "cast"
  input {
    key: "in"
    value {
      s: "model-one_hot-310/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-cast-311/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "pin_memory"
    value {
      at_bool: false
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.114111 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-cast-311
I20220824 03:16:06.114220 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-312"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 322; ... 6 more"
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model-scalar_mul-309/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-cast-311/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-312/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.114451 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-312
(MODULE:model.head:Linear1D(in_features=1024, out_features=6064, bias=False, parallel=row))
(INPUT:_model.head_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.head.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 03:16:06.115839 1458836 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.head.weight"
device_tag: "cuda"
scope_symbol_id: 1581
variable_conf {
  out: "out"
  shape {
    dim: 6064
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 03:16:06.115949 1458836 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.head.weight
I20220824 03:16:06.116272 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-hierarchical_parallel_cast-313"
device_tag: "cuda"
scope_symbol_id: 1584
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.ln_out-layer_norm-302/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-hierarchical_parallel_cast-313/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.116466 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-hierarchical_parallel_cast-313
I20220824 03:16:06.116616 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-broadcast_matmul-314"
device_tag: "cuda"
scope_symbol_id: 1584
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 324; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 7 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.head-hierarchical_parallel_cast-313/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-broadcast_matmul-314/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.116871 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-broadcast_matmul-314
(OUTPUT:_model.head_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 6064),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 03:16:06.117143 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-add_n-315"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 324; ... 6 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.head-broadcast_matmul-314/out_0"
      s: "model-batch_matmul-312/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-add_n-315/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.117362 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-add_n-315
I20220824 03:16:06.117524 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-316"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-add_n-315/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-316/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.117738 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-316
I20220824 03:16:06.118067 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-317"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "_GraphBase_0_input.1.1_targets/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-317/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.118273 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-317
I20220824 03:16:06.118468 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-318"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model-reshape-316/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-318/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 03:16:06.118660 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-318
I20220824 03:16:06.118763 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-319"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-transpose-318/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-319/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.118968 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-319
I20220824 03:16:06.119068 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-log_softmax-320"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "log_softmax"
  input {
    key: "in"
    value {
      s: "model-reshape-319/out_0"
    }
  }
  output {
    key: "prob"
    value {
      s: "model-log_softmax-320/prob_0"
    }
  }
  input_order: "in"
  output_order: "prob"
}

I20220824 03:16:06.119226 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-log_softmax-320
I20220824 03:16:06.119360 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-nll-321"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "nll"
  input {
    key: "input"
    value {
      s: "model-log_softmax-320/prob_0"
    }
  }
  input {
    key: "target"
    value {
      s: "model-reshape-317/out_0"
    }
  }
  output {
    key: "out_weight"
    value {
      s: "model-nll-321/out_weight_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-nll-321/output_0"
    }
  }
  attr {
    key: "ignore_index"
    value {
      at_int64: -100
    }
  }
  input_order: "input"
  input_order: "target"
  output_order: "output"
  output_order: "out_weight"
}

I20220824 03:16:06.119587 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-nll-321
I20220824 03:16:06.119706 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-322"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-nll-321/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-322/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.119896 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-322
I20220824 03:16:06.120051 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reduce_sum-323"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model-reshape-322/out_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model-reduce_sum-323/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 03:16:06.120250 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reduce_sum-323
I20220824 03:16:06.120358 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reduce_sum-324"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model-nll-321/out_weight_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model-reduce_sum-324/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 03:16:06.120546 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reduce_sum-324
I20220824 03:16:06.120658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_div-325"
device_tag: "cuda"
scope_symbol_id: 1561
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "broadcast_div"
  input {
    key: "x"
    value {
      s: "model-reduce_sum-323/output_tensor_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model-reduce_sum-324/output_tensor_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model-broadcast_div-325/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.120836 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_div-325
(OUTPUT:_model_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.partial_sum,), is_lazy='True', size=(),
       dtype=oneflow.float32, grad_fn=<broadcast_div_backward>))
I20220824 03:16:06.121125 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "scalar_add-326"
device_tag: "cuda"
scope_symbol_id: 1623
loc: "Python Stack[-2]: \'run_step\' at \'/home/zhangxiaoyu/libai/libai/engine/trainer.py\': line 348; Python Stack[-1]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 107; ... 5 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model-broadcast_div-325/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "scalar_add-326/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.121333 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
scalar_add-326
I20220824 03:16:06.121579 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "ones_like-327"
device_tag: "cuda"
scope_symbol_id: 1627
loc: ""
user_conf {
  op_type_name: "ones_like"
  input {
    key: "like"
    value {
      s: "scalar_add-326/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "ones_like-327/out_0"
    }
  }
  input_order: "like"
  output_order: "out"
}

I20220824 03:16:06.121745 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
ones_like-327
I20220824 03:16:06.121992 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_div-328"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "broadcast_div"
  input {
    key: "x"
    value {
      s: "ones_like-327/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model-reduce_sum-324/output_tensor_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model-broadcast_div-328/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.122192 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_div-328
I20220824 03:16:06.122310 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_div_grad-329"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "broadcast_div_grad"
  input {
    key: "dz"
    value {
      s: "ones_like-327/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model-reduce_sum-324/output_tensor_0"
    }
  }
  input {
    key: "z"
    value {
      s: "model-broadcast_div-325/z_0"
    }
  }
  output {
    key: "dy"
    value {
      s: "model-broadcast_div_grad-329/dy_0"
    }
  }
  input_order: "dz"
  input_order: "z"
  input_order: "y"
  output_order: "dy"
}

I20220824 03:16:06.122519 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_div_grad-329
I20220824 03:16:06.122642 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_like-330"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "broadcast_like"
  input {
    key: "like"
    value {
      s: "model-reshape-322/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model-broadcast_div-328/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model-broadcast_like-330/y_0"
    }
  }
  attr {
    key: "broadcast_axes"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.122871 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_like-330
I20220824 03:16:06.123003 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_like-331"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "broadcast_like"
  input {
    key: "like"
    value {
      s: "model-nll-321/out_weight_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model-broadcast_div_grad-329/dy_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model-broadcast_like-331/y_0"
    }
  }
  attr {
    key: "broadcast_axes"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.123232 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_like-331
I20220824 03:16:06.123330 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-332"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-broadcast_like-330/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-332/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.123548 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-332
I20220824 03:16:06.123677 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-nll_grad-333"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "nll_grad"
  input {
    key: "input"
    value {
      s: "model-log_softmax-320/prob_0"
    }
  }
  input {
    key: "out_grad"
    value {
      s: "model-reshape-332/out_0"
    }
  }
  input {
    key: "target"
    value {
      s: "model-reshape-317/out_0"
    }
  }
  output {
    key: "in_grad"
    value {
      s: "model-nll_grad-333/in_grad_0"
    }
  }
  attr {
    key: "ignore_index"
    value {
      at_int64: -100
    }
  }
  input_order: "out_grad"
  input_order: "input"
  input_order: "target"
  output_order: "in_grad"
}

I20220824 03:16:06.123908 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-nll_grad-333
I20220824 03:16:06.124004 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-log_softmax_grad-334"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "log_softmax_grad"
  input {
    key: "dy"
    value {
      s: "model-nll_grad-333/in_grad_0"
    }
  }
  input {
    key: "prob"
    value {
      s: "model-log_softmax-320/prob_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model-log_softmax_grad-334/dx_0"
    }
  }
  input_order: "prob"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.124159 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-log_softmax_grad-334
I20220824 03:16:06.124262 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-335"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-log_softmax_grad-334/dx_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-335/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.124452 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-335
I20220824 03:16:06.124557 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-336"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model-reshape-335/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-336/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 03:16:06.124740 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-336
I20220824 03:16:06.124832 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-337"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-transpose-336/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-337/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.125030 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-337
I20220824 03:16:06.125128 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-broadcast_matmul-338"
device_tag: "cuda"
scope_symbol_id: 1662
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model-reshape-337/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-broadcast_matmul-338/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.125363 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-broadcast_matmul-338
I20220824 03:16:06.125479 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-broadcast_matmul_grad_b-339"
device_tag: "cuda"
scope_symbol_id: 1662
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model-reshape-337/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head-hierarchical_parallel_cast-313/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-broadcast_matmul_grad_b-339/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.126168 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-broadcast_matmul_grad_b-339
I20220824 03:16:06.126278 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-340"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model-reshape-337/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-cast-311/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-340/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.126538 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-340
I20220824 03:16:06.126798 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-scalar_mul-341"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model-batch_matmul-340/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-scalar_mul-341/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0.00390625
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.127019 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-scalar_mul-341
I20220824 03:16:06.127122 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-342"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model-scalar_mul-341/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-transpose-307/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-342/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.127382 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-342
I20220824 03:16:06.127475 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-343"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model.head_q-broadcast_matmul-304/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-scalar_mul-341/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-343/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: true
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.127714 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-343
I20220824 03:16:06.127813 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-broadcast_matmul-344"
device_tag: "cuda"
scope_symbol_id: 1683
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model-batch_matmul-342/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_q.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-broadcast_matmul-344/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.128062 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-broadcast_matmul-344
I20220824 03:16:06.128154 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-broadcast_matmul_grad_b-345"
device_tag: "cuda"
scope_symbol_id: 1683
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model-batch_matmul-342/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_q-hierarchical_parallel_cast-303/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-broadcast_matmul_grad_b-345/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.128374 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-broadcast_matmul_grad_b-345
I20220824 03:16:06.128470 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-346"
device_tag: "cuda"
scope_symbol_id: 1631
loc: ""
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model-batch_matmul-343/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-346/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 2
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 03:16:06.128693 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-346
I20220824 03:16:06.128930 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-add_n-347"
device_tag: "cuda"
scope_symbol_id: 1683
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.head_q-broadcast_matmul-344/out_0"
      s: "model.head-broadcast_matmul-338/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-add_n-347/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.129117 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-add_n-347
I20220824 03:16:06.129218 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-broadcast_matmul-348"
device_tag: "cuda"
scope_symbol_id: 1697
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model-transpose-346/output_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_k.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-broadcast_matmul-348/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.129458 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-broadcast_matmul-348
I20220824 03:16:06.129550 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-broadcast_matmul_grad_b-349"
device_tag: "cuda"
scope_symbol_id: 1697
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model-transpose-346/output_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_k-hierarchical_parallel_cast-305/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-broadcast_matmul_grad_b-349/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.129772 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-broadcast_matmul_grad_b-349
I20220824 03:16:06.130023 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-add_n-350"
device_tag: "cuda"
scope_symbol_id: 1697
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.head_k-broadcast_matmul-348/out_0"
      s: "model.head_q-add_n-347/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-add_n-350/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.130232 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-add_n-350
I20220824 03:16:06.130498 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-constant-351"
device_tag: "cuda"
scope_symbol_id: 1709
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.ln_out-constant-351/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.130673 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-constant-351
I20220824 03:16:06.130910 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-constant-352"
device_tag: "cuda"
scope_symbol_id: 1709
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.ln_out-constant-352/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.131088 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-constant-352
I20220824 03:16:06.131215 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-layer_norm_param_grad-353"
device_tag: "cuda"
scope_symbol_id: 1709
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.head_k-add_n-350/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.ln_out-layer_norm-302/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.ln_out-layer_norm-302/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-301/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.ln_out-layer_norm_param_grad-353/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.ln_out-layer_norm_param_grad-353/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.131494 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-layer_norm_param_grad-353
I20220824 03:16:06.131634 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-layer_norm_grad-354"
device_tag: "cuda"
scope_symbol_id: 1709
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.head_k-add_n-350/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.ln_out.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.ln_out-layer_norm-302/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.ln_out-layer_norm-302/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-301/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.ln_out-layer_norm_grad-354/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.131953 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-layer_norm_grad-354
I20220824 03:16:06.132082 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-355"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.ln_out-layer_norm_grad-354/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-296/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-355/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.132323 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-355
I20220824 03:16:06.132457 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-356"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.ln_out-layer_norm_grad-354/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2-299/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-356/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.132761 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-356
I20220824 03:16:06.132961 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-sigmoid_v2_grad-357"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-355/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-298/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2_grad-357/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.133235 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-sigmoid_v2_grad-357
I20220824 03:16:06.133427 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-broadcast_matmul-358"
device_tag: "cuda"
scope_symbol_id: 1738
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-356/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-358/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.133785 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-broadcast_matmul-358
I20220824 03:16:06.133941 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-broadcast_matmul_grad_b-359"
device_tag: "cuda"
scope_symbol_id: 1738
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-356/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.value-hierarchical_parallel_cast-295/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul_grad_b-359/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.134301 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-broadcast_matmul_grad_b-359
I20220824 03:16:06.134472 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-broadcast_matmul-360"
device_tag: "cuda"
scope_symbol_id: 1745
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2_grad-357/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.134851 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-broadcast_matmul-360
I20220824 03:16:06.135007 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361"
device_tag: "cuda"
scope_symbol_id: 1745
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2_grad-357/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-297/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.135375 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361
I20220824 03:16:06.135913 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-square_grad-362"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-358/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-relu-293/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ffn-square_grad-362/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.136241 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-square_grad-362
I20220824 03:16:06.136444 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-relu_grad-363"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn-square_grad-362/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-relu-293/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ffn-relu_grad-363/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 03:16:06.136759 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-relu_grad-363
I20220824 03:16:06.136924 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-364"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-364/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.137265 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-364
I20220824 03:16:06.137418 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-365"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-365/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.137770 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-365
I20220824 03:16:06.137975 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-366"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-365/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-366/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.138360 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-366
I20220824 03:16:06.138531 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-367"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-288/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-367/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.138893 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-367
I20220824 03:16:06.139048 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-368"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-280/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-368/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.139398 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-368
I20220824 03:16:06.139562 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-369"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn-scalar_add-288/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-368/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-369/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.139942 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-369
I20220824 03:16:06.140146 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-broadcast_matmul-370"
device_tag: "cuda"
scope_symbol_id: 1780
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-relu_grad-363/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-370/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.140548 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-broadcast_matmul-370
I20220824 03:16:06.140710 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-broadcast_matmul_grad_b-371"
device_tag: "cuda"
scope_symbol_id: 1780
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-relu_grad-363/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.key-hierarchical_parallel_cast-291/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul_grad_b-371/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.141059 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-broadcast_matmul_grad_b-371
I20220824 03:16:06.141412 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-372"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-369/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-372/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.141741 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-372
I20220824 03:16:06.141898 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-373"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-372/out_0"
      s: "model.blocks.5.ffn-reduce_sum_like-366/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-373/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.142223 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-373
I20220824 03:16:06.142410 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-374"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-370/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-374/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.142782 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-374
I20220824 03:16:06.142939 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-375"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-370/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-375/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.143311 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-375
I20220824 03:16:06.143472 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-376"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-375/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-376/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.143911 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-376
I20220824 03:16:06.144076 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-377"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-374/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-364/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-377/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.144428 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-377
I20220824 03:16:06.144594 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-378"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-370/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-283/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-378/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.144937 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-378
I20220824 03:16:06.145088 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-379"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-370/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-280/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-379/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.145442 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-379
I20220824 03:16:06.145604 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-380"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn-scalar_add-283/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-379/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-380/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.146013 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-380
I20220824 03:16:06.146176 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-381"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-378/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-367/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-381/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.146531 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-381
I20220824 03:16:06.146732 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.time_shift-pad-382"
device_tag: "cuda"
scope_symbol_id: 1821
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-add_n-381/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-382/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.147089 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_shift-pad-382
I20220824 03:16:06.147246 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.time_shift-add_n-383"
device_tag: "cuda"
scope_symbol_id: 1821
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-382/y_0"
      s: "model.blocks.5.ffn-add_n-377/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.time_shift-add_n-383/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.147574 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_shift-add_n-383
I20220824 03:16:06.148056 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-constant-384"
device_tag: "cuda"
scope_symbol_id: 1828
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln2-constant-384/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.148346 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-constant-384
I20220824 03:16:06.148720 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-constant-385"
device_tag: "cuda"
scope_symbol_id: 1828
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln2-constant-385/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.149025 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-constant-385
I20220824 03:16:06.149204 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-layer_norm_param_grad-386"
device_tag: "cuda"
scope_symbol_id: 1828
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn.time_shift-add_n-383/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-278/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.5.ln2-layer_norm_param_grad-386/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.5.ln2-layer_norm_param_grad-386/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.149675 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-layer_norm_param_grad-386
I20220824 03:16:06.149856 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-layer_norm_grad-387"
device_tag: "cuda"
scope_symbol_id: 1828
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn.time_shift-add_n-383/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln2-layer_norm-279/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-278/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ln2-layer_norm_grad-387/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.150348 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-layer_norm_grad-387
I20220824 03:16:06.150521 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-add_n-388"
device_tag: "cuda"
scope_symbol_id: 1828
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ln2-layer_norm_grad-387/dx_0"
      s: "model.ln_out-layer_norm_grad-354/dx_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln2-add_n-388/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.150857 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-add_n-388
I20220824 03:16:06.151029 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-389"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-380/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-389/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.151374 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-389
I20220824 03:16:06.151527 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-390"
device_tag: "cuda"
scope_symbol_id: 1726
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-389/out_0"
      s: "model.blocks.5.ffn-reduce_sum_like-376/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-390/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.151878 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-390
I20220824 03:16:06.152083 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-broadcast_matmul-391"
device_tag: "cuda"
scope_symbol_id: 1853
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ln2-add_n-388/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul-391/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.152475 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-broadcast_matmul-391
I20220824 03:16:06.152633 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-broadcast_matmul_grad_b-392"
device_tag: "cuda"
scope_symbol_id: 1853
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ln2-add_n-388/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.output-hierarchical_parallel_cast-276/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul_grad_b-392/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.152990 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-broadcast_matmul_grad_b-392
I20220824 03:16:06.153332 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-sigmoid_v2_grad-393"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul-391/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-274/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.att-sigmoid_v2_grad-393/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.153656 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-sigmoid_v2_grad-393
I20220824 03:16:06.153841 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-broadcast_matmul-394"
device_tag: "cuda"
scope_symbol_id: 1869
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-sigmoid_v2_grad-393/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-394/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.154186 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-broadcast_matmul-394
I20220824 03:16:06.154321 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-broadcast_matmul_grad_b-395"
device_tag: "cuda"
scope_symbol_id: 1869
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-sigmoid_v2_grad-393/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.receptance-hierarchical_parallel_cast-273/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul_grad_b-395/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.154639 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-broadcast_matmul_grad_b-395
I20220824 03:16:06.154974 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-396"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-394/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-396/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.155267 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-396
I20220824 03:16:06.155400 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-397"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-394/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-397/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.155755 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-397
I20220824 03:16:06.155925 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-398"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-397/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-398/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.156301 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-398
I20220824 03:16:06.156471 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-399"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-394/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-266/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-399/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.156843 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-399
I20220824 03:16:06.157002 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-400"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-394/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-253/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-400/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.157366 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-400
I20220824 03:16:06.157531 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-401"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att-scalar_add-266/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-400/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-401/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.157914 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-401
I20220824 03:16:06.158104 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.time_shift-pad-402"
device_tag: "cuda"
scope_symbol_id: 1896
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-399/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-402/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.158454 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_shift-pad-402
I20220824 03:16:06.158620 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.time_shift-add_n-403"
device_tag: "cuda"
scope_symbol_id: 1896
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_shift-pad-402/y_0"
      s: "model.blocks.5.att-broadcast_mul-396/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.time_shift-add_n-403/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.158947 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_shift-add_n-403
I20220824 03:16:06.161904 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-constant-404"
device_tag: "cuda"
scope_symbol_id: 1903
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln1-constant-404/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.162324 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-constant-404
I20220824 03:16:06.162741 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-constant-405"
device_tag: "cuda"
scope_symbol_id: 1903
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln1-constant-405/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.163028 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-constant-405
I20220824 03:16:06.163208 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-layer_norm_param_grad-406"
device_tag: "cuda"
scope_symbol_id: 1903
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.att.time_shift-add_n-403/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-251/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.5.ln1-layer_norm_param_grad-406/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.5.ln1-layer_norm_param_grad-406/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.163677 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-layer_norm_param_grad-406
I20220824 03:16:06.163877 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-layer_norm_grad-407"
device_tag: "cuda"
scope_symbol_id: 1903
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.att.time_shift-add_n-403/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln1-layer_norm-252/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-251/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ln1-layer_norm_grad-407/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.164347 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-layer_norm_grad-407
I20220824 03:16:06.164511 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-add_n-408"
device_tag: "cuda"
scope_symbol_id: 1903
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ln1-layer_norm_grad-407/dx_0"
      s: "model.blocks.5.ln2-add_n-388/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln1-add_n-408/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.164846 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-add_n-408
I20220824 03:16:06.165010 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-409"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-reduce_sum_like-401/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-409/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.165339 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-409
I20220824 03:16:06.165490 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-410"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-409/out_0"
      s: "model.blocks.5.att-reduce_sum_like-398/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-410/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.165788 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-410
I20220824 03:16:06.165972 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-411"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-add_n-408/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-246/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-411/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.166291 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-411
I20220824 03:16:06.166440 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-412"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-add_n-408/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2-249/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-412/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.166800 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-412
I20220824 03:16:06.167246 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-sigmoid_v2_grad-413"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-411/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-248/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2_grad-413/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.167557 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-sigmoid_v2_grad-413
I20220824 03:16:06.167783 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-broadcast_matmul-414"
device_tag: "cuda"
scope_symbol_id: 1942
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-412/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-414/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.168177 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-broadcast_matmul-414
I20220824 03:16:06.168344 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-broadcast_matmul_grad_b-415"
device_tag: "cuda"
scope_symbol_id: 1942
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-412/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.value-hierarchical_parallel_cast-245/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul_grad_b-415/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.168704 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-broadcast_matmul_grad_b-415
I20220824 03:16:06.168881 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-broadcast_matmul-416"
device_tag: "cuda"
scope_symbol_id: 1949
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2_grad-413/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.169272 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-broadcast_matmul-416
I20220824 03:16:06.169427 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417"
device_tag: "cuda"
scope_symbol_id: 1949
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2_grad-413/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-247/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.169778 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417
I20220824 03:16:06.170239 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-square_grad-418"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-414/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-relu-243/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ffn-square_grad-418/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.170581 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-square_grad-418
I20220824 03:16:06.170775 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-relu_grad-419"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn-square_grad-418/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-relu-243/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ffn-relu_grad-419/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 03:16:06.171097 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-relu_grad-419
I20220824 03:16:06.171265 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-420"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-420/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.171633 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-420
I20220824 03:16:06.171800 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-421"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-421/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.172166 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-421
I20220824 03:16:06.172331 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-422"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-421/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-422/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.173254 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-422
I20220824 03:16:06.173431 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-423"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-238/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-423/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.173784 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-423
I20220824 03:16:06.173938 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-424"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-230/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-424/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.174293 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-424
I20220824 03:16:06.174460 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-425"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn-scalar_add-238/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-424/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-425/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.174840 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-425
I20220824 03:16:06.175038 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-broadcast_matmul-426"
device_tag: "cuda"
scope_symbol_id: 1984
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-relu_grad-419/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-426/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.175428 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-broadcast_matmul-426
I20220824 03:16:06.175591 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-broadcast_matmul_grad_b-427"
device_tag: "cuda"
scope_symbol_id: 1984
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-relu_grad-419/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.key-hierarchical_parallel_cast-241/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul_grad_b-427/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.175972 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-broadcast_matmul_grad_b-427
I20220824 03:16:06.176297 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-428"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-425/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-428/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.176633 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-428
I20220824 03:16:06.176791 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-429"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-428/out_0"
      s: "model.blocks.4.ffn-reduce_sum_like-422/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-429/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.177103 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-429
I20220824 03:16:06.177287 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-430"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-426/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-430/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.177649 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-430
I20220824 03:16:06.177803 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-431"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-426/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-431/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.178154 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-431
I20220824 03:16:06.178316 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-432"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-431/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-432/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.178727 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-432
I20220824 03:16:06.178890 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-433"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-430/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-420/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-433/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.179241 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-433
I20220824 03:16:06.179394 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-434"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-426/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-233/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-434/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.179728 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-434
I20220824 03:16:06.179883 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-435"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-426/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-230/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-435/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.180281 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-435
I20220824 03:16:06.180440 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-436"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn-scalar_add-233/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-435/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-436/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.180832 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-436
I20220824 03:16:06.180985 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-437"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-434/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-423/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-437/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.181321 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-437
I20220824 03:16:06.181514 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.time_shift-pad-438"
device_tag: "cuda"
scope_symbol_id: 2025
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-add_n-437/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-438/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.181869 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_shift-pad-438
I20220824 03:16:06.182029 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.time_shift-add_n-439"
device_tag: "cuda"
scope_symbol_id: 2025
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-438/y_0"
      s: "model.blocks.4.ffn-add_n-433/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.time_shift-add_n-439/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.182345 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_shift-add_n-439
I20220824 03:16:06.182690 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-constant-440"
device_tag: "cuda"
scope_symbol_id: 2032
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln2-constant-440/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.182971 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-constant-440
I20220824 03:16:06.183288 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-constant-441"
device_tag: "cuda"
scope_symbol_id: 2032
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln2-constant-441/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.183568 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-constant-441
I20220824 03:16:06.183749 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-layer_norm_param_grad-442"
device_tag: "cuda"
scope_symbol_id: 2032
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn.time_shift-add_n-439/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-228/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.4.ln2-layer_norm_param_grad-442/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.4.ln2-layer_norm_param_grad-442/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.184233 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-layer_norm_param_grad-442
I20220824 03:16:06.184417 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-layer_norm_grad-443"
device_tag: "cuda"
scope_symbol_id: 2032
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn.time_shift-add_n-439/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln2-layer_norm-229/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-228/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ln2-layer_norm_grad-443/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.184914 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-layer_norm_grad-443
I20220824 03:16:06.185081 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-add_n-444"
device_tag: "cuda"
scope_symbol_id: 2032
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ln2-layer_norm_grad-443/dx_0"
      s: "model.blocks.5.ln1-add_n-408/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln2-add_n-444/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.185416 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-add_n-444
I20220824 03:16:06.185590 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-445"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-436/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-445/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.185930 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-445
I20220824 03:16:06.186091 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-446"
device_tag: "cuda"
scope_symbol_id: 1929
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-445/out_0"
      s: "model.blocks.4.ffn-reduce_sum_like-432/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-446/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.186434 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-446
I20220824 03:16:06.186633 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-broadcast_matmul-447"
device_tag: "cuda"
scope_symbol_id: 2057
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ln2-add_n-444/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul-447/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.187021 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-broadcast_matmul-447
I20220824 03:16:06.187183 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-broadcast_matmul_grad_b-448"
device_tag: "cuda"
scope_symbol_id: 2057
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ln2-add_n-444/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.output-hierarchical_parallel_cast-226/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul_grad_b-448/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.187544 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-broadcast_matmul_grad_b-448
I20220824 03:16:06.187894 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-sigmoid_v2_grad-449"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul-447/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-224/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.att-sigmoid_v2_grad-449/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.188215 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-sigmoid_v2_grad-449
I20220824 03:16:06.188395 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-broadcast_matmul-450"
device_tag: "cuda"
scope_symbol_id: 2073
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-sigmoid_v2_grad-449/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-450/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.188776 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-broadcast_matmul-450
I20220824 03:16:06.188936 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-broadcast_matmul_grad_b-451"
device_tag: "cuda"
scope_symbol_id: 2073
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-sigmoid_v2_grad-449/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.receptance-hierarchical_parallel_cast-223/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul_grad_b-451/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.189294 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-broadcast_matmul_grad_b-451
I20220824 03:16:06.189631 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-452"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-450/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-452/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.189977 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-452
I20220824 03:16:06.190131 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-453"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-450/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-453/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.190465 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-453
I20220824 03:16:06.190625 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-454"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-453/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-454/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.190992 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-454
I20220824 03:16:06.191155 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-455"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-450/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-216/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-455/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.191511 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-455
I20220824 03:16:06.191658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-456"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-450/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-203/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-456/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.192035 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-456
I20220824 03:16:06.192193 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-457"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att-scalar_add-216/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-456/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-457/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.192574 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-457
I20220824 03:16:06.192754 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.time_shift-pad-458"
device_tag: "cuda"
scope_symbol_id: 2100
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-455/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-458/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.193085 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_shift-pad-458
I20220824 03:16:06.193236 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.time_shift-add_n-459"
device_tag: "cuda"
scope_symbol_id: 2100
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_shift-pad-458/y_0"
      s: "model.blocks.4.att-broadcast_mul-452/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.time_shift-add_n-459/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.193534 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_shift-add_n-459
I20220824 03:16:06.193868 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-constant-460"
device_tag: "cuda"
scope_symbol_id: 2107
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln1-constant-460/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.194157 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-constant-460
I20220824 03:16:06.194487 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-constant-461"
device_tag: "cuda"
scope_symbol_id: 2107
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln1-constant-461/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.194777 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-constant-461
I20220824 03:16:06.194945 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-layer_norm_param_grad-462"
device_tag: "cuda"
scope_symbol_id: 2107
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.att.time_shift-add_n-459/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-201/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.4.ln1-layer_norm_param_grad-462/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.4.ln1-layer_norm_param_grad-462/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.195427 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-layer_norm_param_grad-462
I20220824 03:16:06.195611 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-layer_norm_grad-463"
device_tag: "cuda"
scope_symbol_id: 2107
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.att.time_shift-add_n-459/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln1-layer_norm-202/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-201/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ln1-layer_norm_grad-463/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.196125 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-layer_norm_grad-463
I20220824 03:16:06.196302 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-add_n-464"
device_tag: "cuda"
scope_symbol_id: 2107
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ln1-layer_norm_grad-463/dx_0"
      s: "model.blocks.4.ln2-add_n-444/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln1-add_n-464/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.196648 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-add_n-464
I20220824 03:16:06.196817 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-465"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-reduce_sum_like-457/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-465/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.197139 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-465
I20220824 03:16:06.197276 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-466"
device_tag: "cuda"
scope_symbol_id: 2068
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-465/out_0"
      s: "model.blocks.4.att-reduce_sum_like-454/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-466/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.197556 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-466
I20220824 03:16:06.197743 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-467"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-add_n-464/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-196/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-467/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.198063 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-467
I20220824 03:16:06.198204 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-468"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-add_n-464/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2-199/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-468/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.198513 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-468
I20220824 03:16:06.198658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-sigmoid_v2_grad-469"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-467/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-198/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2_grad-469/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.198882 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-sigmoid_v2_grad-469
I20220824 03:16:06.199016 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-broadcast_matmul-470"
device_tag: "cuda"
scope_symbol_id: 2146
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-468/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-470/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.199348 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-broadcast_matmul-470
I20220824 03:16:06.199455 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-broadcast_matmul_grad_b-471"
device_tag: "cuda"
scope_symbol_id: 2146
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-468/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.value-hierarchical_parallel_cast-195/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul_grad_b-471/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.199681 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-broadcast_matmul_grad_b-471
I20220824 03:16:06.199790 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-broadcast_matmul-472"
device_tag: "cuda"
scope_symbol_id: 2153
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2_grad-469/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.200031 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-broadcast_matmul-472
I20220824 03:16:06.200137 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473"
device_tag: "cuda"
scope_symbol_id: 2153
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2_grad-469/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-197/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.200350 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473
I20220824 03:16:06.200700 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-square_grad-474"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-470/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-relu-193/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ffn-square_grad-474/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.200915 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-square_grad-474
I20220824 03:16:06.201035 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-relu_grad-475"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn-square_grad-474/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-relu-193/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ffn-relu_grad-475/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 03:16:06.201287 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-relu_grad-475
I20220824 03:16:06.201380 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-476"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-476/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.201604 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-476
I20220824 03:16:06.201705 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-477"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-477/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.201915 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-477
I20220824 03:16:06.202009 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-478"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-477/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-478/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.202235 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-478
I20220824 03:16:06.202350 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-479"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-188/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-479/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.202575 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-479
I20220824 03:16:06.202668 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-480"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-180/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-480/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.202879 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-480
I20220824 03:16:06.202971 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-481"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn-scalar_add-188/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-480/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-481/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.203189 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-481
I20220824 03:16:06.203317 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-broadcast_matmul-482"
device_tag: "cuda"
scope_symbol_id: 2188
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-relu_grad-475/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-482/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.203549 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-broadcast_matmul-482
I20220824 03:16:06.203653 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-broadcast_matmul_grad_b-483"
device_tag: "cuda"
scope_symbol_id: 2188
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-relu_grad-475/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.key-hierarchical_parallel_cast-191/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul_grad_b-483/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.203876 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-broadcast_matmul_grad_b-483
I20220824 03:16:06.204097 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-484"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-481/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-484/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.204409 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-484
I20220824 03:16:06.204514 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-485"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-484/out_0"
      s: "model.blocks.3.ffn-reduce_sum_like-478/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-485/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.204706 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-485
I20220824 03:16:06.204824 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-486"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-482/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-486/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.205041 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-486
I20220824 03:16:06.205145 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-487"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-482/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-487/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.205350 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-487
I20220824 03:16:06.205459 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-488"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-487/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-488/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.205703 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-488
I20220824 03:16:06.205794 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-489"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-486/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-476/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-489/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.206007 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-489
I20220824 03:16:06.206107 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-490"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-482/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-183/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-490/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.206303 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-490
I20220824 03:16:06.206399 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-491"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-482/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-180/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-491/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.206602 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-491
I20220824 03:16:06.206709 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-492"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn-scalar_add-183/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-491/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-492/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.206948 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-492
I20220824 03:16:06.207036 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-493"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-490/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-479/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-493/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.207238 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-493
I20220824 03:16:06.207348 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.time_shift-pad-494"
device_tag: "cuda"
scope_symbol_id: 2229
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-add_n-493/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-494/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.207564 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_shift-pad-494
I20220824 03:16:06.207669 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.time_shift-add_n-495"
device_tag: "cuda"
scope_symbol_id: 2229
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-494/y_0"
      s: "model.blocks.3.ffn-add_n-489/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.time_shift-add_n-495/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.207854 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_shift-add_n-495
I20220824 03:16:06.208099 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-constant-496"
device_tag: "cuda"
scope_symbol_id: 2236
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln2-constant-496/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.208273 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-constant-496
I20220824 03:16:06.208492 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-constant-497"
device_tag: "cuda"
scope_symbol_id: 2236
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln2-constant-497/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.208678 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-constant-497
I20220824 03:16:06.208789 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-layer_norm_param_grad-498"
device_tag: "cuda"
scope_symbol_id: 2236
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn.time_shift-add_n-495/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-178/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.3.ln2-layer_norm_param_grad-498/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.3.ln2-layer_norm_param_grad-498/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.209085 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-layer_norm_param_grad-498
I20220824 03:16:06.209203 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-layer_norm_grad-499"
device_tag: "cuda"
scope_symbol_id: 2236
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn.time_shift-add_n-495/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln2-layer_norm-179/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-178/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ln2-layer_norm_grad-499/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.209508 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-layer_norm_grad-499
I20220824 03:16:06.209614 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-add_n-500"
device_tag: "cuda"
scope_symbol_id: 2236
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ln2-layer_norm_grad-499/dx_0"
      s: "model.blocks.4.ln1-add_n-464/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln2-add_n-500/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.209831 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-add_n-500
I20220824 03:16:06.209946 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-501"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-492/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-501/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.210155 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-501
I20220824 03:16:06.210258 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-502"
device_tag: "cuda"
scope_symbol_id: 2133
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-501/out_0"
      s: "model.blocks.3.ffn-reduce_sum_like-488/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-502/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.210456 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-502
I20220824 03:16:06.210573 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-broadcast_matmul-503"
device_tag: "cuda"
scope_symbol_id: 2261
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ln2-add_n-500/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul-503/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.210808 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-broadcast_matmul-503
I20220824 03:16:06.210911 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-broadcast_matmul_grad_b-504"
device_tag: "cuda"
scope_symbol_id: 2261
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ln2-add_n-500/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.output-hierarchical_parallel_cast-176/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul_grad_b-504/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.211150 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-broadcast_matmul_grad_b-504
I20220824 03:16:06.211535 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-sigmoid_v2_grad-505"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul-503/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-174/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.att-sigmoid_v2_grad-505/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.211859 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-sigmoid_v2_grad-505
I20220824 03:16:06.211973 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-broadcast_matmul-506"
device_tag: "cuda"
scope_symbol_id: 2277
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-sigmoid_v2_grad-505/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-506/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.212208 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-broadcast_matmul-506
I20220824 03:16:06.212322 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-broadcast_matmul_grad_b-507"
device_tag: "cuda"
scope_symbol_id: 2277
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-sigmoid_v2_grad-505/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.receptance-hierarchical_parallel_cast-173/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul_grad_b-507/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.212559 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-broadcast_matmul_grad_b-507
I20220824 03:16:06.212805 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-508"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-506/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-508/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.213048 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-508
I20220824 03:16:06.213165 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-509"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-506/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-509/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.213372 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-509
I20220824 03:16:06.213477 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-510"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-509/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-510/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.213717 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-510
I20220824 03:16:06.213843 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-511"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-506/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-166/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-511/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.214073 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-511
I20220824 03:16:06.214179 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-512"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-506/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-153/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-512/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.214398 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-512
I20220824 03:16:06.214517 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-513"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att-scalar_add-166/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-512/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-513/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.214772 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-513
I20220824 03:16:06.214910 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.time_shift-pad-514"
device_tag: "cuda"
scope_symbol_id: 2304
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-511/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-514/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.215133 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_shift-pad-514
I20220824 03:16:06.215245 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.time_shift-add_n-515"
device_tag: "cuda"
scope_symbol_id: 2304
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_shift-pad-514/y_0"
      s: "model.blocks.3.att-broadcast_mul-508/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.time_shift-add_n-515/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.215447 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_shift-add_n-515
I20220824 03:16:06.215683 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-constant-516"
device_tag: "cuda"
scope_symbol_id: 2311
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln1-constant-516/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.215867 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-constant-516
I20220824 03:16:06.216089 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-constant-517"
device_tag: "cuda"
scope_symbol_id: 2311
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln1-constant-517/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.216259 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-constant-517
I20220824 03:16:06.216372 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-layer_norm_param_grad-518"
device_tag: "cuda"
scope_symbol_id: 2311
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.att.time_shift-add_n-515/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-151/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.3.ln1-layer_norm_param_grad-518/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.3.ln1-layer_norm_param_grad-518/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.216675 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-layer_norm_param_grad-518
I20220824 03:16:06.216794 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-layer_norm_grad-519"
device_tag: "cuda"
scope_symbol_id: 2311
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.att.time_shift-add_n-515/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln1-layer_norm-152/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-151/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ln1-layer_norm_grad-519/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.217093 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-layer_norm_grad-519
I20220824 03:16:06.217190 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-add_n-520"
device_tag: "cuda"
scope_symbol_id: 2311
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ln1-layer_norm_grad-519/dx_0"
      s: "model.blocks.3.ln2-add_n-500/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln1-add_n-520/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.217414 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-add_n-520
I20220824 03:16:06.217531 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-521"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-reduce_sum_like-513/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-521/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.217747 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-521
I20220824 03:16:06.217836 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-522"
device_tag: "cuda"
scope_symbol_id: 2272
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-521/out_0"
      s: "model.blocks.3.att-reduce_sum_like-510/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-522/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.218034 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-522
I20220824 03:16:06.218161 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-523"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-add_n-520/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-146/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-523/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.218391 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-523
I20220824 03:16:06.218524 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-524"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-add_n-520/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2-149/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-524/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.218842 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-524
I20220824 03:16:06.219017 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-sigmoid_v2_grad-525"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-523/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-148/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2_grad-525/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.219326 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-sigmoid_v2_grad-525
I20220824 03:16:06.219506 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-broadcast_matmul-526"
device_tag: "cuda"
scope_symbol_id: 2350
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-524/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-526/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.219853 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-broadcast_matmul-526
I20220824 03:16:06.219977 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-broadcast_matmul_grad_b-527"
device_tag: "cuda"
scope_symbol_id: 2350
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-524/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.value-hierarchical_parallel_cast-145/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul_grad_b-527/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.220211 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-broadcast_matmul_grad_b-527
I20220824 03:16:06.220326 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-broadcast_matmul-528"
device_tag: "cuda"
scope_symbol_id: 2357
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2_grad-525/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.220569 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-broadcast_matmul-528
I20220824 03:16:06.220706 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529"
device_tag: "cuda"
scope_symbol_id: 2357
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2_grad-525/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-147/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.220942 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529
I20220824 03:16:06.221261 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-square_grad-530"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-526/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-relu-143/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ffn-square_grad-530/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.221469 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-square_grad-530
I20220824 03:16:06.221597 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-relu_grad-531"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn-square_grad-530/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-relu-143/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ffn-relu_grad-531/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 03:16:06.221793 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-relu_grad-531
I20220824 03:16:06.221904 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-532"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-532/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.222137 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-532
I20220824 03:16:06.222223 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-533"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-533/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.222429 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-533
I20220824 03:16:06.222532 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-534"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-533/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-534/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.222762 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-534
I20220824 03:16:06.222872 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-535"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-138/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-535/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.223090 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-535
I20220824 03:16:06.223174 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-536"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-130/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-536/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.223387 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-536
I20220824 03:16:06.223497 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-537"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn-scalar_add-138/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-536/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-537/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.223752 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-537
I20220824 03:16:06.223889 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-broadcast_matmul-538"
device_tag: "cuda"
scope_symbol_id: 2392
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-relu_grad-531/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-538/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.224123 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-broadcast_matmul-538
I20220824 03:16:06.224251 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-broadcast_matmul_grad_b-539"
device_tag: "cuda"
scope_symbol_id: 2392
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-relu_grad-531/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.key-hierarchical_parallel_cast-141/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul_grad_b-539/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.224543 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-broadcast_matmul_grad_b-539
I20220824 03:16:06.224798 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-540"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-537/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-540/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.225020 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-540
I20220824 03:16:06.225145 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-541"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-540/out_0"
      s: "model.blocks.2.ffn-reduce_sum_like-534/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-541/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.225342 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-541
I20220824 03:16:06.225464 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-542"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-538/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-542/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.225695 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-542
I20220824 03:16:06.225786 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-543"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-538/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-543/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.226011 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-543
I20220824 03:16:06.226119 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-544"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-543/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-544/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.226369 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-544
I20220824 03:16:06.226475 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-545"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-542/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-532/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-545/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.226699 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-545
I20220824 03:16:06.226794 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-546"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-538/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-133/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-546/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.226992 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-546
I20220824 03:16:06.227090 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-547"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-538/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-130/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-547/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.227299 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-547
I20220824 03:16:06.227411 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-548"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn-scalar_add-133/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-547/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-548/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.227656 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-548
I20220824 03:16:06.227767 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-549"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-546/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-535/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-549/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.227982 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-549
I20220824 03:16:06.228112 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.time_shift-pad-550"
device_tag: "cuda"
scope_symbol_id: 2433
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-add_n-549/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-550/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.228322 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_shift-pad-550
I20220824 03:16:06.228425 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.time_shift-add_n-551"
device_tag: "cuda"
scope_symbol_id: 2433
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-550/y_0"
      s: "model.blocks.2.ffn-add_n-545/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.time_shift-add_n-551/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.228600 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_shift-add_n-551
I20220824 03:16:06.228853 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-constant-552"
device_tag: "cuda"
scope_symbol_id: 2440
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln2-constant-552/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.229032 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-constant-552
I20220824 03:16:06.229262 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-constant-553"
device_tag: "cuda"
scope_symbol_id: 2440
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln2-constant-553/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.229427 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-constant-553
I20220824 03:16:06.229523 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-layer_norm_param_grad-554"
device_tag: "cuda"
scope_symbol_id: 2440
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn.time_shift-add_n-551/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-128/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.2.ln2-layer_norm_param_grad-554/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.2.ln2-layer_norm_param_grad-554/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.229817 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-layer_norm_param_grad-554
I20220824 03:16:06.229933 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-layer_norm_grad-555"
device_tag: "cuda"
scope_symbol_id: 2440
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn.time_shift-add_n-551/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln2-layer_norm-129/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-128/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ln2-layer_norm_grad-555/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.230248 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-layer_norm_grad-555
I20220824 03:16:06.230362 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-add_n-556"
device_tag: "cuda"
scope_symbol_id: 2440
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ln2-layer_norm_grad-555/dx_0"
      s: "model.blocks.3.ln1-add_n-520/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln2-add_n-556/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.230581 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-add_n-556
I20220824 03:16:06.230764 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-557"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-548/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-557/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.231586 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-557
I20220824 03:16:06.231961 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-558"
device_tag: "cuda"
scope_symbol_id: 2337
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-557/out_0"
      s: "model.blocks.2.ffn-reduce_sum_like-544/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-558/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.232633 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-558
I20220824 03:16:06.233007 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-broadcast_matmul-559"
device_tag: "cuda"
scope_symbol_id: 2465
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ln2-add_n-556/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul-559/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.233775 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-broadcast_matmul-559
I20220824 03:16:06.234100 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-broadcast_matmul_grad_b-560"
device_tag: "cuda"
scope_symbol_id: 2465
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ln2-add_n-556/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.output-hierarchical_parallel_cast-126/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul_grad_b-560/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.234783 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-broadcast_matmul_grad_b-560
I20220824 03:16:06.235316 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-sigmoid_v2_grad-561"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul-559/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-124/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.att-sigmoid_v2_grad-561/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.235953 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-sigmoid_v2_grad-561
I20220824 03:16:06.236277 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-broadcast_matmul-562"
device_tag: "cuda"
scope_symbol_id: 2481
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-sigmoid_v2_grad-561/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-562/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.237007 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-broadcast_matmul-562
I20220824 03:16:06.237300 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-broadcast_matmul_grad_b-563"
device_tag: "cuda"
scope_symbol_id: 2481
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-sigmoid_v2_grad-561/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.receptance-hierarchical_parallel_cast-123/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul_grad_b-563/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.237958 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-broadcast_matmul_grad_b-563
I20220824 03:16:06.238361 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-564"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-562/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-564/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.238620 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-564
I20220824 03:16:06.238720 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-565"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-562/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-565/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.238960 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-565
I20220824 03:16:06.239086 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-566"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-565/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-566/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.239356 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-566
I20220824 03:16:06.239465 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-567"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-562/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-116/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-567/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.239727 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-567
I20220824 03:16:06.239828 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-568"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-562/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-103/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-568/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.240064 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-568
I20220824 03:16:06.240185 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-569"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att-scalar_add-116/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-568/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-569/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.240471 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-569
I20220824 03:16:06.240597 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.time_shift-pad-570"
device_tag: "cuda"
scope_symbol_id: 2508
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-567/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-570/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.240844 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_shift-pad-570
I20220824 03:16:06.240950 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.time_shift-add_n-571"
device_tag: "cuda"
scope_symbol_id: 2508
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_shift-pad-570/y_0"
      s: "model.blocks.2.att-broadcast_mul-564/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.time_shift-add_n-571/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.241176 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_shift-add_n-571
I20220824 03:16:06.241429 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-constant-572"
device_tag: "cuda"
scope_symbol_id: 2515
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln1-constant-572/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.241632 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-constant-572
I20220824 03:16:06.241887 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-constant-573"
device_tag: "cuda"
scope_symbol_id: 2515
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln1-constant-573/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.242079 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-constant-573
I20220824 03:16:06.242208 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-layer_norm_param_grad-574"
device_tag: "cuda"
scope_symbol_id: 2515
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.att.time_shift-add_n-571/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-101/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.2.ln1-layer_norm_param_grad-574/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.2.ln1-layer_norm_param_grad-574/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.242564 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-layer_norm_param_grad-574
I20220824 03:16:06.242686 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-layer_norm_grad-575"
device_tag: "cuda"
scope_symbol_id: 2515
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.att.time_shift-add_n-571/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln1-layer_norm-102/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-101/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ln1-layer_norm_grad-575/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.243028 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-layer_norm_grad-575
I20220824 03:16:06.243137 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-add_n-576"
device_tag: "cuda"
scope_symbol_id: 2515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ln1-layer_norm_grad-575/dx_0"
      s: "model.blocks.2.ln2-add_n-556/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln1-add_n-576/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.243388 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-add_n-576
I20220824 03:16:06.243522 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-577"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-reduce_sum_like-569/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-577/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.243770 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-577
I20220824 03:16:06.243894 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-578"
device_tag: "cuda"
scope_symbol_id: 2476
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-577/out_0"
      s: "model.blocks.2.att-reduce_sum_like-566/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-578/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.244112 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-578
I20220824 03:16:06.244256 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-579"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-add_n-576/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-96/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-579/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.244494 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-579
I20220824 03:16:06.244593 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-580"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-add_n-576/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2-99/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-580/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.244794 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-580
I20220824 03:16:06.244915 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-sigmoid_v2_grad-581"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-579/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-98/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2_grad-581/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.245110 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-sigmoid_v2_grad-581
I20220824 03:16:06.245230 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-broadcast_matmul-582"
device_tag: "cuda"
scope_symbol_id: 2554
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-580/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-582/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.245527 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-broadcast_matmul-582
I20220824 03:16:06.245635 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-broadcast_matmul_grad_b-583"
device_tag: "cuda"
scope_symbol_id: 2554
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-580/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.value-hierarchical_parallel_cast-95/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul_grad_b-583/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.245918 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-broadcast_matmul_grad_b-583
I20220824 03:16:06.246052 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-broadcast_matmul-584"
device_tag: "cuda"
scope_symbol_id: 2561
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2_grad-581/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.246362 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-broadcast_matmul-584
I20220824 03:16:06.246479 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585"
device_tag: "cuda"
scope_symbol_id: 2561
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2_grad-581/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-97/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.246752 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585
I20220824 03:16:06.247095 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-square_grad-586"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-582/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-relu-93/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ffn-square_grad-586/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.247361 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-square_grad-586
I20220824 03:16:06.247491 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-relu_grad-587"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn-square_grad-586/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-relu-93/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ffn-relu_grad-587/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 03:16:06.247742 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-relu_grad-587
I20220824 03:16:06.247864 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-588"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-588/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.248144 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-588
I20220824 03:16:06.248246 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-589"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-589/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.248528 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-589
I20220824 03:16:06.248658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-590"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-589/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-590/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.249409 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-590
I20220824 03:16:06.249536 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-591"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-88/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-591/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.249761 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-591
I20220824 03:16:06.249861 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-592"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-80/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-592/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.250070 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-592
I20220824 03:16:06.250175 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-593"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn-scalar_add-88/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-592/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-593/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.250406 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-593
I20220824 03:16:06.250530 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-broadcast_matmul-594"
device_tag: "cuda"
scope_symbol_id: 2596
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-relu_grad-587/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-594/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.250766 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-broadcast_matmul-594
I20220824 03:16:06.250869 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-broadcast_matmul_grad_b-595"
device_tag: "cuda"
scope_symbol_id: 2596
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-relu_grad-587/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.key-hierarchical_parallel_cast-91/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul_grad_b-595/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.251085 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-broadcast_matmul_grad_b-595
I20220824 03:16:06.251307 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-596"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-593/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-596/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.251515 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-596
I20220824 03:16:06.251603 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-597"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-596/out_0"
      s: "model.blocks.1.ffn-reduce_sum_like-590/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-597/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.251794 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-597
I20220824 03:16:06.251919 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-598"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-594/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-598/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.252136 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-598
I20220824 03:16:06.252221 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-599"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-594/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-599/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.252431 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-599
I20220824 03:16:06.252535 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-600"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-599/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-600/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.252779 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-600
I20220824 03:16:06.252871 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-601"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-598/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-588/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-601/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.253077 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-601
I20220824 03:16:06.253178 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-602"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-594/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-83/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-602/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.253363 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-602
I20220824 03:16:06.253458 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-603"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-594/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-80/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-603/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.253670 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-603
I20220824 03:16:06.253774 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-604"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn-scalar_add-83/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-603/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-604/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.254016 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-604
I20220824 03:16:06.254122 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-605"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-602/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-591/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-605/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.254323 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-605
I20220824 03:16:06.254442 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.time_shift-pad-606"
device_tag: "cuda"
scope_symbol_id: 2637
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-add_n-605/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-606/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.254647 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_shift-pad-606
I20220824 03:16:06.254735 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.time_shift-add_n-607"
device_tag: "cuda"
scope_symbol_id: 2637
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-606/y_0"
      s: "model.blocks.1.ffn-add_n-601/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.time_shift-add_n-607/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.254921 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_shift-add_n-607
I20220824 03:16:06.255172 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-constant-608"
device_tag: "cuda"
scope_symbol_id: 2644
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln2-constant-608/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.255348 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-constant-608
I20220824 03:16:06.255576 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-constant-609"
device_tag: "cuda"
scope_symbol_id: 2644
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln2-constant-609/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.255751 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-constant-609
I20220824 03:16:06.255861 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-layer_norm_param_grad-610"
device_tag: "cuda"
scope_symbol_id: 2644
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn.time_shift-add_n-607/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-78/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.1.ln2-layer_norm_param_grad-610/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.1.ln2-layer_norm_param_grad-610/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.256147 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-layer_norm_param_grad-610
I20220824 03:16:06.256266 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-layer_norm_grad-611"
device_tag: "cuda"
scope_symbol_id: 2644
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn.time_shift-add_n-607/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln2-layer_norm-79/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-78/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ln2-layer_norm_grad-611/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.256567 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-layer_norm_grad-611
I20220824 03:16:06.256659 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-add_n-612"
device_tag: "cuda"
scope_symbol_id: 2644
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ln2-layer_norm_grad-611/dx_0"
      s: "model.blocks.2.ln1-add_n-576/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln2-add_n-612/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.256880 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-add_n-612
I20220824 03:16:06.257000 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-613"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-604/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-613/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.257206 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-613
I20220824 03:16:06.257303 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-614"
device_tag: "cuda"
scope_symbol_id: 2541
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-613/out_0"
      s: "model.blocks.1.ffn-reduce_sum_like-600/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-614/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.257503 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-614
I20220824 03:16:06.257617 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-broadcast_matmul-615"
device_tag: "cuda"
scope_symbol_id: 2669
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ln2-add_n-612/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul-615/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.257846 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-broadcast_matmul-615
I20220824 03:16:06.257953 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-broadcast_matmul_grad_b-616"
device_tag: "cuda"
scope_symbol_id: 2669
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ln2-add_n-612/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.output-hierarchical_parallel_cast-76/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul_grad_b-616/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.258179 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-broadcast_matmul_grad_b-616
I20220824 03:16:06.258428 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-sigmoid_v2_grad-617"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul-615/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-74/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.att-sigmoid_v2_grad-617/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.258635 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-sigmoid_v2_grad-617
I20220824 03:16:06.258749 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-broadcast_matmul-618"
device_tag: "cuda"
scope_symbol_id: 2685
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-sigmoid_v2_grad-617/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-618/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.258980 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-broadcast_matmul-618
I20220824 03:16:06.259107 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-broadcast_matmul_grad_b-619"
device_tag: "cuda"
scope_symbol_id: 2685
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-sigmoid_v2_grad-617/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.receptance-hierarchical_parallel_cast-73/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul_grad_b-619/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.259403 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-broadcast_matmul_grad_b-619
I20220824 03:16:06.259658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-620"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-618/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-620/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.259900 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-620
I20220824 03:16:06.260010 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-621"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-618/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-621/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.260244 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-621
I20220824 03:16:06.260356 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-622"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-621/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-622/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.260598 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-622
I20220824 03:16:06.260695 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-623"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-618/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-66/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-623/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.260924 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-623
I20220824 03:16:06.261024 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-624"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-618/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-53/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-624/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.261236 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-624
I20220824 03:16:06.261345 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-625"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att-scalar_add-66/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-624/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-625/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.261590 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-625
I20220824 03:16:06.261714 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.time_shift-pad-626"
device_tag: "cuda"
scope_symbol_id: 2712
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-623/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-626/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.261935 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_shift-pad-626
I20220824 03:16:06.262039 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.time_shift-add_n-627"
device_tag: "cuda"
scope_symbol_id: 2712
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_shift-pad-626/y_0"
      s: "model.blocks.1.att-broadcast_mul-620/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.time_shift-add_n-627/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.262218 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_shift-add_n-627
I20220824 03:16:06.262463 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-constant-628"
device_tag: "cuda"
scope_symbol_id: 2719
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln1-constant-628/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.262642 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-constant-628
I20220824 03:16:06.262873 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-constant-629"
device_tag: "cuda"
scope_symbol_id: 2719
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln1-constant-629/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.263036 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-constant-629
I20220824 03:16:06.263147 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-layer_norm_param_grad-630"
device_tag: "cuda"
scope_symbol_id: 2719
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.att.time_shift-add_n-627/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-51/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.1.ln1-layer_norm_param_grad-630/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.1.ln1-layer_norm_param_grad-630/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.263450 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-layer_norm_param_grad-630
I20220824 03:16:06.263567 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-layer_norm_grad-631"
device_tag: "cuda"
scope_symbol_id: 2719
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.att.time_shift-add_n-627/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln1-layer_norm-52/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-51/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ln1-layer_norm_grad-631/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.263891 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-layer_norm_grad-631
I20220824 03:16:06.264007 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-add_n-632"
device_tag: "cuda"
scope_symbol_id: 2719
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ln1-layer_norm_grad-631/dx_0"
      s: "model.blocks.1.ln2-add_n-612/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln1-add_n-632/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.264227 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-add_n-632
I20220824 03:16:06.264344 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-633"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-reduce_sum_like-625/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-633/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.264554 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-633
I20220824 03:16:06.264658 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-634"
device_tag: "cuda"
scope_symbol_id: 2680
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-633/out_0"
      s: "model.blocks.1.att-reduce_sum_like-622/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-634/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.264847 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-634
I20220824 03:16:06.264978 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-635"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-add_n-632/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-46/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-635/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.265189 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-635
I20220824 03:16:06.265286 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-636"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-add_n-632/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2-49/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-636/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.265511 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-636
I20220824 03:16:06.265632 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-sigmoid_v2_grad-637"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-635/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-48/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2_grad-637/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.265830 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-sigmoid_v2_grad-637
I20220824 03:16:06.265951 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-broadcast_matmul-638"
device_tag: "cuda"
scope_symbol_id: 2758
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-636/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-638/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.266185 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-broadcast_matmul-638
I20220824 03:16:06.266294 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-broadcast_matmul_grad_b-639"
device_tag: "cuda"
scope_symbol_id: 2758
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-636/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.value-hierarchical_parallel_cast-45/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul_grad_b-639/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.266510 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-broadcast_matmul_grad_b-639
I20220824 03:16:06.266613 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-broadcast_matmul-640"
device_tag: "cuda"
scope_symbol_id: 2765
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2_grad-637/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.266849 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-broadcast_matmul-640
I20220824 03:16:06.266937 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641"
device_tag: "cuda"
scope_symbol_id: 2765
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2_grad-637/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-47/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.267148 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641
I20220824 03:16:06.267484 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-square_grad-642"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-638/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-relu-43/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ffn-square_grad-642/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.267688 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-square_grad-642
I20220824 03:16:06.267818 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-relu_grad-643"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn-square_grad-642/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-relu-43/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ffn-relu_grad-643/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 03:16:06.268010 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-relu_grad-643
I20220824 03:16:06.268115 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-644"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-644/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.268316 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-644
I20220824 03:16:06.268426 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-645"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-645/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.268641 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-645
I20220824 03:16:06.268751 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-646"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-645/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-646/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.268981 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-646
I20220824 03:16:06.269093 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-647"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-38/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-647/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.269315 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-647
I20220824 03:16:06.269412 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-648"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-30/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-648/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.269618 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-648
I20220824 03:16:06.269726 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-649"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn-scalar_add-38/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-648/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-649/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.269948 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-649
I20220824 03:16:06.270081 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-broadcast_matmul-650"
device_tag: "cuda"
scope_symbol_id: 2800
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-relu_grad-643/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-650/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.270323 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-broadcast_matmul-650
I20220824 03:16:06.270422 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-broadcast_matmul_grad_b-651"
device_tag: "cuda"
scope_symbol_id: 2800
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-relu_grad-643/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.key-hierarchical_parallel_cast-41/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul_grad_b-651/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.270638 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-broadcast_matmul_grad_b-651
I20220824 03:16:06.270857 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-652"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-649/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-652/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.271071 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-652
I20220824 03:16:06.271174 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-653"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-652/out_0"
      s: "model.blocks.0.ffn-reduce_sum_like-646/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-653/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.271366 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-653
I20220824 03:16:06.271483 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-654"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-650/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-654/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.271709 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-654
I20220824 03:16:06.271817 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-655"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-650/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-655/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.272027 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-655
I20220824 03:16:06.272131 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-656"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-655/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-656/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.272382 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-656
I20220824 03:16:06.272485 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-657"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-654/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-644/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-657/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.272696 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-657
I20220824 03:16:06.272800 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-658"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-650/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-33/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-658/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.272990 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-658
I20220824 03:16:06.273074 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-659"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-650/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-30/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-659/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.273280 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-659
I20220824 03:16:06.273386 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-660"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn-scalar_add-33/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-659/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-660/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.273627 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-660
I20220824 03:16:06.273736 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-661"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-658/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-647/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-661/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.273947 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-661
I20220824 03:16:06.274071 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.time_shift-pad-662"
device_tag: "cuda"
scope_symbol_id: 2841
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-add_n-661/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-662/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.274284 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_shift-pad-662
I20220824 03:16:06.274390 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.time_shift-add_n-663"
device_tag: "cuda"
scope_symbol_id: 2841
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-662/y_0"
      s: "model.blocks.0.ffn-add_n-657/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.time_shift-add_n-663/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.274588 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_shift-add_n-663
I20220824 03:16:06.274828 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-constant-664"
device_tag: "cuda"
scope_symbol_id: 2848
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln2-constant-664/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.275002 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-constant-664
I20220824 03:16:06.275224 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-constant-665"
device_tag: "cuda"
scope_symbol_id: 2848
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln2-constant-665/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.275393 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-constant-665
I20220824 03:16:06.275503 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-layer_norm_param_grad-666"
device_tag: "cuda"
scope_symbol_id: 2848
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn.time_shift-add_n-663/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-28/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.0.ln2-layer_norm_param_grad-666/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.0.ln2-layer_norm_param_grad-666/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.275801 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-layer_norm_param_grad-666
I20220824 03:16:06.275920 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-layer_norm_grad-667"
device_tag: "cuda"
scope_symbol_id: 2848
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn.time_shift-add_n-663/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln2-layer_norm-29/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-28/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ln2-layer_norm_grad-667/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.276218 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-layer_norm_grad-667
I20220824 03:16:06.276330 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-add_n-668"
device_tag: "cuda"
scope_symbol_id: 2848
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ln2-layer_norm_grad-667/dx_0"
      s: "model.blocks.1.ln1-add_n-632/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln2-add_n-668/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.276559 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-add_n-668
I20220824 03:16:06.276671 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-669"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-660/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-669/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.276890 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-669
I20220824 03:16:06.276983 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-670"
device_tag: "cuda"
scope_symbol_id: 2745
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-669/out_0"
      s: "model.blocks.0.ffn-reduce_sum_like-656/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-670/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.277180 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-670
I20220824 03:16:06.277295 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-broadcast_matmul-671"
device_tag: "cuda"
scope_symbol_id: 2873
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ln2-add_n-668/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul-671/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.277529 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-broadcast_matmul-671
I20220824 03:16:06.277632 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-broadcast_matmul_grad_b-672"
device_tag: "cuda"
scope_symbol_id: 2873
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ln2-add_n-668/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.output-hierarchical_parallel_cast-26/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul_grad_b-672/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.277846 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-broadcast_matmul_grad_b-672
I20220824 03:16:06.278074 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-sigmoid_v2_grad-673"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul-671/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-24/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.att-sigmoid_v2_grad-673/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 03:16:06.278280 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-sigmoid_v2_grad-673
I20220824 03:16:06.278394 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-broadcast_matmul-674"
device_tag: "cuda"
scope_symbol_id: 2889
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-sigmoid_v2_grad-673/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-674/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.278631 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-broadcast_matmul-674
I20220824 03:16:06.278721 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-broadcast_matmul_grad_b-675"
device_tag: "cuda"
scope_symbol_id: 2889
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-sigmoid_v2_grad-673/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul_grad_b-675/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 03:16:06.278945 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-broadcast_matmul_grad_b-675
I20220824 03:16:06.279167 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-676"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-674/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-676/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.279381 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-676
I20220824 03:16:06.279469 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-677"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-674/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-677/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.280151 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-677
I20220824 03:16:06.280274 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-678"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-677/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-678/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.280514 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-678
I20220824 03:16:06.280624 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-679"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-674/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-679/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.280851 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-679
I20220824 03:16:06.280948 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-680"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-674/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-680/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 03:16:06.281155 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-680
I20220824 03:16:06.281262 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-681"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-680/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-681/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 03:16:06.281500 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-681
I20220824 03:16:06.281623 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.time_shift-pad-682"
device_tag: "cuda"
scope_symbol_id: 2916
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-679/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-682/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 03:16:06.281831 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_shift-pad-682
I20220824 03:16:06.281937 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.time_shift-add_n-683"
device_tag: "cuda"
scope_symbol_id: 2916
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_shift-pad-682/y_0"
      s: "model.blocks.0.att-broadcast_mul-676/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.time_shift-add_n-683/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.282140 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_shift-add_n-683
I20220824 03:16:06.282380 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-constant-684"
device_tag: "cuda"
scope_symbol_id: 2923
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln1-constant-684/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.282558 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-constant-684
I20220824 03:16:06.282778 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-constant-685"
device_tag: "cuda"
scope_symbol_id: 2923
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln1-constant-685/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.282941 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-constant-685
I20220824 03:16:06.283036 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-layer_norm_param_grad-686"
device_tag: "cuda"
scope_symbol_id: 2923
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.att.time_shift-add_n-683/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.0.ln1-layer_norm_param_grad-686/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.0.ln1-layer_norm_param_grad-686/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.283318 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-layer_norm_param_grad-686
I20220824 03:16:06.283434 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-layer_norm_grad-687"
device_tag: "cuda"
scope_symbol_id: 2923
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.att.time_shift-add_n-683/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ln1-layer_norm_grad-687/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.283758 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-layer_norm_grad-687
I20220824 03:16:06.283854 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-add_n-688"
device_tag: "cuda"
scope_symbol_id: 2923
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ln1-layer_norm_grad-687/dx_0"
      s: "model.blocks.0.ln2-add_n-668/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln1-add_n-688/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.284078 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-add_n-688
I20220824 03:16:06.284176 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-689"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-reduce_sum_like-681/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-689/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.284394 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-689
I20220824 03:16:06.284498 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-690"
device_tag: "cuda"
scope_symbol_id: 2884
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-689/out_0"
      s: "model.blocks.0.att-reduce_sum_like-678/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-690/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.284685 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-690
I20220824 03:16:06.284967 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-constant-691"
device_tag: "cuda"
scope_symbol_id: 2945
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln0-constant-691/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.285141 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-constant-691
I20220824 03:16:06.285400 1458836 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-constant-692"
device_tag: "cuda"
scope_symbol_id: 2945
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln0-constant-692/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 03:16:06.285565 1458836 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-constant-692
I20220824 03:16:06.285661 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-layer_norm_param_grad-693"
device_tag: "cuda"
scope_symbol_id: 2945
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ln1-add_n-688/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.0.ln0-layer_norm_param_grad-693/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.0.ln0-layer_norm_param_grad-693/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 03:16:06.285941 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-layer_norm_param_grad-693
I20220824 03:16:06.286056 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-layer_norm_grad-694"
device_tag: "cuda"
scope_symbol_id: 2945
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ln1-add_n-688/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln0.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ln0-layer_norm_grad-694/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 03:16:06.286347 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-layer_norm_grad-694
I20220824 03:16:06.286686 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.emb-unsorted_segment_sum_like-695"
device_tag: "cuda"
scope_symbol_id: 2966
loc: ""
user_conf {
  op_type_name: "unsorted_segment_sum_like"
  input {
    key: "data"
    value {
      s: "model.blocks.0.ln0-layer_norm_grad-694/dx_0"
    }
  }
  input {
    key: "like"
    value {
      s: "model.emb.weight/out"
    }
  }
  input {
    key: "segment_ids"
    value {
      s: "_GraphBase_0_input.1.0_idx/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.emb-unsorted_segment_sum_like-695/out_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_int64: 0
    }
  }
  input_order: "data"
  input_order: "segment_ids"
  input_order: "like"
  output_order: "out"
}

I20220824 03:16:06.286993 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.emb-unsorted_segment_sum_like-695
I20220824 03:16:06.287578 1458836 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "hierarchical_parallel_cast-696"
device_tag: "cpu"
scope_symbol_id: 2974
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model-broadcast_div-325/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "hierarchical_parallel_cast-696/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "identity"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 03:16:06.287770 1458836 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
hierarchical_parallel_cast-696
(GRAPH:GraphBase_0:GraphBase) end building graph modules.
(GRAPH:GraphBase_0:GraphBase) start building graph outputs.
I20220824 03:16:06.288229 1458836 lazy_op_interpreter.cpp:535] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "_GraphBase_0_output.0.0.0_loss"
device_tag: "cpu"
scope_symbol_id: 2974
output_conf {
  in: "hierarchical_parallel_cast-696/out_0"
  out: "out"
  blob_conf {
    shape {
    }
    data_type: kFloat
    is_dynamic: false
    nd_sbp {
      sbp_parallel {
        broadcast_parallel {
        }
      }
    }
  }
}

I20220824 03:16:06.288378 1458836 lazy_op_interpreter.cpp:538] Lazy nn.Graph name GraphBase_0 add op : 
_GraphBase_0_output.0.0.0_loss
(OUTPUT:_GraphBase_0_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(GRAPH:GraphBase_0:GraphBase) end building graph outputs.
(GRAPH:GraphBase_0:GraphBase) start building graph with compile passes.
I20220824 03:16:06.360263 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_0-InsertPinnedIdentityOpPass
I20220824 03:16:06.411545 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_0-InsertPinnedIdentityOpPass
I20220824 03:16:06.411581 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_1-EliminateDeadNodesPass
I20220824 03:16:06.475767 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_1-EliminateDeadNodesPass
I20220824 03:16:06.475803 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_2-NormalizationExponentialAverageAutoTickPass
I20220824 03:16:06.475821 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_2-NormalizationExponentialAverageAutoTickPass
I20220824 03:16:06.475824 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_3-AutoMixedPrecision
I20220824 03:16:06.529453 1458836 auto_mixed_precision.cpp:214] WhiteSet include: model.emb-gather-0,
model.blocks.0.att.time_shift-pad-3,
model.blocks.0.att.key-hierarchical_parallel_cast-19,
model.blocks.0.att.value-hierarchical_parallel_cast-21,
model.blocks.0.ffn.value-hierarchical_parallel_cast-45,
model.blocks.1.att.output-hierarchical_parallel_cast-76,
model.blocks.2.att.output-hierarchical_parallel_cast-126,
model.blocks.2.ffn.value-hierarchical_parallel_cast-145,
model.blocks.3.att.output-hierarchical_parallel_cast-176,
model.blocks.3.ffn.value-hierarchical_parallel_cast-195,
model.blocks.4.ffn.value-hierarchical_parallel_cast-245,
model.blocks.5.ffn.value-hierarchical_parallel_cast-295,
model.head-broadcast_matmul_grad_b-339_out_0_pinned_identity,
model.head-broadcast_matmul_grad_b-339,
model.blocks.5.ffn.value-broadcast_matmul_grad_b-359,
model.blocks.5.ffn.key-broadcast_matmul_grad_b-371_out_0_pinned_identity,
model.blocks.5.ffn-add_n-390_out_0_pinned_identity,
model.blocks.5.ffn-add_n-390,
model.blocks.5.ffn-scalar_mul-389,
model.blocks.5.ffn-reduce_sum_like-380,
model.blocks.5.ffn-broadcast_mul-379,
model.blocks.5.att.output-broadcast_matmul_grad_b-392,
model.blocks.2.ffn-reduce_sum_like-544,
model.blocks.5.att.output-broadcast_matmul-391,
model.blocks.5.ln2-add_n-388,
model.blocks.5.ffn.key-broadcast_matmul-292,
model.blocks.5.ffn-broadcast_mul-378,
model.blocks.0.ln2-layer_norm_grad-667,
model.blocks.5.ffn-broadcast_mul-374,
model.blocks.5.ffn-reduce_sum_like-366,
model.blocks.5.ffn-broadcast_mul-365,
model.blocks.2.ffn-add_n-549,
model.blocks.5.ffn.receptance-broadcast_matmul-360,
model.head_k-broadcast_matmul-348,
model.blocks.0.att-scalar_mul-689,
model-batch_matmul-342,
model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361_out_0_pinned_identity,
model.blocks.0.ffn.value-broadcast_matmul-638,
model.head-broadcast_matmul-338,
model-reshape-337,
model.blocks.3.att.time_shift-pad-153,
model.blocks.3.ffn-reduce_sum_like-488,
model-reshape-335,
model.blocks.1.att.output-broadcast_matmul_grad_b-616,
model-log_softmax_grad-334,
model.blocks.5.ln2-layer_norm_grad-387,
model.blocks.5.ffn.key-broadcast_matmul_grad_b-371,
model-transpose-318,
model.blocks.3.att.key-broadcast_matmul-170,
model.blocks.1.ffn-relu_grad-587,
model-batch_matmul-312,
model.blocks.1.att.value-broadcast_matmul-72,
model.blocks.5.att.receptance-broadcast_matmul_grad_b-395,
model.blocks.1.att-broadcast_mul-59,
model.blocks.1.ffn-broadcast_mul-598,
model.blocks.1.att-add_n-58,
model.blocks.5.ffn.value-broadcast_matmul-296,
model.blocks.1.att-broadcast_mul-54,
model.blocks.2.ffn-reduce_sum_like-548,
model-reshape-319,
model.blocks.3.ffn.key-broadcast_matmul-482,
model.blocks.1.ffn-broadcast_mul-592,
model.blocks.4.att-broadcast_mul-204,
model.blocks.2.att-add_n-108,
model.blocks.4.att.value-broadcast_matmul-222,
model.blocks.5.ffn-broadcast_mul-300,
model.blocks.5.ffn.time_shift-pad-280,
model.blocks.3.att.receptance-broadcast_matmul-174,
model.head_q-hierarchical_parallel_cast-303,
model.blocks.1.att.output-broadcast_matmul_grad_b-616_out_0_pinned_identity,
model.blocks.4.att.key-broadcast_matmul-220,
model-log_softmax-320,
model.blocks.5.ffn.time_shift-add_n-383,
model.emb-unsorted_segment_sum_like-695_out_0_pinned_identity,
model.blocks.1.att-add_n-634,
model.blocks.5.att.value-broadcast_matmul-272,
model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529_out_0_pinned_identity,
model.blocks.5.att.value-hierarchical_parallel_cast-271,
model.blocks.5.att-broadcast_mul-254,
model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361,
model.blocks.3.att-broadcast_mul-157,
model.head_k-broadcast_matmul-306,
model.head_k-hierarchical_parallel_cast-305,
model.head-broadcast_matmul-314,
model.blocks.3.ffn.time_shift-pad-494,
model.head_q-broadcast_matmul-304,
model.head-hierarchical_parallel_cast-313,
model-add_n-315,
model.blocks.4.ffn-broadcast_mul-430,
model.ln_out-layer_norm-302,
model.blocks.5.ffn.receptance-broadcast_matmul-298,
model.blocks.2.att.time_shift-pad-103,
model.blocks.5.ffn.receptance-hierarchical_parallel_cast-297,
model.blocks.5.ffn.key-hierarchical_parallel_cast-291,
model.blocks.2.ffn-broadcast_mul-547,
model.blocks.5.ffn-add_n-285,
model-transpose-346,
model.blocks.5.ffn-broadcast_mul-289,
model.blocks.5-add_n-278,
model.blocks.5.att.receptance-broadcast_matmul-274,
model.blocks.5.att-broadcast_mul-267,
model.blocks.1.att.output-broadcast_matmul-77,
model.blocks.5.att-broadcast_mul-257,
model.blocks.1.att-broadcast_mul-62,
model.blocks.4.ffn-add_n-240,
model.blocks.5.att.key-hierarchical_parallel_cast-269,
model.blocks.4.ffn-relu-243,
model.blocks.4.ffn.key-broadcast_matmul-242,
model.blocks.5.att-add_n-263,
model.blocks.1.att.time_shift-pad-53,
model.blocks.5.ln1-layer_norm_grad-407,
model.blocks.2.ffn-broadcast_mul-150,
model.blocks.4.ffn-broadcast_mul-424,
model.blocks.1.ffn-add_n-90,
model.blocks.2-add_n-128,
model.blocks.2.att.receptance-broadcast_matmul-124,
model.blocks.4.ffn-broadcast_mul-239,
model.blocks.4.ln2-layer_norm_grad-443,
model.blocks.4.ffn.value-broadcast_matmul-246,
model.blocks.4.att.value-hierarchical_parallel_cast-221,
model.blocks.2.att.receptance-hierarchical_parallel_cast-123,
model.blocks.5.att-add_n-410,
model.blocks.1.ffn.key-broadcast_matmul-594,
model.blocks.1.ffn-broadcast_mul-84,
model.blocks.0.ffn-broadcast_mul-655,
model.blocks.5.att.key-broadcast_matmul-270,
model.blocks.1.ln2-layer_norm-79,
model.blocks.3.att.receptance-hierarchical_parallel_cast-173,
model.blocks.3.ffn.key-hierarchical_parallel_cast-191,
model.blocks.0.ffn.key-broadcast_matmul-42,
model.blocks.0.ffn-broadcast_mul-644,
model.blocks.1-add_n-101,
model.head_q-broadcast_matmul_grad_b-345_out_0_pinned_identity,
model.blocks.1.ffn-scalar_mul-613,
model.blocks.5.att-broadcast_mul-262,
model.blocks.5-add_n-301,
model.blocks.3.ln2-layer_norm-179,
model.blocks.0.ln2-add_n-668,
model.blocks.2.ffn.key-hierarchical_parallel_cast-141,
model.blocks.0.ffn.time_shift-pad-30,
model.blocks.2.ffn-broadcast_mul-536,
model.blocks.5.ffn-add_n-290,
model.blocks.2-add_n-151,
model.blocks.0.ffn-broadcast_mul-39,
model.ln_out-layer_norm_grad-354,
model-batch_matmul-343,
model.blocks.4.att.output-hierarchical_parallel_cast-226,
model.blocks.1.ffn.receptance-hierarchical_parallel_cast-97,
model.blocks.4.ffn-add_n-437,
model.blocks.3.ffn.receptance-broadcast_matmul-198,
model.blocks.4.att.output-broadcast_matmul_grad_b-448,
model.blocks.2.att.value-broadcast_matmul-122,
model.blocks.5.ffn-broadcast_mul-281,
model.blocks.1.ffn-broadcast_mul-89,
model.blocks.5.ffn-broadcast_mul-356,
model.blocks.4.ffn-relu_grad-419,
model.blocks.4-add_n-251,
model.blocks.3.ffn-broadcast_mul-186,
model.blocks.1.ffn-reduce_sum_like-593,
model.blocks.1.att.time_shift-pad-626,
model.blocks.0.att.receptance-broadcast_matmul-24,
model.blocks.2.att.key-hierarchical_parallel_cast-119,
model.blocks.0.ffn-add_n-40,
model.blocks.4.ffn-add_n-235,
model.blocks.4.ffn.value-broadcast_matmul-414,
model.blocks.3.ffn-add_n-502_out_0_pinned_identity,
model.blocks.2.ffn-broadcast_mul-546,
model.blocks.2.att.time_shift-pad-570,
model.blocks.1.att-add_n-68,
model.head_q-add_n-347,
model.blocks.0.ln2-layer_norm-29,
model.blocks.2.att.output-broadcast_matmul-127,
model.blocks.2.ffn.value-broadcast_matmul-146,
model.blocks.5.att.output-broadcast_matmul_grad_b-392_out_0_pinned_identity,
model.blocks.3.att.key-hierarchical_parallel_cast-169,
model.blocks.3.ffn-broadcast_mul-486,
model.blocks.1.att.key-broadcast_matmul-70,
model.blocks.0.ffn.key-hierarchical_parallel_cast-41,
model.blocks.2.att.output-broadcast_matmul_grad_b-560_out_0_pinned_identity,
model.blocks.2.att-broadcast_mul-107,
model.blocks.3.att-scalar_mul-521,
model.blocks.1-add_n-78,
model.blocks.1.ffn.time_shift-pad-80,
model.blocks.5.ln1-add_n-408,
model.blocks.4.ffn.receptance-broadcast_matmul-248,
model.blocks.2.ffn.value-broadcast_matmul_grad_b-527_out_0_pinned_identity,
model.blocks.0.att.output-broadcast_matmul-27,
model.blocks.1.att.receptance-broadcast_matmul-74,
model.blocks.4.att.receptance-broadcast_matmul-450,
model.blocks.2.att.receptance-broadcast_matmul_grad_b-563_out_0_pinned_identity,
model.blocks.1.ffn.value-hierarchical_parallel_cast-95,
model.blocks.5.att.output-hierarchical_parallel_cast-276,
model.blocks.4.att-add_n-208,
model.blocks.2.ffn-broadcast_mul-134,
model.blocks.3.ffn-add_n-185,
model.blocks.3.ffn-broadcast_mul-200,
model.blocks.3.ffn.time_shift-pad-180,
model.blocks.3.ffn-broadcast_mul-479,
model.blocks.1.ffn.value-broadcast_matmul-96,
model.blocks.3.ln1-add_n-520,
model.blocks.2.att.output-broadcast_matmul-559,
model.blocks.5.att-add_n-268,
model.blocks.4.att.receptance-broadcast_matmul_grad_b-451,
model.blocks.0.att.value-broadcast_matmul-22,
model.blocks.1.ffn.receptance-broadcast_matmul-584,
model.blocks.4.att.time_shift-add_n-459,
model.blocks.2.ffn-broadcast_mul-139,
model.blocks.3.att-add_n-168,
model.blocks.0.ffn.receptance-broadcast_matmul-48,
model.blocks.4.ffn-broadcast_mul-434,
model.blocks.0.ffn-relu-43,
model.blocks.3.ffn-reduce_sum_like-481,
model.blocks.5.att-add_n-258,
model.blocks.1.ffn.value-broadcast_matmul-582,
model.blocks.4.ffn-broadcast_mul-234,
model.blocks.5.ffn-broadcast_mul-375,
model.blocks.3.att-broadcast_mul-154,
model.blocks.4.ffn-scalar_mul-445,
model.blocks.2.ffn-reduce_sum_like-537,
model.blocks.2.ffn-broadcast_mul-131,
model.blocks.4.att-add_n-213,
model.blocks.5.ffn-broadcast_mul-286,
model.blocks.3.ffn-broadcast_mul-476,
model.blocks.4.ffn-add_n-429,
model.blocks.3.ffn-broadcast_mul-189,
model.blocks.2.ffn-broadcast_mul-524,
model.blocks.3.ffn-broadcast_mul-490,
model.blocks.1.att-broadcast_mul-67,
model.blocks.3.ffn.value-broadcast_matmul-470,
model.head_k-add_n-350,
model.blocks.2.ffn.receptance-broadcast_matmul-148,
model.blocks.0.ffn.receptance-hierarchical_parallel_cast-47,
model.blocks.3.ffn.receptance-broadcast_matmul-472,
model.blocks.2.ln2-layer_norm-129,
model.blocks.3.att-broadcast_mul-162,
model.blocks.3.ffn-add_n-493,
model.blocks.4.att.key-hierarchical_parallel_cast-219,
model.blocks.3.att-broadcast_mul-509,
model.blocks.2.att-broadcast_mul-117,
model.blocks.4.att.time_shift-pad-203,
model.blocks.0-add_n-51,
model.blocks.2.att-broadcast_mul-112,
model.blocks.4.att-broadcast_mul-217,
model.blocks.3-add_n-201,
model.blocks.0.ffn.value-broadcast_matmul-46,
model.blocks.1.ffn-broadcast_mul-81,
model.blocks.5.ffn-broadcast_mul-364,
model.blocks.5.att.time_shift-add_n-403,
model.blocks.5.ffn.key-broadcast_matmul-370,
model.blocks.0.ffn-broadcast_mul-34,
model.blocks.0.att.receptance-hierarchical_parallel_cast-23,
model.blocks.2.att-broadcast_mul-114,
model.blocks.0.ffn-add_n-35,
model.blocks.0.ffn-broadcast_mul-636,
model.blocks.2.ffn-broadcast_mul-535,
model.blocks.5.ffn.value-broadcast_matmul-358,
model.blocks.2.att-broadcast_mul-109,
model.blocks.5.att-broadcast_mul-259,
model.blocks.2.att-reduce_sum_like-569,
model.blocks.2.att-add_n-578_out_0_pinned_identity,
model.blocks.5.att.receptance-hierarchical_parallel_cast-273,
model.blocks.1.att.receptance-broadcast_matmul-618,
model.blocks.0.att-broadcast_mul-680,
model.blocks.2.ffn.time_shift-pad-130,
model-scalar_mul-309,
model.blocks.2.ffn-add_n-135,
model.blocks.1.att.key-hierarchical_parallel_cast-69,
model.blocks.2.ffn.key-broadcast_matmul-142,
model.blocks.2.ffn-add_n-541_out_0_pinned_identity,
model.blocks.2.ffn-broadcast_mul-136,
model.blocks.2.att.value-hierarchical_parallel_cast-121,
model.blocks.1.ffn-relu-93,
model.blocks.2.ffn-add_n-140,
model.blocks.2.ffn-add_n-541,
model.blocks.2.ffn.receptance-hierarchical_parallel_cast-147,
model.blocks.3.att.receptance-broadcast_matmul-506,
model.blocks.5.ffn.time_shift-pad-382,
model.blocks.4.att.output-broadcast_matmul-447,
model.blocks.3.ln1-layer_norm-152,
model.blocks.0.ffn-broadcast_mul-31,
model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473_out_0_pinned_identity,
model-batch_matmul-340,
model.blocks.2.ffn-broadcast_mul-542,
model.blocks.4.att-add_n-466,
model.blocks.3.att-broadcast_mul-167,
model.blocks.4.ffn.time_shift-pad-438,
model.head_q-broadcast_matmul-344,
model.blocks.5.ffn-reduce_sum_like-369,
model.blocks.5.att-add_n-410_out_0_pinned_identity,
model.blocks.5.ffn-relu-293,
model.blocks.3.att-broadcast_mul-164,
model-transpose-336,
model.blocks.4.att-reduce_sum_like-457,
model.blocks.3.att-add_n-158,
model.blocks.3.att-broadcast_mul-159,
model.blocks.3.att-add_n-163,
model.blocks.3.ffn-broadcast_mul-468,
model.blocks.1.ffn-broadcast_mul-100,
model.blocks.3.ffn.key-broadcast_matmul-192,
model.blocks.0.att.key-broadcast_matmul-20,
model.blocks.4.att.receptance-hierarchical_parallel_cast-223,
model.blocks.2.att.receptance-broadcast_matmul_grad_b-563,
model.blocks.5.ln1-layer_norm-252,
model.blocks.3.ffn-relu-193,
model.blocks.0.ffn-broadcast_mul-647,
model.blocks.2.att.output-broadcast_matmul_grad_b-560,
model.head_k-broadcast_matmul_grad_b-349,
model.blocks.4.att-broadcast_mul-453,
model.blocks.5.ffn-broadcast_mul-368,
model.head_q-broadcast_matmul_grad_b-345,
model.blocks.5.ffn-broadcast_mul-367,
model.blocks.4.att.time_shift-pad-458,
model.blocks.3.att.value-hierarchical_parallel_cast-171,
model-transpose-307,
model.blocks.5.ffn-add_n-373,
model.blocks.0.ffn-broadcast_mul-36,
model.blocks.4-add_n-228,
model.blocks.4.att.receptance-broadcast_matmul-224,
model.blocks.5.att-broadcast_mul-400,
model.blocks.3.ffn.receptance-hierarchical_parallel_cast-197,
model.blocks.5.ffn-scalar_mul-372,
model.blocks.2.ln2-add_n-556,
model.blocks.3.ffn-broadcast_mul-184,
model.blocks.1.att.value-hierarchical_parallel_cast-71,
model.blocks.5.att.output-broadcast_matmul-277,
model.blocks.0.ffn-reduce_sum_like-656,
model.blocks.3.att.value-broadcast_matmul-172,
model-reshape-316,
model.blocks.4.att-broadcast_mul-214,
model.blocks.1.ffn.value-broadcast_matmul_grad_b-583_out_0_pinned_identity,
model.blocks.3.att-broadcast_mul-508,
model.blocks.3.ln1-layer_norm_grad-519,
model.blocks.2.att-add_n-113,
model.blocks.4.att-add_n-218,
model.blocks.2.att-add_n-118,
model.blocks.4.ffn-broadcast_mul-250,
model.blocks.1.ffn.receptance-broadcast_matmul-98,
model.blocks.4.att.output-broadcast_matmul-227,
model.blocks.0.ffn.receptance-broadcast_matmul-640,
model.blocks.4.ln2-add_n-444,
model.blocks.5.ffn-broadcast_mul-284,
model.blocks.5.att-broadcast_mul-396,
model.blocks.5.att.receptance-broadcast_matmul_grad_b-395_out_0_pinned_identity,
model.blocks.0.ffn-reduce_sum_like-646,
model.blocks.0.ffn.time_shift-add_n-663,
model.blocks.0.att.receptance-broadcast_matmul_grad_b-675,
model.blocks.5.att-broadcast_mul-399,
model.blocks.5.att.time_shift-pad-402,
model.blocks.4.ffn.receptance-broadcast_matmul-416,
model.blocks.3.att.receptance-broadcast_matmul_grad_b-507,
model.blocks.2.ffn-broadcast_mul-543,
model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585_out_0_pinned_identity,
model.blocks.0.ffn-broadcast_mul-654,
model.blocks.4.ffn-broadcast_mul-423,
model.blocks.4.ffn-broadcast_mul-421,
model.blocks.3.att.output-broadcast_matmul-503,
model.blocks.4.ffn-reduce_sum_like-422,
model.blocks.4.ffn.time_shift-add_n-439,
model.blocks.4.ffn-scalar_mul-428,
model.blocks.3.ffn-broadcast_mul-491,
model.blocks.4.ffn-add_n-429_out_0_pinned_identity,
model.blocks.2.ffn-relu_grad-531,
model.blocks.2.att.receptance-broadcast_matmul-562,
model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417_out_0_pinned_identity,
model.blocks.4.ffn-broadcast_mul-412,
model.blocks.4.ffn.key-broadcast_matmul-426,
model.blocks.1.ffn-broadcast_mul-599,
model.blocks.4.ffn-add_n-433,
model.blocks.4.ffn-broadcast_mul-431,
model.blocks.4.ffn-reduce_sum_like-432,
model.blocks.4.att-broadcast_mul-452,
model.blocks.0.ln1-layer_norm_grad-687,
model.blocks.1.ffn-scalar_mul-596,
model.blocks.4.ffn.time_shift-pad-230,
model.blocks.4.att-broadcast_mul-455,
model.blocks.5.ffn-add_n-377,
model.blocks.4.ffn.key-hierarchical_parallel_cast-241,
model.blocks.4.ln1-layer_norm_grad-463,
model.blocks.4.ln1-add_n-464,
model.blocks.3.ffn-broadcast_mul-477,
model.blocks.0.att.time_shift-pad-682,
model.blocks.3.ffn-reduce_sum_like-478,
model.blocks.1.ffn-broadcast_mul-603,
model.blocks.3.ffn-broadcast_mul-480,
model.blocks.1.ffn-add_n-605,
model.blocks.0-add_n-28,
model.blocks.2.att-broadcast_mul-565,
model.blocks.3.ffn-add_n-485,
model.head_k-broadcast_matmul_grad_b-349_out_0_pinned_identity,
model.blocks.3.ffn-reduce_sum_like-492,
model.blocks.0.att-broadcast_mul-677,
model.blocks.3.ffn-add_n-485_out_0_pinned_identity,
model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473,
model.blocks.1.ln1-add_n-632,
model.blocks.3.ffn-relu_grad-475,
model.blocks.3.ffn-add_n-489,
model.blocks.0.ffn-broadcast_mul-648,
model.blocks.4.att-reduce_sum_like-454,
model.blocks.3.ffn-broadcast_mul-487,
model.blocks.3.ffn.time_shift-add_n-495,
model.blocks.2.ffn.value-broadcast_matmul_grad_b-527,
model.blocks.3.ln2-add_n-500,
model.blocks.3.att-reduce_sum_like-510,
model.blocks.2.ln1-layer_norm-102,
model.blocks.3.att-broadcast_mul-511,
model.blocks.3.att.time_shift-pad-514,
model.blocks.1.ln1-layer_norm-52,
model.blocks.3.att.time_shift-add_n-515,
model.blocks.2.ffn-broadcast_mul-532,
model.blocks.1.ffn.key-broadcast_matmul_grad_b-595_out_0_pinned_identity,
model.blocks.4.att-scalar_mul-465,
model.blocks.2.ffn-broadcast_mul-533,
model.blocks.4.ffn-reduce_sum_like-425,
model.blocks.2.ffn-reduce_sum_like-534,
model.blocks.1.ffn.key-broadcast_matmul-92,
model.blocks.2.ffn.time_shift-add_n-551,
model.blocks.2.att-broadcast_mul-104,
model.blocks.3.att.receptance-broadcast_matmul_grad_b-507_out_0_pinned_identity,
model.blocks.2.ffn-scalar_mul-540,
model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641_out_0_pinned_identity,
model.blocks.4.ffn-broadcast_mul-420,
model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529,
model.blocks.2.ffn.value-broadcast_matmul-526,
model.blocks.5.ffn-reduce_sum_like-376,
model.blocks.1.att.receptance-broadcast_matmul_grad_b-619,
model.blocks.2.ffn-add_n-545,
model.blocks.2.ffn.time_shift-pad-550,
model.blocks.2.ln2-layer_norm_grad-555,
model.blocks.2.att-scalar_mul-577,
model.blocks.1.ffn.key-broadcast_matmul_grad_b-595,
model.blocks.2.att-add_n-578,
model.blocks.2.att-broadcast_mul-567,
model.blocks.2.ln1-layer_norm_grad-575,
model.blocks.2.ln1-add_n-576,
model.blocks.1.ffn-broadcast_mul-588,
model.blocks.1.ffn-broadcast_mul-591,
model.blocks.1.ffn-broadcast_mul-589,
model.blocks.1.ffn-add_n-614_out_0_pinned_identity,
model.blocks.0.att.receptance-broadcast_matmul-674,
model.blocks.0.ffn-add_n-670,
model.blocks.1.ffn-reduce_sum_like-590,
model.blocks.1.att-broadcast_mul-623,
model.blocks.4.ffn-broadcast_mul-236,
model.blocks.1.ffn-add_n-597,
model.blocks.5.ffn-add_n-373_out_0_pinned_identity,
model.blocks.1.att.receptance-hierarchical_parallel_cast-73,
model.blocks.1.ffn-reduce_sum_like-604,
model.blocks.2.att-reduce_sum_like-566,
model.blocks.1.ffn-add_n-597_out_0_pinned_identity,
model.blocks.1.ffn-broadcast_mul-580,
model.blocks.1.ffn-add_n-601,
model.blocks.5.att.time_shift-pad-253,
model.blocks.1.ffn-reduce_sum_like-600,
model.blocks.1.ffn.time_shift-pad-606,
model.blocks.1.ln2-layer_norm_grad-611,
model.blocks.4.ffn-broadcast_mul-435,
model.blocks.4.ln2-layer_norm-229,
model.blocks.1.ln2-add_n-612,
model.blocks.1.att-broadcast_mul-64,
model.blocks.1.att-broadcast_mul-620,
model.blocks.4.ffn.receptance-hierarchical_parallel_cast-247,
model.blocks.1.att.time_shift-add_n-627,
model.blocks.1.ln1-layer_norm_grad-631,
model.blocks.5.att-broadcast_mul-264,
model.blocks.0.ffn-broadcast_mul-645,
model.blocks.0.ffn-reduce_sum_like-649,
model.blocks.0.ffn-scalar_mul-652,
model.blocks.0.ffn-add_n-653,
model.blocks.1.ffn-broadcast_mul-602,
model.blocks.0.ffn-reduce_sum_like-660,
model.blocks.0.ffn-scalar_mul-669,
model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641,
model.blocks.0.ffn-add_n-653_out_0_pinned_identity,
model.blocks.0.ffn-relu_grad-643,
model.blocks.0.ffn-add_n-657,
model.blocks.3.ln2-layer_norm_grad-499,
model.blocks.1.ffn.time_shift-add_n-607,
model.blocks.0.ffn-broadcast_mul-658,
model.blocks.0.ffn-add_n-661,
model.blocks.0.ffn.time_shift-pad-662,
model.blocks.0.att.output-broadcast_matmul-671,
model.blocks.0.att-broadcast_mul-676,
model.blocks.5.ffn-relu_grad-363,
model.blocks.0.att-reduce_sum_like-678,
model.blocks.0.att.output-broadcast_matmul_grad_b-672_out_0_pinned_identity,
model.blocks.1.att-add_n-63,
model.blocks.0.att-broadcast_mul-679,
model.blocks.0.att.output-hierarchical_parallel_cast-26,
model.blocks.4.ffn.key-broadcast_matmul_grad_b-427,
model.blocks.3.ffn-scalar_mul-484,
model.blocks.0.att.time_shift-add_n-683,
model.blocks.3.ffn-add_n-190,
model.blocks.2.att.time_shift-add_n-571,
model.blocks.3.att-add_n-522,
model.blocks.0.ln1-add_n-688,
model.blocks.0.ln0-layer_norm_grad-694,
model.blocks.4.att-broadcast_mul-212,
model.emb-unsorted_segment_sum_like-695,
model.blocks.0.att-reduce_sum_like-681,
model.blocks.5.ffn-add_n-381,
model.blocks.0.att-add_n-690,
model.blocks.2.ffn-relu-143,
model.blocks.0.att-add_n-690_out_0_pinned_identity,
model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417,
model.blocks.0.att.receptance-broadcast_matmul_grad_b-675_out_0_pinned_identity,
model.blocks.4.ln1-layer_norm-202,
model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585,
model.blocks.0.att.output-broadcast_matmul_grad_b-672,
model.blocks.0.ffn-broadcast_mul-659,
model.blocks.5.ffn.value-broadcast_matmul_grad_b-359_out_0_pinned_identity,
model.blocks.1.ffn.key-hierarchical_parallel_cast-91,
model.blocks.3.ffn.value-broadcast_matmul-196,
model.blocks.0.ffn.key-broadcast_matmul_grad_b-651,
model.blocks.1.ffn-add_n-85,
model.blocks.0.ffn-add_n-670_out_0_pinned_identity,
model.blocks.3.ffn-broadcast_mul-181,
model.blocks.1.att-scalar_mul-633,
model.blocks.0.ffn.key-broadcast_matmul_grad_b-651_out_0_pinned_identity,
model.blocks.0.ffn.value-broadcast_matmul_grad_b-639,
model.blocks.2.ffn.receptance-broadcast_matmul-528,
model.blocks.3.ffn.key-broadcast_matmul_grad_b-483_out_0_pinned_identity,
model.blocks.0.ffn.value-broadcast_matmul_grad_b-639_out_0_pinned_identity,
model.blocks.1.att-broadcast_mul-621,
model.blocks.1.att-broadcast_mul-624,
model.blocks.1.att-reduce_sum_like-625,
model.blocks.1.att-add_n-634_out_0_pinned_identity,
model.blocks.5.att.receptance-broadcast_matmul-394,
model.blocks.2.att-broadcast_mul-564,
model.blocks.1.att-reduce_sum_like-622,
model.blocks.1.att.receptance-broadcast_matmul_grad_b-619_out_0_pinned_identity,
model.blocks.1.ffn-add_n-614,
model.blocks.2.att-broadcast_mul-568,
model.blocks.1.ffn.value-broadcast_matmul_grad_b-583,
model.blocks.0.ffn.key-broadcast_matmul-650,
model.blocks.2.ffn-scalar_mul-557,
model.blocks.2.ffn-add_n-558,
model.blocks.4.ffn-reduce_sum_like-436,
model.blocks.2.ffn-add_n-558_out_0_pinned_identity,
model.blocks.1.att-broadcast_mul-57,
model.blocks.2.ffn.key-broadcast_matmul_grad_b-539,
model.blocks.2.att.key-broadcast_matmul-120,
model.blocks.2.ffn.key-broadcast_matmul_grad_b-539_out_0_pinned_identity,
model.blocks.3.att-broadcast_mul-512,
model.blocks.1.ffn-broadcast_mul-86,
model.blocks.3.att-reduce_sum_like-513,
model.blocks.0.ffn-broadcast_mul-50,
model.blocks.3.att-add_n-522_out_0_pinned_identity,
model.blocks.4.att-broadcast_mul-209,
model.blocks.3.att.output-broadcast_matmul_grad_b-504,
model.blocks.3.att.output-broadcast_matmul_grad_b-504_out_0_pinned_identity,
model.blocks.3.ffn-scalar_mul-501,
model.blocks.5.ln2-layer_norm-279,
model.blocks.3.ffn-add_n-502,
model.blocks.1.att.output-broadcast_matmul-615,
model.blocks.3.ffn.key-broadcast_matmul_grad_b-483,
model.blocks.3.ffn.value-broadcast_matmul_grad_b-471,
model.blocks.4.att-broadcast_mul-456,
model.blocks.4.att-add_n-466_out_0_pinned_identity,
model.blocks.4.att.receptance-broadcast_matmul_grad_b-451_out_0_pinned_identity,
model.blocks.4.ffn-broadcast_mul-231,
model.blocks.3.ffn.value-broadcast_matmul_grad_b-471_out_0_pinned_identity,
model.blocks.4.att.output-broadcast_matmul_grad_b-448_out_0_pinned_identity,
model.blocks.4.ffn-add_n-446,
model.blocks.2.ffn.key-broadcast_matmul-538,
model.blocks.4.ffn-add_n-446_out_0_pinned_identity,
model.blocks.4.ffn.key-broadcast_matmul_grad_b-427_out_0_pinned_identity,
model.blocks.4.ffn.value-broadcast_matmul_grad_b-415,
model.blocks.4.ffn.value-broadcast_matmul_grad_b-415_out_0_pinned_identity,
model-scalar_mul-341,
model.blocks.4.att-broadcast_mul-207,
model.blocks.5.att-broadcast_mul-397,
model.blocks.3.att.output-broadcast_matmul-177,
model.blocks.5.att-reduce_sum_like-398,
model.blocks.3-add_n-178,
model.blocks.5.att-reduce_sum_like-401,
model-batch_matmul-308,
model.blocks.5.att-scalar_mul-409
I20220824 03:16:06.552426 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_3-AutoMixedPrecision
I20220824 03:16:06.552460 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_4-PruneAmpWhiteIdentityOpPass
I20220824 03:16:06.627537 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_4-PruneAmpWhiteIdentityOpPass
I20220824 03:16:06.627574 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_5-OptimizerPlacementOptimizationPass
I20220824 03:16:06.627591 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_5-OptimizerPlacementOptimizationPass
I20220824 03:16:06.627595 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_6-FuseAddToOutputPass
I20220824 03:16:06.721735 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_6-FuseAddToOutputPass
I20220824 03:16:06.721786 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_7-IRRoundTripBeforeAD
I20220824 03:16:06.721807 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_7-IRRoundTripBeforeAD
I20220824 03:16:06.721812 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_8-DynamicLossScaleSchedulePass
I20220824 03:16:06.721820 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_8-DynamicLossScaleSchedulePass
I20220824 03:16:06.721827 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_9-AutoTrainStep
I20220824 03:16:06.858484 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_9-AutoTrainStep
I20220824 03:16:06.858589 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_10-AutoLearningRate
I20220824 03:16:06.954139 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_10-AutoLearningRate
I20220824 03:16:06.954176 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_11-QuantAwareTraining
I20220824 03:16:06.954190 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_11-QuantAwareTraining
I20220824 03:16:06.954193 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_12-GenerateOptimizerOpConfs
I20220824 03:16:07.043275 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_12-GenerateOptimizerOpConfs
I20220824 03:16:07.043313 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_13-PrunePinnedIdentityOpPass
I20220824 03:16:08.079548 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_13-PrunePinnedIdentityOpPass
I20220824 03:16:08.079587 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_14-ReplaceEmbeddingOps
I20220824 03:16:08.199185 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_14-ReplaceEmbeddingOps
I20220824 03:16:08.199229 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_15-FuseEmbeddingShuffleInteractionPass
I20220824 03:16:08.199237 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_15-FuseEmbeddingShuffleInteractionPass
I20220824 03:16:08.199241 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_16-FuseBCEReduceMeanFwBwPass
I20220824 03:16:08.199246 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_16-FuseBCEReduceMeanFwBwPass
I20220824 03:16:08.199250 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_17-AddSspVariableProxy
I20220824 03:16:08.199259 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_17-AddSspVariableProxy
I20220824 03:16:08.199262 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_18-CheckpointingPass
I20220824 03:16:08.296574 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_18-CheckpointingPass
I20220824 03:16:08.296618 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_19-CudnnFusedNormalizationAddReluPass
I20220824 03:16:08.388864 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_19-CudnnFusedNormalizationAddReluPass
I20220824 03:16:08.388916 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_20-PruneCastToStaticShapeOpsPass
I20220824 03:16:08.512241 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_20-PruneCastToStaticShapeOpsPass
I20220824 03:16:08.512295 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_21-IRRoundTrip
I20220824 03:16:08.512317 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_21-IRRoundTrip
I20220824 03:16:08.512323 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_22-FuseAddToOutputPass1
I20220824 03:16:08.646719 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_22-FuseAddToOutputPass1
I20220824 03:16:08.646776 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_23-FuseConsecutiveAddPass
I20220824 03:16:08.748080 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_23-FuseConsecutiveAddPass
I20220824 03:16:08.748122 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_24-IndexedSlicesOptimizerRewritePass
I20220824 03:16:08.748144 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_24-IndexedSlicesOptimizerRewritePass
I20220824 03:16:08.748148 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_25-SplitSparseSoftmaxCrossEntropyOpPass
I20220824 03:16:08.879974 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_25-SplitSparseSoftmaxCrossEntropyOpPass
I20220824 03:16:08.880017 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_26-DoParallelCastBeforeWideningTypeCast
I20220824 03:16:08.971130 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_26-DoParallelCastBeforeWideningTypeCast
I20220824 03:16:08.971171 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_27-FuseCastScalePass
I20220824 03:16:09.080509 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_27-FuseCastScalePass
I20220824 03:16:09.080549 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_28-PruneParallelCastOpsPass
I20220824 03:16:09.199905 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_28-PruneParallelCastOpsPass
I20220824 03:16:09.199944 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_29-FuseUpdateOpsPass
I20220824 03:16:09.321633 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_29-FuseUpdateOpsPass
I20220824 03:16:09.321674 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_30-FuseModelUpdateCastOpsPass
I20220824 03:16:09.321691 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_30-FuseModelUpdateCastOpsPass
I20220824 03:16:09.321696 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_31-MultiTensorModelUpdatePass
I20220824 03:16:09.321700 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_31-MultiTensorModelUpdatePass
I20220824 03:16:09.321704 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_32-FixPipelineStageIdPass
I20220824 03:16:09.417326 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_32-FixPipelineStageIdPass
I20220824 03:16:09.417369 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_33-PipelineBufferPass
I20220824 03:16:09.525110 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_33-PipelineBufferPass
I20220824 03:16:09.525151 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_34-DumpVariableInfoPass
I20220824 03:16:09.611284 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_34-DumpVariableInfoPass
I20220824 03:16:09.611325 1458836 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_35-DumpBlobParallelConfPass
I20220824 03:16:09.705972 1458836 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_35-DumpBlobParallelConfPass
(GRAPH:GraphBase_0:GraphBase) end building graph with compile passes.
(GRAPH:GraphBase_0:GraphBase) start re-building graph outputs for optimizatioin.
(GRAPH:GraphBase_0:GraphBase) end re-building graph outputs for optimizatioin.
(GRAPH:GraphBase_0:GraphBase) building graph Done! Cost time: 4.11s.

(GRAPH:GraphBase_0:GraphBase) start building plan.
I20220824 03:16:11.785894 1458836 nn_graph.cpp:291] Graph name: GraphBase_0 compile time: 0.608303 seconds.
I20220824 03:16:12.143959 1458836 plan_util.cpp:965] 
 Graph name GraphBase_0 in Rank: 0, Device: 0 needs to allocate [ 3918.12 MiB ] device memory. 
   In general, Chunk id: 0  memory is [ 2716.86 MiB ] with mem_block_num = 56
        Unreused memory not eager var is  [ 164.52 MiB ] with mem_block_num = 889
        Eager Variable Tensor total memory is [ 1036.74 MiB ] with mem_block_num = 264
I20220824 03:16:12.144088 1458836 plan_util.cpp:965] 
 Graph name GraphBase_0 in Rank: 1, Device: 1 needs to allocate [ 3918.12 MiB ] device memory. 
   In general, Chunk id: 1  memory is [ 2716.86 MiB ] with mem_block_num = 56
        Unreused memory not eager var is  [ 164.52 MiB ] with mem_block_num = 889
        Eager Variable Tensor total memory is [ 1036.74 MiB ] with mem_block_num = 264
I20220824 03:16:12.144102 1458836 plan_util.cpp:1002] ========================= In Device : 0 Chunk Memory info details:
I20220824 03:16:12.144106 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 95 has num = 942 tensor with mem size = 2388.19
I20220824 03:16:12.144758 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 72 has num = 1 tensor with mem size = 24.8381
I20220824 03:16:12.144766 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 91 has num = 1 tensor with mem size = 24.8381
I20220824 03:16:12.144771 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 62 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144778 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 100 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144783 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 78 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144788 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 56 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144793 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 109 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144798 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 98 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144804 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 54 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144809 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 23 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144814 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 86 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144819 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 88 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144825 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 64 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144830 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 80 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.144835 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 49 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144843 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 79 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144848 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 83 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144853 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 87 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144858 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 99 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144862 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 60 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144867 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 104 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144872 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 107 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144877 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 68 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144882 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 74 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144887 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 67 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144891 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 63 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144898 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 55 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144903 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 96 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144908 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 82 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144913 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 75 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144918 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 103 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144923 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 59 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.144928 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 90 has num = 1 tensor with mem size = 1.04858
I20220824 03:16:12.144932 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 89 has num = 1 tensor with mem size = 1.04858
I20220824 03:16:12.144938 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 51 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144943 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 97 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144948 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 101 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144953 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 111 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144958 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 58 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144963 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 66 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144966 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 81 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144971 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 73 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144976 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 61 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144984 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 102 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144989 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 105 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144994 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 84 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.144999 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 77 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.145004 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 57 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.145009 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 65 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.145012 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 85 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.145017 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 76 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.145022 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 53 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.145027 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 93 has num = 1 tensor with mem size = 0.000512
I20220824 03:16:12.145033 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 94 has num = 1 tensor with mem size = 0.000512
I20220824 03:16:12.145038 1458836 plan_util.cpp:1007]      In Device: 0 Chunk id: 0 MemBlock id: 92 has num = 1 tensor with mem size = 0.000512
I20220824 03:16:12.145273 1458836 plan_util.cpp:1002] ========================= In Device : 1 Chunk Memory info details:
I20220824 03:16:12.145277 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 71 has num = 942 tensor with mem size = 2388.19
I20220824 03:16:12.145922 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 1 has num = 1 tensor with mem size = 24.8381
I20220824 03:16:12.145928 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 37 has num = 1 tensor with mem size = 24.8381
I20220824 03:16:12.145933 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 50 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145938 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 29 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145943 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 20 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145948 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 12 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145953 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 6 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145958 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 31 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145963 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 47 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145968 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 4 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145973 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 39 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145978 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 14 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145983 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 41 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145988 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 22 has num = 1 tensor with mem size = 16.7772
I20220824 03:16:12.145993 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 21 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146001 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 35 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146006 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 43 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146011 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 30 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146016 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 40 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146021 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 108 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146026 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 27 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146031 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 48 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146035 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 9 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146040 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 34 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146045 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 18 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146050 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 17 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146055 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 110 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146062 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 10 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146066 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 5 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146071 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 13 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146076 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 44 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146081 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 26 has num = 1 tensor with mem size = 4.1943
I20220824 03:16:12.146086 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 2 has num = 1 tensor with mem size = 1.04858
I20220824 03:16:12.146091 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 3 has num = 1 tensor with mem size = 1.04858
I20220824 03:16:12.146095 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 11 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146100 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 106 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146106 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 42 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146109 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 16 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146114 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 7 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146119 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 19 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146124 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 46 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146128 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 38 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146133 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 25 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146138 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 15 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146144 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 24 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146149 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 45 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146154 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 33 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146159 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 36 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146163 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 32 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146168 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 52 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146173 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 28 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146178 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 8 has num = 1 tensor with mem size = 0.004096
I20220824 03:16:12.146183 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 69 has num = 1 tensor with mem size = 0.000512
I20220824 03:16:12.146188 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 70 has num = 1 tensor with mem size = 0.000512
I20220824 03:16:12.146193 1458836 plan_util.cpp:1007]      In Device: 1 Chunk id: 1 MemBlock id: 0 has num = 1 tensor with mem size = 0.000512
I20220824 03:16:12.297598 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_k.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.298003 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.298417 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.298717 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.299015 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.299295 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.299566 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.299998 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.300319 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.300658 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.302594 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.emb.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.303520 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.303862 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.304200 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.304538 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.304867 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.305208 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.305552 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.305860 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_k.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.306471 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.306802 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.307133 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.307451 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.308032 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.308295 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.308568 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.308794 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.309015 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.309239 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.309742 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.310276 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.310503 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.310752 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.310979 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.311224 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.311455 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.311673 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.312178 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.312469 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.312960 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.313196 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.313422 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.313645 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.313910 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.314121 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.314648 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.314872 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.315085 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.315292 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_q.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.315508 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.315768 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.316011 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_q.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.316279 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.316524 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.316762 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.317010 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.317255 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.317711 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.318188 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.318442 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.318711 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.318943 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.319175 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.319419 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.319658 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.320031 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: System-Train-TrainStep created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.320274 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.320715 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.320943 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.321146 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.321347 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.321547 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.321748 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.321965 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.322177 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.322599 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.323032 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.323264 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.323473 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.323678 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.323894 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.324318 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.324537 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.324997 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.325464 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.325881 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.326144 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.326582 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.326778 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.326989 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.327184 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.327383 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.327821 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.emb.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.328258 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.328449 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.328624 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.328804 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.329202 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.329413 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.329599 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.329794 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.330193 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.330384 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.330562 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.330929 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.331342 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.331535 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.331918 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.332123 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.332314 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.332733 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.332931 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.333119 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.333305 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.333492 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.333675 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.333871 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.334065 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.334252 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.334448 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.334637 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.334815 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.335000 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.335196 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.335624 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.335923 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.336304 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.336549 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.336798 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.337258 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.337716 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.337954 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.338176 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.338393 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.338613 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.339028 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.339257 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.339476 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.339725 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.339951 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.340169 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.340411 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.340864 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.341094 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.341315 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.341528 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.341964 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.342196 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.342412 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.342638 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.342856 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.343070 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.343478 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.343711 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.343940 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.344365 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.344789 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.345027 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.345245 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.345464 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.345685 1458836 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 03:16:12.361506 1458836 thread_manager.cpp:53]  Actor thread: 524306 created.
I20220824 03:16:12.361552 1458836 thread_manager.cpp:53]  Actor thread: 524308 created.
I20220824 03:16:12.361584 1458836 thread_manager.cpp:53]  Actor thread: 524356 created.
I20220824 03:16:12.361627 1458836 thread_manager.cpp:53]  Actor thread: 524297 created.
I20220824 03:16:12.361696 1458836 thread_manager.cpp:53]  Actor thread: 524296 created.
I20220824 03:16:12.361724 1458836 thread_manager.cpp:53]  Actor thread: 524355 created.
I20220824 03:16:12.361750 1458836 thread_manager.cpp:53]  Actor thread: 524352 created.
I20220824 03:16:12.361778 1458836 thread_manager.cpp:53]  Actor thread: 524293 created.
I20220824 03:16:12.361807 1458836 thread_manager.cpp:53]  Actor thread: 524290 created.
I20220824 03:16:12.363304 1458836 thread_manager.cpp:53]  Actor thread: 1048579 created.
I20220824 03:16:12.363346 1458836 thread_manager.cpp:53]  Actor thread: 524305 created.
I20220824 03:16:12.363374 1458836 thread_manager.cpp:53]  Actor thread: 524292 created.
I20220824 03:16:12.363420 1458836 thread_manager.cpp:53]  Actor thread: 524291 created.
I20220824 03:16:12.363459 1458836 thread_manager.cpp:53]  Actor thread: 524302 created.
I20220824 03:16:12.364506 1458836 thread_manager.cpp:53]  Actor thread: 1048576 created.
I20220824 03:16:12.364555 1458836 thread_manager.cpp:53]  Actor thread: 524289 created.
I20220824 03:16:12.364591 1458836 thread_manager.cpp:53]  Actor thread: 524288 created.
I20220824 03:16:12.364619 1458836 thread_manager.cpp:53]  Actor thread: 524354 created.
I20220824 03:16:12.364645 1458836 thread_manager.cpp:53]  Actor thread: 524295 created.
I20220824 03:16:12.364673 1458836 thread_manager.cpp:53]  Actor thread: 524294 created.
I20220824 03:16:12.364706 1458836 thread_manager.cpp:53]  Actor thread: 524353 created.
I20220824 03:16:12.364732 1458836 thread_manager.cpp:53]  Actor thread: 524310 created.
I20220824 03:16:12.364758 1458836 thread_manager.cpp:53]  Actor thread: 524309 created.
I20220824 03:16:12.364786 1458836 thread_manager.cpp:53]  Actor thread: 524299 created.
I20220824 03:16:12.364812 1458836 thread_manager.cpp:53]  Actor thread: 524298 created.
I20220824 03:16:12.364837 1458836 thread_manager.cpp:53]  Actor thread: 524357 created.
I20220824 03:16:12.364868 1458836 thread_manager.cpp:53]  Actor thread: 524301 created.
I20220824 03:16:12.365630 1458836 thread_manager.cpp:53]  Actor thread: 1048578 created.
I20220824 03:16:12.365660 1458836 thread_manager.cpp:53]  Actor thread: 524304 created.
I20220824 03:16:12.365680 1458836 thread_manager.cpp:53]  Actor thread: 524311 created.
I20220824 03:16:12.367305 1458836 thread_manager.cpp:53]  Actor thread: 1048577 created.
I20220824 03:16:12.367338 1458836 thread_manager.cpp:53]  Actor thread: 524303 created.
I20220824 03:16:12.367363 1458836 thread_manager.cpp:53]  Actor thread: 524307 created.
I20220824 03:16:12.367390 1458836 thread_manager.cpp:53]  Actor thread: 524300 created.
(GRAPH:GraphBase_0:GraphBase) building plan Done! Cost time: 2.89s.

GraphBase_0 with operators:
(GRAPH:GraphBase_0:GraphBase): (
  (CONFIG:config:GraphConfig(training=True, ))
  (INPUT:_GraphBase_0_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
         sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
  (INPUT:_GraphBase_0_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
         sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
  (MODULE:model:GPT()): (
    (INPUT:_model_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
           sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
           dtype=oneflow.int64))
    (INPUT:_model_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
           sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
           dtype=oneflow.int64))
    (BUFFER:model.copy_mask:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
           sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32)): ()
    (MODULE:model.emb:VocabEmbedding(num_embeddings=6064, embedding_dim=1024)): (
      (INPUT:_model.emb_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
             dtype=oneflow.int64))
      (PARAMETER:model.emb.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.emb-gather-0(model.emb.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16)), System-Boxing-Identity-222/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64))) -> (model.emb-gather-0/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.emb-unsorted_segment_sum_like-695(model.blocks.0.ln0-layer_norm_grad-694/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), System-Boxing-Identity-222/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), model.emb.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.emb-unsorted_segment_sum_like-695/out_0:(sbp=(P), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.emb-gather-0-out_0-cast_h2f(model.emb-gather-0/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.emb-gather-0-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.emb-unsorted_segment_sum_like-695_out_0_pinned_identity-out_0-cast_h2f(model.emb-unsorted_segment_sum_like-695/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.emb-unsorted_segment_sum_like-695_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.emb_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
    )
    (MODULE:model.blocks:Sequential()): (
      (INPUT:_model.blocks_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
      (MODULE:model.blocks.0:Block()): (
        (INPUT:_model.blocks.0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
        (MODULE:model.blocks.0.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.0.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.0.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.0.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm-2(model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm_param_grad-686(model.blocks.0.att.time_shift-add_n-683-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1-layer_norm_param_grad-686/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm_param_grad-686/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm_grad-687(model.blocks.0.att.time_shift-add_n-683/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln0-layer_norm-1-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln1-layer_norm-2/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln1-layer_norm_grad-687/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight-out-cast_f2h(model.blocks.0.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-add_n-688-out_0-cast_h2f(model.blocks.0.ln1-layer_norm_grad-687/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln1-add_n-688-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight_optimizer(model.blocks.0.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm_param_grad-686/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias_optimizer(model.blocks.0.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm_param_grad-686/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.0.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.0.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.0.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.0.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2-layer_norm-29(model.blocks.0.att.output-broadcast_matmul-27/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln2-layer_norm-29/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-29/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-29/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2-layer_norm_param_grad-666(model.blocks.0.ffn.time_shift-add_n-663-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0-add_n-28-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-29/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-29/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln2-layer_norm_param_grad-666/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm_param_grad-666/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2-layer_norm_grad-667(model.blocks.0.ffn.time_shift-add_n-663/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.output-broadcast_matmul-27/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-29/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-29/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln2-layer_norm_grad-667/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias-out-cast_f2h(model.blocks.0.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight-out-cast_f2h(model.blocks.0.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight_optimizer(model.blocks.0.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm_param_grad-666/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias_optimizer(model.blocks.0.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm_param_grad-666/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.0.ln0:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.0.ln0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
          (PARAMETER:model.blocks.0.ln0.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ln0.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.0.ln0.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm-1(model.emb-gather-0-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm_param_grad-693(model.blocks.0.ln1-add_n-688-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.emb-gather-0-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0-layer_norm_param_grad-693/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm_param_grad-693/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm_grad-694(model.blocks.0.ln1-layer_norm_grad-687/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.emb-gather-0/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln0-layer_norm-1/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln0-layer_norm_grad-694/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight-out-cast_f2h(model.blocks.0.ln0.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm-1-y_0-cast_f2h(model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0-layer_norm-1-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight_optimizer(model.blocks.0.ln0.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm_param_grad-693/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln0.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias_optimizer(model.blocks.0.ln0.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm_param_grad-693/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln0.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ln0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.0.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.0.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.0.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.0.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.0.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.0.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.0.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.0.att.time_shift-pad-3(model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-pad-682(model.blocks.0.att-broadcast_mul-679/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-pad-682/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-add_n-683([model.blocks.0.att.time_shift-pad-682/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-broadcast_mul-676/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att.time_shift-add_n-683/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f(model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-add_n-683-out_0-cast_h2f(model.blocks.0.att.time_shift-add_n-683/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-add_n-683-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.0.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-broadcast_matmul-20(model.blocks.0.att-add_n-8-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-broadcast_matmul-20/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key.weight-out-cast_f2h(model.blocks.0.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-broadcast_matmul-22(model.blocks.0.att-add_n-13-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-broadcast_matmul-22/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value.weight-out-cast_f2h(model.blocks.0.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul-24(model.blocks.0.att-add_n-18-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul-24/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul-674(model.blocks.0.att-sigmoid_v2_grad-673-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul-674/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul_grad_b-675(model.blocks.0.att-sigmoid_v2_grad-673-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-add_n-18-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul_grad_b-675/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight-out-cast_f2h(model.blocks.0.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f(model.blocks.0.att.receptance-broadcast_matmul-24/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul_grad_b-675_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att.receptance-broadcast_matmul_grad_b-675/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul_grad_b-675_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight_optimizer(model.blocks.0.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.receptance-broadcast_matmul_grad_b-675_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.0.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
            (PARAMETER:model.blocks.0.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul-27(model.blocks.0.att-sigmoid_v2-25-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul-27/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul-671(model.blocks.0.ln2-layer_norm_grad-667/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul-671/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul_grad_b-672(model.blocks.0.ln2-layer_norm_grad-667/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-sigmoid_v2-25-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul_grad_b-672/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight-out-cast_f2h(model.blocks.0.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul-671-out_0-cast_h2f(model.blocks.0.att.output-broadcast_matmul-671/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul-671-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul_grad_b-672_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att.output-broadcast_matmul_grad_b-672/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul_grad_b-672_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight_optimizer(model.blocks.0.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.output-broadcast_matmul_grad_b-672_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.0.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-4(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-4/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-5(model.blocks.0.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_mul-5/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-6(model.blocks.0.att-scalar_mul-5/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-6/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-7(model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-scalar_add-6/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-7/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-8([model.blocks.0.att-broadcast_mul-4/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-7/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))]) -> (model.blocks.0.att-add_n-8/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-9(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-9/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-10(model.blocks.0.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_mul-10/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-11(model.blocks.0.att-scalar_mul-10/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-11/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-12(model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-scalar_add-11/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-12/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-13([model.blocks.0.att-broadcast_mul-9/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-12/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))]) -> (model.blocks.0.att-add_n-13/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-14(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-14/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-15(model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_mul-15/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-16(model.blocks.0.att-scalar_mul-15/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-16/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-17(model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-scalar_add-16/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-17/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-18([model.blocks.0.att-broadcast_mul-14/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-17/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))]) -> (model.blocks.0.att-add_n-18/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-sigmoid_v2-25(model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-sigmoid_v2-25/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-sigmoid_v2_grad-673(model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.output-broadcast_matmul-671-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-sigmoid_v2_grad-673/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-676(model.blocks.0.att.receptance-broadcast_matmul-674/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-676/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-677(model.blocks.0.att.receptance-broadcast_matmul-674/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-677/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-678(model.blocks.0.att-broadcast_mul-677/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-678/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-679(model.blocks.0.att.receptance-broadcast_matmul-674/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-16-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-679/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-680(model.blocks.0.att.receptance-broadcast_matmul-674/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-680/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-681(model.blocks.0.att-broadcast_mul-680/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-16-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-681/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-689(model.blocks.0.att-reduce_sum_like-681/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-scalar_mul-689/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-690([model.blocks.0.att-scalar_mul-689/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-reduce_sum_like-678/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att-add_n-690/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r-out-cast_f2h(model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-sigmoid_v2-25-y_0-cast_f2h(model.blocks.0.att-sigmoid_v2-25/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-sigmoid_v2-25-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-16-out_0-cast_f2h(model.blocks.0.att-scalar_add-16/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-16-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-8-out_0-cast_f2h(model.blocks.0.att-add_n-8/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-add_n-8-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-18-out_0-cast_f2h(model.blocks.0.att-add_n-18/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-add_n-18-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-13-out_0-cast_f2h(model.blocks.0.att-add_n-13/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-add_n-13-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-sigmoid_v2_grad-673-dx_0-cast_f2h(model.blocks.0.att-sigmoid_v2_grad-673/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-sigmoid_v2_grad-673-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-690_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att-add_n-690/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-add_n-690_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r_optimizer(model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att-add_n-690_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.0.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.0.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.0.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.0.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.0.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.0.ffn.time_shift-pad-30(model.blocks.0.ln2-layer_norm-29/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.time_shift-pad-30/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.time_shift-pad-662(model.blocks.0.ffn-add_n-661/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.time_shift-pad-662/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.time_shift-add_n-663([model.blocks.0.ffn.time_shift-pad-662/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-add_n-657/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn.time_shift-add_n-663/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.time_shift-add_n-663-out_0-cast_h2f(model.blocks.0.ffn.time_shift-add_n-663/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.time_shift-add_n-663-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.0.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul-42(model.blocks.0.ffn-add_n-35/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul-42/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul-650(model.blocks.0.ffn-relu_grad-643/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul-650/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul_grad_b-651(model.blocks.0.ffn-relu_grad-643/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-add_n-35/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul_grad_b-651/out_0:(sbp=(P), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight-out-cast_f2h(model.blocks.0.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul_grad_b-651_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn.key-broadcast_matmul_grad_b-651/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul_grad_b-651_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight_optimizer(model.blocks.0.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.key-broadcast_matmul_grad_b-651_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul-48(model.blocks.0.ffn-add_n-40/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul-48/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul-640(model.blocks.0.ffn-sigmoid_v2_grad-637-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641(model.blocks.0.ffn-sigmoid_v2_grad-637-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-add_n-40/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight-out-cast_f2h(model.blocks.0.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul-48-out_0-cast_h2f(model.blocks.0.ffn.receptance-broadcast_matmul-48/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul-48-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight_optimizer(model.blocks.0.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-641_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.0.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.0.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-46(model.blocks.0.ffn-square-44-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-46/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-638(model.blocks.0.ffn-broadcast_mul-636/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-638/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul_grad_b-639(model.blocks.0.ffn-broadcast_mul-636/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-square-44-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul_grad_b-639/out_0:(sbp=(P), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight-out-cast_f2h(model.blocks.0.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-638-out_0-cast_h2f(model.blocks.0.ffn.value-broadcast_matmul-638/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-638-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-46-out_0-cast_h2f(model.blocks.0.ffn.value-broadcast_matmul-46/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-46-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul_grad_b-639_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn.value-broadcast_matmul_grad_b-639/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul_grad_b-639_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight_optimizer(model.blocks.0.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.0.ffn.value-broadcast_matmul_grad_b-639_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.0.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.0.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-31(model.blocks.0.ln2-layer_norm-29/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-31/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-32(model.blocks.0.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_mul-32/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-33(model.blocks.0.ffn-scalar_mul-32/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-33/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-34(model.blocks.0.ffn.time_shift-pad-30/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-33-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-34/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-35([model.blocks.0.ffn-broadcast_mul-31/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-34/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-35/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-36(model.blocks.0.ln2-layer_norm-29/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-36/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-37(model.blocks.0.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_mul-37/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-38(model.blocks.0.ffn-scalar_mul-37/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-38/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-39(model.blocks.0.ffn.time_shift-pad-30/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-38-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-39/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-40([model.blocks.0.ffn-broadcast_mul-36/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-39/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-40/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-relu-43(model.blocks.0.ffn.key-broadcast_matmul-42/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-relu-43/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square-44(model.blocks.0.ffn-relu-43-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square-44/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2-49(model.blocks.0.ffn.receptance-broadcast_matmul-48-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2-49/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-50(model.blocks.0.ffn-sigmoid_v2-49-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.value-broadcast_matmul-46/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-50/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-635(model.blocks.1.ln1-add_n-632-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.value-broadcast_matmul-46-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-broadcast_mul-635/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-636(model.blocks.1.ln1-layer_norm_grad-631/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-sigmoid_v2-49-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-636/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2_grad-637(model.blocks.0.ffn.receptance-broadcast_matmul-48-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn-broadcast_mul-635/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2_grad-637/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square_grad-642(model.blocks.0.ffn-relu-43-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.0.ffn.value-broadcast_matmul-638-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square_grad-642/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-relu_grad-643(model.blocks.0.ffn-square_grad-642-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-relu-43/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-relu_grad-643/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-644(model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-644/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-645(model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-29/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-645/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-646(model.blocks.0.ffn-broadcast_mul-645/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-646/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-647(model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-38-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-647/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-648(model.blocks.0.ffn.receptance-broadcast_matmul-640/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_shift-pad-30/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-648/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-649(model.blocks.0.ffn-broadcast_mul-648/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-38-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-649/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-652(model.blocks.0.ffn-reduce_sum_like-649/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-scalar_mul-652/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-653([model.blocks.0.ffn-scalar_mul-652/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-reduce_sum_like-646/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-653/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-654(model.blocks.0.ffn.key-broadcast_matmul-650/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-654/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-655(model.blocks.0.ffn.key-broadcast_matmul-650/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-29/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-655/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-656(model.blocks.0.ffn-broadcast_mul-655/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-656/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-657([model.blocks.0.ffn-broadcast_mul-654/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-644/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-657/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-658(model.blocks.0.ffn.key-broadcast_matmul-650/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-33-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-658/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-659(model.blocks.0.ffn.key-broadcast_matmul-650/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_shift-pad-30/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-659/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-660(model.blocks.0.ffn-broadcast_mul-659/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-33-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-660/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-661([model.blocks.0.ffn-broadcast_mul-658/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-647/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-661/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-669(model.blocks.0.ffn-reduce_sum_like-660/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-scalar_mul-669/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-670([model.blocks.0.ffn-scalar_mul-669/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-reduce_sum_like-656/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-670/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2_grad-637-dx_0-cast_f2h(model.blocks.0.ffn-sigmoid_v2_grad-637/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2_grad-637-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-33-out_0-cast_f2h(model.blocks.0.ffn-scalar_add-33/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-33-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2-49-y_0-cast_f2h(model.blocks.0.ffn-sigmoid_v2-49/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2-49-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-38-out_0-cast_f2h(model.blocks.0.ffn-scalar_add-38/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-38-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square-44-y_0-cast_f2h(model.blocks.0.ffn-square-44/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square-44-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square_grad-642-dx_0-cast_f2h(model.blocks.0.ffn-square_grad-642/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square_grad-642-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r-out-cast_f2h(model.blocks.0.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k-out-cast_f2h(model.blocks.0.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-relu-43-y_0-cast_h2f(model.blocks.0.ffn-relu-43/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-relu-43-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-670_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn-add_n-670/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-add_n-670_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-653_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn-add_n-653/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-add_n-653_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k_optimizer(model.blocks.0.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn-add_n-670_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r_optimizer(model.blocks.0.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn-add_n-653_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.0-add_n-51([model.blocks.0.att.output-broadcast_matmul-27/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-50/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0-add_n-51/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.0-add_n-28-out_0-cast_h2f(model.blocks.0.att.output-broadcast_matmul-27/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0-add_n-28-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.0-add_n-51-out_0-cast_h2f(model.blocks.0-add_n-51/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0-add_n-51-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.1:Block()): (
        (INPUT:_model.blocks.1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.1.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.1.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.1.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.1.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-layer_norm-52(model.blocks.0-add_n-51/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln1-layer_norm-52/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-52/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-52/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-layer_norm_param_grad-630(model.blocks.1.att.time_shift-add_n-627-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0-add_n-51-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-52/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-52/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln1-layer_norm_param_grad-630/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm_param_grad-630/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-layer_norm_grad-631(model.blocks.1.att.time_shift-add_n-627/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0-add_n-51/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-52/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-52/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln1-layer_norm_grad-631/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias-out-cast_f2h(model.blocks.1.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight-out-cast_f2h(model.blocks.1.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-add_n-632-out_0-cast_h2f(model.blocks.1.ln1-layer_norm_grad-631/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln1-add_n-632-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight_optimizer(model.blocks.1.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm_param_grad-630/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias_optimizer(model.blocks.1.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm_param_grad-630/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.1.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.1.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.1.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.1.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2-layer_norm-79(model.blocks.1.att.output-broadcast_matmul-77/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln2-layer_norm-79/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-79/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-79/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2-layer_norm_param_grad-610(model.blocks.1.ffn.time_shift-add_n-607-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1-add_n-78-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-79/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-79/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln2-layer_norm_param_grad-610/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm_param_grad-610/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2-layer_norm_grad-611(model.blocks.1.ffn.time_shift-add_n-607/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.output-broadcast_matmul-77/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-79/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-79/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln2-layer_norm_grad-611/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight-out-cast_f2h(model.blocks.1.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias-out-cast_f2h(model.blocks.1.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight_optimizer(model.blocks.1.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm_param_grad-610/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias_optimizer(model.blocks.1.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm_param_grad-610/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.1.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.1.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.1.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.1.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.1.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.1.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.1.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.1.att.time_shift-pad-53(model.blocks.1.ln1-layer_norm-52/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.time_shift-pad-53/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.time_shift-pad-626(model.blocks.1.att-broadcast_mul-623/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.time_shift-pad-626/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.time_shift-add_n-627([model.blocks.1.att.time_shift-pad-626/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-620/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att.time_shift-add_n-627/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.time_shift-add_n-627-out_0-cast_h2f(model.blocks.1.att.time_shift-add_n-627/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.time_shift-add_n-627-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.1.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key-broadcast_matmul-70(model.blocks.1.att-add_n-58/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.key-broadcast_matmul-70/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key.weight-out-cast_f2h(model.blocks.1.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value-broadcast_matmul-72(model.blocks.1.att-add_n-63/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.value-broadcast_matmul-72/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value.weight-out-cast_f2h(model.blocks.1.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul-74(model.blocks.1.att-add_n-68/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul-74/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul-618(model.blocks.1.att-sigmoid_v2_grad-617-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul-618/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul_grad_b-619(model.blocks.1.att-sigmoid_v2_grad-617-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-add_n-68/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul_grad_b-619/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight-out-cast_f2h(model.blocks.1.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul-74-out_0-cast_h2f(model.blocks.1.att.receptance-broadcast_matmul-74/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul-74-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul_grad_b-619_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att.receptance-broadcast_matmul_grad_b-619/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul_grad_b-619_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight_optimizer(model.blocks.1.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.receptance-broadcast_matmul_grad_b-619_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.1.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
            (PARAMETER:model.blocks.1.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul-77(model.blocks.1.att-sigmoid_v2-75-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul-77/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul-615(model.blocks.1.ln2-layer_norm_grad-611/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul-615/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul_grad_b-616(model.blocks.1.ln2-layer_norm_grad-611/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-sigmoid_v2-75-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul_grad_b-616/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight-out-cast_f2h(model.blocks.1.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul-615-out_0-cast_h2f(model.blocks.1.att.output-broadcast_matmul-615/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul-615-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul_grad_b-616_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att.output-broadcast_matmul_grad_b-616/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul_grad_b-616_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight_optimizer(model.blocks.1.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.output-broadcast_matmul_grad_b-616_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.1.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-54(model.blocks.1.ln1-layer_norm-52/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-54/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-55(model.blocks.1.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_mul-55/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-56(model.blocks.1.att-scalar_mul-55/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-56/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-57(model.blocks.1.att.time_shift-pad-53/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-56-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-57/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-58([model.blocks.1.att-broadcast_mul-54/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-57/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-58/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-59(model.blocks.1.ln1-layer_norm-52/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-59/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-60(model.blocks.1.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_mul-60/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-61(model.blocks.1.att-scalar_mul-60/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-61/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-62(model.blocks.1.att.time_shift-pad-53/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-61-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-62/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-63([model.blocks.1.att-broadcast_mul-59/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-62/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-63/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-64(model.blocks.1.ln1-layer_norm-52/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-64/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-65(model.blocks.1.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_mul-65/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-66(model.blocks.1.att-scalar_mul-65/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-66/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-67(model.blocks.1.att.time_shift-pad-53/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-66-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-67/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-68([model.blocks.1.att-broadcast_mul-64/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-67/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-68/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-sigmoid_v2-75(model.blocks.1.att.receptance-broadcast_matmul-74-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-sigmoid_v2-75/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-sigmoid_v2_grad-617(model.blocks.1.att.receptance-broadcast_matmul-74-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.output-broadcast_matmul-615-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-sigmoid_v2_grad-617/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-620(model.blocks.1.att.receptance-broadcast_matmul-618/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-620/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-621(model.blocks.1.att.receptance-broadcast_matmul-618/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-52/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-621/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-622(model.blocks.1.att-broadcast_mul-621/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-622/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-623(model.blocks.1.att.receptance-broadcast_matmul-618/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-66-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-623/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-624(model.blocks.1.att.receptance-broadcast_matmul-618/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_shift-pad-53/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-624/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-625(model.blocks.1.att-broadcast_mul-624/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-66-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-625/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-633(model.blocks.1.att-reduce_sum_like-625/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-scalar_mul-633/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-634([model.blocks.1.att-scalar_mul-633/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-reduce_sum_like-622/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-634/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-sigmoid_v2-75-y_0-cast_f2h(model.blocks.1.att-sigmoid_v2-75/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-sigmoid_v2-75-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-56-out_0-cast_f2h(model.blocks.1.att-scalar_add-56/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-56-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-61-out_0-cast_f2h(model.blocks.1.att-scalar_add-61/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-61-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-sigmoid_v2_grad-617-dx_0-cast_f2h(model.blocks.1.att-sigmoid_v2_grad-617/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-sigmoid_v2_grad-617-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-66-out_0-cast_f2h(model.blocks.1.att-scalar_add-66/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-66-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r-out-cast_f2h(model.blocks.1.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_k-out-cast_f2h(model.blocks.1.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_v-out-cast_f2h(model.blocks.1.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-634_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att-add_n-634/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-add_n-634_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r_optimizer(model.blocks.1.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att-add_n-634_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.1.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.1.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.1.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.1.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.1.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.1.ffn.time_shift-pad-80(model.blocks.1.ln2-layer_norm-79/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.time_shift-pad-80/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.time_shift-pad-606(model.blocks.1.ffn-add_n-605/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.time_shift-pad-606/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.time_shift-add_n-607([model.blocks.1.ffn.time_shift-pad-606/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-add_n-601/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn.time_shift-add_n-607/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.time_shift-add_n-607-out_0-cast_h2f(model.blocks.1.ffn.time_shift-add_n-607/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.time_shift-add_n-607-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.1.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul-92(model.blocks.1.ffn-add_n-85/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul-92/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul-594(model.blocks.1.ffn-relu_grad-587/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul-594/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul_grad_b-595(model.blocks.1.ffn-relu_grad-587/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-add_n-85/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul_grad_b-595/out_0:(sbp=(P), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight-out-cast_f2h(model.blocks.1.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul_grad_b-595_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn.key-broadcast_matmul_grad_b-595/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul_grad_b-595_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight_optimizer(model.blocks.1.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.key-broadcast_matmul_grad_b-595_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul-98(model.blocks.1.ffn-add_n-90/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul-98/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul-584(model.blocks.1.ffn-sigmoid_v2_grad-581-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585(model.blocks.1.ffn-sigmoid_v2_grad-581-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-add_n-90/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight-out-cast_f2h(model.blocks.1.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul-98-out_0-cast_h2f(model.blocks.1.ffn.receptance-broadcast_matmul-98/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul-98-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight_optimizer(model.blocks.1.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-585_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.1.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.1.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-96(model.blocks.1.ffn-square-94-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-96/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-582(model.blocks.1.ffn-broadcast_mul-580/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-582/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul_grad_b-583(model.blocks.1.ffn-broadcast_mul-580/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-square-94-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul_grad_b-583/out_0:(sbp=(P), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight-out-cast_f2h(model.blocks.1.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-96-out_0-cast_h2f(model.blocks.1.ffn.value-broadcast_matmul-96/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-96-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-582-out_0-cast_h2f(model.blocks.1.ffn.value-broadcast_matmul-582/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-582-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul_grad_b-583_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn.value-broadcast_matmul_grad_b-583/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul_grad_b-583_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight_optimizer(model.blocks.1.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.1.ffn.value-broadcast_matmul_grad_b-583_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.1.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.1.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-81(model.blocks.1.ln2-layer_norm-79/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-81/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-82(model.blocks.1.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_mul-82/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-83(model.blocks.1.ffn-scalar_mul-82/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-83/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-84(model.blocks.1.ffn.time_shift-pad-80/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-83-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-84/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-85([model.blocks.1.ffn-broadcast_mul-81/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-84/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-85/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-86(model.blocks.1.ln2-layer_norm-79/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-86/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-87(model.blocks.1.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_mul-87/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-88(model.blocks.1.ffn-scalar_mul-87/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-88/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-89(model.blocks.1.ffn.time_shift-pad-80/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-88-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-89/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-90([model.blocks.1.ffn-broadcast_mul-86/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-89/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-90/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-relu-93(model.blocks.1.ffn.key-broadcast_matmul-92/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-relu-93/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square-94(model.blocks.1.ffn-relu-93-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square-94/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2-99(model.blocks.1.ffn.receptance-broadcast_matmul-98-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2-99/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-100(model.blocks.1.ffn-sigmoid_v2-99-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.value-broadcast_matmul-96/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-100/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-579(model.blocks.2.ln1-add_n-576-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.value-broadcast_matmul-96-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-broadcast_mul-579/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-580(model.blocks.2.ln1-layer_norm_grad-575/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-sigmoid_v2-99-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-580/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2_grad-581(model.blocks.1.ffn.receptance-broadcast_matmul-98-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn-broadcast_mul-579/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2_grad-581/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square_grad-586(model.blocks.1.ffn-relu-93-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.1.ffn.value-broadcast_matmul-582-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square_grad-586/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-relu_grad-587(model.blocks.1.ffn-square_grad-586-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-relu-93/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-relu_grad-587/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-588(model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-588/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-589(model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-79/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-589/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-590(model.blocks.1.ffn-broadcast_mul-589/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-590/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-591(model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-88-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-591/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-592(model.blocks.1.ffn.receptance-broadcast_matmul-584/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_shift-pad-80/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-592/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-593(model.blocks.1.ffn-broadcast_mul-592/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-88-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-593/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-596(model.blocks.1.ffn-reduce_sum_like-593/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-scalar_mul-596/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-597([model.blocks.1.ffn-scalar_mul-596/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-reduce_sum_like-590/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-597/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-598(model.blocks.1.ffn.key-broadcast_matmul-594/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-598/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-599(model.blocks.1.ffn.key-broadcast_matmul-594/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-79/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-599/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-600(model.blocks.1.ffn-broadcast_mul-599/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-600/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-601([model.blocks.1.ffn-broadcast_mul-598/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-588/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-601/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-602(model.blocks.1.ffn.key-broadcast_matmul-594/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-83-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-602/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-603(model.blocks.1.ffn.key-broadcast_matmul-594/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_shift-pad-80/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-603/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-604(model.blocks.1.ffn-broadcast_mul-603/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-83-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-604/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-605([model.blocks.1.ffn-broadcast_mul-602/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-591/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-605/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-613(model.blocks.1.ffn-reduce_sum_like-604/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-scalar_mul-613/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-614([model.blocks.1.ffn-scalar_mul-613/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-reduce_sum_like-600/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-614/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r-out-cast_f2h(model.blocks.1.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2_grad-581-dx_0-cast_f2h(model.blocks.1.ffn-sigmoid_v2_grad-581/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2_grad-581-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k-out-cast_f2h(model.blocks.1.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-88-out_0-cast_f2h(model.blocks.1.ffn-scalar_add-88/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-88-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-83-out_0-cast_f2h(model.blocks.1.ffn-scalar_add-83/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-83-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square_grad-586-dx_0-cast_f2h(model.blocks.1.ffn-square_grad-586/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square_grad-586-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2-99-y_0-cast_f2h(model.blocks.1.ffn-sigmoid_v2-99/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2-99-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square-94-y_0-cast_f2h(model.blocks.1.ffn-square-94/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square-94-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-relu-93-y_0-cast_h2f(model.blocks.1.ffn-relu-93/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-relu-93-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-597_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn-add_n-597/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-add_n-597_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-614_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn-add_n-614/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-add_n-614_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k_optimizer(model.blocks.1.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn-add_n-614_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r_optimizer(model.blocks.1.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn-add_n-597_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.1-add_n-101([model.blocks.1.att.output-broadcast_matmul-77/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-100/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1-add_n-101/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.1-add_n-101-out_0-cast_h2f(model.blocks.1-add_n-101/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1-add_n-101-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.1-add_n-78-out_0-cast_h2f(model.blocks.1.att.output-broadcast_matmul-77/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1-add_n-78-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.2:Block()): (
        (INPUT:_model.blocks.2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.2.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.2.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.2.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.2.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-layer_norm-102(model.blocks.1-add_n-101/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln1-layer_norm-102/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-102/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-102/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-layer_norm_param_grad-574(model.blocks.2.att.time_shift-add_n-571-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1-add_n-101-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-102/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-102/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln1-layer_norm_param_grad-574/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm_param_grad-574/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-layer_norm_grad-575(model.blocks.2.att.time_shift-add_n-571/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1-add_n-101/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-102/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-102/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln1-layer_norm_grad-575/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias-out-cast_f2h(model.blocks.2.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight-out-cast_f2h(model.blocks.2.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-add_n-576-out_0-cast_h2f(model.blocks.2.ln1-layer_norm_grad-575/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln1-add_n-576-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight_optimizer(model.blocks.2.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm_param_grad-574/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias_optimizer(model.blocks.2.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm_param_grad-574/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.2.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.2.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.2.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.2.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2-layer_norm-129(model.blocks.2.att.output-broadcast_matmul-127/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln2-layer_norm-129/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-129/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-129/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2-layer_norm_param_grad-554(model.blocks.2.ffn.time_shift-add_n-551-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2-add_n-128-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-129/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-129/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln2-layer_norm_param_grad-554/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm_param_grad-554/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2-layer_norm_grad-555(model.blocks.2.ffn.time_shift-add_n-551/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.output-broadcast_matmul-127/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-129/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-129/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln2-layer_norm_grad-555/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias-out-cast_f2h(model.blocks.2.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight-out-cast_f2h(model.blocks.2.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight_optimizer(model.blocks.2.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm_param_grad-554/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias_optimizer(model.blocks.2.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm_param_grad-554/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.2.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.2.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.2.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.2.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.2.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.2.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.2.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.2.att.time_shift-pad-103(model.blocks.2.ln1-layer_norm-102/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.time_shift-pad-103/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.time_shift-pad-570(model.blocks.2.att-broadcast_mul-567/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.time_shift-pad-570/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.time_shift-add_n-571([model.blocks.2.att.time_shift-pad-570/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-564/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att.time_shift-add_n-571/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.time_shift-add_n-571-out_0-cast_h2f(model.blocks.2.att.time_shift-add_n-571/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.time_shift-add_n-571-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.2.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key-broadcast_matmul-120(model.blocks.2.att-add_n-108/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.key-broadcast_matmul-120/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key.weight-out-cast_f2h(model.blocks.2.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value-broadcast_matmul-122(model.blocks.2.att-add_n-113/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.value-broadcast_matmul-122/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value.weight-out-cast_f2h(model.blocks.2.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul-124(model.blocks.2.att-add_n-118/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul-124/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul-562(model.blocks.2.att-sigmoid_v2_grad-561-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul-562/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul_grad_b-563(model.blocks.2.att-sigmoid_v2_grad-561-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-add_n-118/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul_grad_b-563/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight-out-cast_f2h(model.blocks.2.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul-124-out_0-cast_h2f(model.blocks.2.att.receptance-broadcast_matmul-124/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul-124-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul_grad_b-563_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att.receptance-broadcast_matmul_grad_b-563/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul_grad_b-563_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight_optimizer(model.blocks.2.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.receptance-broadcast_matmul_grad_b-563_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.2.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
            (PARAMETER:model.blocks.2.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul-127(model.blocks.2.att-sigmoid_v2-125-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul-127/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul-559(model.blocks.2.ln2-layer_norm_grad-555/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul-559/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul_grad_b-560(model.blocks.2.ln2-layer_norm_grad-555/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-sigmoid_v2-125-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul_grad_b-560/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight-out-cast_f2h(model.blocks.2.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul-559-out_0-cast_h2f(model.blocks.2.att.output-broadcast_matmul-559/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul-559-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul_grad_b-560_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att.output-broadcast_matmul_grad_b-560/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul_grad_b-560_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight_optimizer(model.blocks.2.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.output-broadcast_matmul_grad_b-560_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.2.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-104(model.blocks.2.ln1-layer_norm-102/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-104/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-105(model.blocks.2.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_mul-105/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-106(model.blocks.2.att-scalar_mul-105/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-106/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-107(model.blocks.2.att.time_shift-pad-103/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-106-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-107/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-108([model.blocks.2.att-broadcast_mul-104/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-107/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-108/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-109(model.blocks.2.ln1-layer_norm-102/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-109/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-110(model.blocks.2.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_mul-110/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-111(model.blocks.2.att-scalar_mul-110/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-111/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-112(model.blocks.2.att.time_shift-pad-103/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-111-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-112/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-113([model.blocks.2.att-broadcast_mul-109/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-112/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-113/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-114(model.blocks.2.ln1-layer_norm-102/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-114/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-115(model.blocks.2.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_mul-115/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-116(model.blocks.2.att-scalar_mul-115/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-116/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-117(model.blocks.2.att.time_shift-pad-103/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-116-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-117/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-118([model.blocks.2.att-broadcast_mul-114/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-117/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-118/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-sigmoid_v2-125(model.blocks.2.att.receptance-broadcast_matmul-124-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-sigmoid_v2-125/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-sigmoid_v2_grad-561(model.blocks.2.att.receptance-broadcast_matmul-124-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.output-broadcast_matmul-559-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-sigmoid_v2_grad-561/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-564(model.blocks.2.att.receptance-broadcast_matmul-562/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-564/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-565(model.blocks.2.att.receptance-broadcast_matmul-562/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-102/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-565/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-566(model.blocks.2.att-broadcast_mul-565/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-566/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-567(model.blocks.2.att.receptance-broadcast_matmul-562/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-116-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-567/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-568(model.blocks.2.att.receptance-broadcast_matmul-562/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_shift-pad-103/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-568/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-569(model.blocks.2.att-broadcast_mul-568/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-116-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-569/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-577(model.blocks.2.att-reduce_sum_like-569/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-scalar_mul-577/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-578([model.blocks.2.att-scalar_mul-577/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-reduce_sum_like-566/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-578/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-106-out_0-cast_f2h(model.blocks.2.att-scalar_add-106/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-106-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-116-out_0-cast_f2h(model.blocks.2.att-scalar_add-116/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-116-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-sigmoid_v2-125-y_0-cast_f2h(model.blocks.2.att-sigmoid_v2-125/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-sigmoid_v2-125-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_v-out-cast_f2h(model.blocks.2.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r-out-cast_f2h(model.blocks.2.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-111-out_0-cast_f2h(model.blocks.2.att-scalar_add-111/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-111-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-sigmoid_v2_grad-561-dx_0-cast_f2h(model.blocks.2.att-sigmoid_v2_grad-561/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-sigmoid_v2_grad-561-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_k-out-cast_f2h(model.blocks.2.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-578_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att-add_n-578/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-add_n-578_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r_optimizer(model.blocks.2.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att-add_n-578_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.2.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.2.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.2.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.2.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.2.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.2.ffn.time_shift-pad-130(model.blocks.2.ln2-layer_norm-129/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.time_shift-pad-130/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.time_shift-pad-550(model.blocks.2.ffn-add_n-549/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.time_shift-pad-550/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.time_shift-add_n-551([model.blocks.2.ffn.time_shift-pad-550/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-add_n-545/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn.time_shift-add_n-551/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.time_shift-add_n-551-out_0-cast_h2f(model.blocks.2.ffn.time_shift-add_n-551/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.time_shift-add_n-551-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.2.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul-142(model.blocks.2.ffn-add_n-135/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul-142/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul-538(model.blocks.2.ffn-relu_grad-531/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul-538/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul_grad_b-539(model.blocks.2.ffn-relu_grad-531/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-add_n-135/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul_grad_b-539/out_0:(sbp=(P), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight-out-cast_f2h(model.blocks.2.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul_grad_b-539_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn.key-broadcast_matmul_grad_b-539/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul_grad_b-539_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight_optimizer(model.blocks.2.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.key-broadcast_matmul_grad_b-539_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul-148(model.blocks.2.ffn-add_n-140/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul-148/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul-528(model.blocks.2.ffn-sigmoid_v2_grad-525-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529(model.blocks.2.ffn-sigmoid_v2_grad-525-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-add_n-140/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight-out-cast_f2h(model.blocks.2.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul-148-out_0-cast_h2f(model.blocks.2.ffn.receptance-broadcast_matmul-148/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul-148-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight_optimizer(model.blocks.2.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-529_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.2.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.2.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-146(model.blocks.2.ffn-square-144-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-146/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-526(model.blocks.2.ffn-broadcast_mul-524/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-526/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul_grad_b-527(model.blocks.2.ffn-broadcast_mul-524/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-square-144-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul_grad_b-527/out_0:(sbp=(P), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight-out-cast_f2h(model.blocks.2.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-526-out_0-cast_h2f(model.blocks.2.ffn.value-broadcast_matmul-526/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-526-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-146-out_0-cast_h2f(model.blocks.2.ffn.value-broadcast_matmul-146/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-146-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul_grad_b-527_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn.value-broadcast_matmul_grad_b-527/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul_grad_b-527_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight_optimizer(model.blocks.2.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.2.ffn.value-broadcast_matmul_grad_b-527_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.2.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.2.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-131(model.blocks.2.ln2-layer_norm-129/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-131/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-132(model.blocks.2.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_mul-132/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-133(model.blocks.2.ffn-scalar_mul-132/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-133/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-134(model.blocks.2.ffn.time_shift-pad-130/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-133-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-134/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-135([model.blocks.2.ffn-broadcast_mul-131/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-134/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-135/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-136(model.blocks.2.ln2-layer_norm-129/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-136/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-137(model.blocks.2.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_mul-137/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-138(model.blocks.2.ffn-scalar_mul-137/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-138/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-139(model.blocks.2.ffn.time_shift-pad-130/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-138-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-139/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-140([model.blocks.2.ffn-broadcast_mul-136/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-139/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-140/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-relu-143(model.blocks.2.ffn.key-broadcast_matmul-142/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-relu-143/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square-144(model.blocks.2.ffn-relu-143-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square-144/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2-149(model.blocks.2.ffn.receptance-broadcast_matmul-148-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2-149/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-150(model.blocks.2.ffn-sigmoid_v2-149-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.value-broadcast_matmul-146/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-150/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-523(model.blocks.3.ln1-add_n-520-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.value-broadcast_matmul-146-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-broadcast_mul-523/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-524(model.blocks.3.ln1-layer_norm_grad-519/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-sigmoid_v2-149-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-524/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2_grad-525(model.blocks.2.ffn.receptance-broadcast_matmul-148-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn-broadcast_mul-523/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2_grad-525/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square_grad-530(model.blocks.2.ffn-relu-143-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.2.ffn.value-broadcast_matmul-526-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square_grad-530/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-relu_grad-531(model.blocks.2.ffn-square_grad-530-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-relu-143/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-relu_grad-531/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-532(model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-532/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-533(model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-129/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-533/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-534(model.blocks.2.ffn-broadcast_mul-533/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-534/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-535(model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-138-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-535/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-536(model.blocks.2.ffn.receptance-broadcast_matmul-528/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_shift-pad-130/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-536/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-537(model.blocks.2.ffn-broadcast_mul-536/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-138-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-537/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-540(model.blocks.2.ffn-reduce_sum_like-537/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-scalar_mul-540/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-541([model.blocks.2.ffn-scalar_mul-540/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-reduce_sum_like-534/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-541/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-542(model.blocks.2.ffn.key-broadcast_matmul-538/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-542/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-543(model.blocks.2.ffn.key-broadcast_matmul-538/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-129/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-543/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-544(model.blocks.2.ffn-broadcast_mul-543/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-544/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-545([model.blocks.2.ffn-broadcast_mul-542/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-532/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-545/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-546(model.blocks.2.ffn.key-broadcast_matmul-538/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-133-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-546/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-547(model.blocks.2.ffn.key-broadcast_matmul-538/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_shift-pad-130/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-547/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-548(model.blocks.2.ffn-broadcast_mul-547/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-133-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-548/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-549([model.blocks.2.ffn-broadcast_mul-546/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-535/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-549/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-557(model.blocks.2.ffn-reduce_sum_like-548/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-scalar_mul-557/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-558([model.blocks.2.ffn-scalar_mul-557/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-reduce_sum_like-544/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-558/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2_grad-525-dx_0-cast_f2h(model.blocks.2.ffn-sigmoid_v2_grad-525/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2_grad-525-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2-149-y_0-cast_f2h(model.blocks.2.ffn-sigmoid_v2-149/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2-149-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square-144-y_0-cast_f2h(model.blocks.2.ffn-square-144/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square-144-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square_grad-530-dx_0-cast_f2h(model.blocks.2.ffn-square_grad-530/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square_grad-530-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-133-out_0-cast_f2h(model.blocks.2.ffn-scalar_add-133/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-133-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k-out-cast_f2h(model.blocks.2.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r-out-cast_f2h(model.blocks.2.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-138-out_0-cast_f2h(model.blocks.2.ffn-scalar_add-138/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-138-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-relu-143-y_0-cast_h2f(model.blocks.2.ffn-relu-143/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-relu-143-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-558_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn-add_n-558/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-add_n-558_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-541_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn-add_n-541/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-add_n-541_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k_optimizer(model.blocks.2.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn-add_n-558_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r_optimizer(model.blocks.2.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn-add_n-541_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.2-add_n-151([model.blocks.2.att.output-broadcast_matmul-127/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-150/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2-add_n-151/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.2-add_n-151-out_0-cast_h2f(model.blocks.2-add_n-151/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2-add_n-151-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.2-add_n-128-out_0-cast_h2f(model.blocks.2.att.output-broadcast_matmul-127/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2-add_n-128-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.3:Block()): (
        (INPUT:_model.blocks.3_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.3.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.3.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.3.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.3.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-layer_norm-152(model.blocks.2-add_n-151/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln1-layer_norm-152/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-152/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-152/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-layer_norm_param_grad-518(model.blocks.3.att.time_shift-add_n-515-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2-add_n-151-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-152/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-152/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln1-layer_norm_param_grad-518/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm_param_grad-518/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-layer_norm_grad-519(model.blocks.3.att.time_shift-add_n-515/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2-add_n-151/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-152/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-152/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln1-layer_norm_grad-519/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias-out-cast_f2h(model.blocks.3.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight-out-cast_f2h(model.blocks.3.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-add_n-520-out_0-cast_h2f(model.blocks.3.ln1-layer_norm_grad-519/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln1-add_n-520-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight_optimizer(model.blocks.3.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm_param_grad-518/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias_optimizer(model.blocks.3.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm_param_grad-518/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.3.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.3.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.3.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.3.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2-layer_norm-179(model.blocks.3.att.output-broadcast_matmul-177/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln2-layer_norm-179/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-179/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-179/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2-layer_norm_param_grad-498(model.blocks.3.ffn.time_shift-add_n-495-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3-add_n-178-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-179/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-179/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln2-layer_norm_param_grad-498/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm_param_grad-498/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2-layer_norm_grad-499(model.blocks.3.ffn.time_shift-add_n-495/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.output-broadcast_matmul-177/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-179/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-179/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln2-layer_norm_grad-499/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight-out-cast_f2h(model.blocks.3.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias-out-cast_f2h(model.blocks.3.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight_optimizer(model.blocks.3.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm_param_grad-498/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias_optimizer(model.blocks.3.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm_param_grad-498/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.3.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.3.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.3.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.3.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.3.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.3.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.3.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.3.att.time_shift-pad-153(model.blocks.3.ln1-layer_norm-152/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.time_shift-pad-153/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.time_shift-pad-514(model.blocks.3.att-broadcast_mul-511/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.time_shift-pad-514/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.time_shift-add_n-515([model.blocks.3.att.time_shift-pad-514/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-508/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att.time_shift-add_n-515/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.time_shift-add_n-515-out_0-cast_h2f(model.blocks.3.att.time_shift-add_n-515/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.time_shift-add_n-515-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.3.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key-broadcast_matmul-170(model.blocks.3.att-add_n-158/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.key-broadcast_matmul-170/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key.weight-out-cast_f2h(model.blocks.3.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value-broadcast_matmul-172(model.blocks.3.att-add_n-163/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.value-broadcast_matmul-172/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value.weight-out-cast_f2h(model.blocks.3.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul-174(model.blocks.3.att-add_n-168/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul-174/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul-506(model.blocks.3.att-sigmoid_v2_grad-505-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul-506/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul_grad_b-507(model.blocks.3.att-sigmoid_v2_grad-505-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-add_n-168/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul_grad_b-507/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight-out-cast_f2h(model.blocks.3.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul-174-out_0-cast_h2f(model.blocks.3.att.receptance-broadcast_matmul-174/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul-174-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul_grad_b-507_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att.receptance-broadcast_matmul_grad_b-507/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul_grad_b-507_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight_optimizer(model.blocks.3.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.receptance-broadcast_matmul_grad_b-507_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.3.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
            (PARAMETER:model.blocks.3.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul-177(model.blocks.3.att-sigmoid_v2-175-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul-177/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul-503(model.blocks.3.ln2-layer_norm_grad-499/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul-503/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul_grad_b-504(model.blocks.3.ln2-layer_norm_grad-499/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-sigmoid_v2-175-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul_grad_b-504/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight-out-cast_f2h(model.blocks.3.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul-503-out_0-cast_h2f(model.blocks.3.att.output-broadcast_matmul-503/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul-503-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul_grad_b-504_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att.output-broadcast_matmul_grad_b-504/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul_grad_b-504_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight_optimizer(model.blocks.3.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.output-broadcast_matmul_grad_b-504_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.3.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-154(model.blocks.3.ln1-layer_norm-152/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-154/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-155(model.blocks.3.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_mul-155/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-156(model.blocks.3.att-scalar_mul-155/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-156/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-157(model.blocks.3.att.time_shift-pad-153/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-156-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-157/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-158([model.blocks.3.att-broadcast_mul-154/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-157/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-158/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-159(model.blocks.3.ln1-layer_norm-152/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-159/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-160(model.blocks.3.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_mul-160/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-161(model.blocks.3.att-scalar_mul-160/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-161/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-162(model.blocks.3.att.time_shift-pad-153/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-161-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-162/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-163([model.blocks.3.att-broadcast_mul-159/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-162/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-163/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-164(model.blocks.3.ln1-layer_norm-152/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-164/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-165(model.blocks.3.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_mul-165/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-166(model.blocks.3.att-scalar_mul-165/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-166/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-167(model.blocks.3.att.time_shift-pad-153/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-166-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-167/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-168([model.blocks.3.att-broadcast_mul-164/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-167/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-168/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-sigmoid_v2-175(model.blocks.3.att.receptance-broadcast_matmul-174-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-sigmoid_v2-175/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-sigmoid_v2_grad-505(model.blocks.3.att.receptance-broadcast_matmul-174-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.output-broadcast_matmul-503-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-sigmoid_v2_grad-505/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-508(model.blocks.3.att.receptance-broadcast_matmul-506/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-508/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-509(model.blocks.3.att.receptance-broadcast_matmul-506/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-152/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-509/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-510(model.blocks.3.att-broadcast_mul-509/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-510/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-511(model.blocks.3.att.receptance-broadcast_matmul-506/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-166-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-511/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-512(model.blocks.3.att.receptance-broadcast_matmul-506/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_shift-pad-153/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-512/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-513(model.blocks.3.att-broadcast_mul-512/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-166-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-513/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-521(model.blocks.3.att-reduce_sum_like-513/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-scalar_mul-521/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-522([model.blocks.3.att-scalar_mul-521/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-reduce_sum_like-510/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-522/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-sigmoid_v2-175-y_0-cast_f2h(model.blocks.3.att-sigmoid_v2-175/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-sigmoid_v2-175-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_v-out-cast_f2h(model.blocks.3.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-161-out_0-cast_f2h(model.blocks.3.att-scalar_add-161/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-161-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-sigmoid_v2_grad-505-dx_0-cast_f2h(model.blocks.3.att-sigmoid_v2_grad-505/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-sigmoid_v2_grad-505-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-166-out_0-cast_f2h(model.blocks.3.att-scalar_add-166/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-166-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r-out-cast_f2h(model.blocks.3.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-156-out_0-cast_f2h(model.blocks.3.att-scalar_add-156/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-156-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_k-out-cast_f2h(model.blocks.3.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-522_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att-add_n-522/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-add_n-522_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r_optimizer(model.blocks.3.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att-add_n-522_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.3.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.3.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.3.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.3.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.3.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.3.ffn.time_shift-pad-180(model.blocks.3.ln2-layer_norm-179/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.time_shift-pad-180/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.time_shift-pad-494(model.blocks.3.ffn-add_n-493/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.time_shift-pad-494/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.time_shift-add_n-495([model.blocks.3.ffn.time_shift-pad-494/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-add_n-489/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn.time_shift-add_n-495/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.time_shift-add_n-495-out_0-cast_h2f(model.blocks.3.ffn.time_shift-add_n-495/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.time_shift-add_n-495-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.3.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul-192(model.blocks.3.ffn-add_n-185/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul-192/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul-482(model.blocks.3.ffn-relu_grad-475/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul-482/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul_grad_b-483(model.blocks.3.ffn-relu_grad-475/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-add_n-185/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul_grad_b-483/out_0:(sbp=(P), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight-out-cast_f2h(model.blocks.3.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul_grad_b-483_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn.key-broadcast_matmul_grad_b-483/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul_grad_b-483_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight_optimizer(model.blocks.3.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.key-broadcast_matmul_grad_b-483_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul-198(model.blocks.3.ffn-add_n-190/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul-198/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul-472(model.blocks.3.ffn-sigmoid_v2_grad-469-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473(model.blocks.3.ffn-sigmoid_v2_grad-469-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-add_n-190/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight-out-cast_f2h(model.blocks.3.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul-198-out_0-cast_h2f(model.blocks.3.ffn.receptance-broadcast_matmul-198/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul-198-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight_optimizer(model.blocks.3.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-473_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.3.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.3.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-196(model.blocks.3.ffn-square-194-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-196/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-470(model.blocks.3.ffn-broadcast_mul-468/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-470/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul_grad_b-471(model.blocks.3.ffn-broadcast_mul-468/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-square-194-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul_grad_b-471/out_0:(sbp=(P), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight-out-cast_f2h(model.blocks.3.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-196-out_0-cast_h2f(model.blocks.3.ffn.value-broadcast_matmul-196/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-196-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-470-out_0-cast_h2f(model.blocks.3.ffn.value-broadcast_matmul-470/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-470-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul_grad_b-471_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn.value-broadcast_matmul_grad_b-471/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul_grad_b-471_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight_optimizer(model.blocks.3.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.3.ffn.value-broadcast_matmul_grad_b-471_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.3.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.3.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-181(model.blocks.3.ln2-layer_norm-179/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-181/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-182(model.blocks.3.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_mul-182/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-183(model.blocks.3.ffn-scalar_mul-182/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-183/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-184(model.blocks.3.ffn.time_shift-pad-180/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-183-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-184/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-185([model.blocks.3.ffn-broadcast_mul-181/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-184/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-185/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-186(model.blocks.3.ln2-layer_norm-179/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-186/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-187(model.blocks.3.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_mul-187/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-188(model.blocks.3.ffn-scalar_mul-187/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-188/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-189(model.blocks.3.ffn.time_shift-pad-180/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-188-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-189/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-190([model.blocks.3.ffn-broadcast_mul-186/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-189/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-190/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-relu-193(model.blocks.3.ffn.key-broadcast_matmul-192/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-relu-193/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square-194(model.blocks.3.ffn-relu-193-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square-194/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2-199(model.blocks.3.ffn.receptance-broadcast_matmul-198-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2-199/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-200(model.blocks.3.ffn-sigmoid_v2-199-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.value-broadcast_matmul-196/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-200/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-467(model.blocks.4.ln1-add_n-464-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.value-broadcast_matmul-196-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-broadcast_mul-467/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-468(model.blocks.4.ln1-layer_norm_grad-463/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-sigmoid_v2-199-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-468/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2_grad-469(model.blocks.3.ffn.receptance-broadcast_matmul-198-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn-broadcast_mul-467/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2_grad-469/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square_grad-474(model.blocks.3.ffn-relu-193-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.3.ffn.value-broadcast_matmul-470-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square_grad-474/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-relu_grad-475(model.blocks.3.ffn-square_grad-474-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-relu-193/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-relu_grad-475/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-476(model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-476/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-477(model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-179/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-477/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-478(model.blocks.3.ffn-broadcast_mul-477/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-478/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-479(model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-188-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-479/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-480(model.blocks.3.ffn.receptance-broadcast_matmul-472/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_shift-pad-180/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-480/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-481(model.blocks.3.ffn-broadcast_mul-480/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-188-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-481/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-484(model.blocks.3.ffn-reduce_sum_like-481/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-scalar_mul-484/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-485([model.blocks.3.ffn-scalar_mul-484/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-reduce_sum_like-478/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-485/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-486(model.blocks.3.ffn.key-broadcast_matmul-482/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-486/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-487(model.blocks.3.ffn.key-broadcast_matmul-482/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-179/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-487/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-488(model.blocks.3.ffn-broadcast_mul-487/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-488/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-489([model.blocks.3.ffn-broadcast_mul-486/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-476/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-489/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-490(model.blocks.3.ffn.key-broadcast_matmul-482/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-183-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-490/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-491(model.blocks.3.ffn.key-broadcast_matmul-482/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_shift-pad-180/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-491/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-492(model.blocks.3.ffn-broadcast_mul-491/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-183-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-492/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-493([model.blocks.3.ffn-broadcast_mul-490/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-479/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-493/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-501(model.blocks.3.ffn-reduce_sum_like-492/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-scalar_mul-501/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-502([model.blocks.3.ffn-scalar_mul-501/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-reduce_sum_like-488/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-502/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square-194-y_0-cast_f2h(model.blocks.3.ffn-square-194/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square-194-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-188-out_0-cast_f2h(model.blocks.3.ffn-scalar_add-188/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-188-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2-199-y_0-cast_f2h(model.blocks.3.ffn-sigmoid_v2-199/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2-199-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-183-out_0-cast_f2h(model.blocks.3.ffn-scalar_add-183/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-183-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r-out-cast_f2h(model.blocks.3.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square_grad-474-dx_0-cast_f2h(model.blocks.3.ffn-square_grad-474/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square_grad-474-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k-out-cast_f2h(model.blocks.3.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2_grad-469-dx_0-cast_f2h(model.blocks.3.ffn-sigmoid_v2_grad-469/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2_grad-469-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-relu-193-y_0-cast_h2f(model.blocks.3.ffn-relu-193/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-relu-193-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-485_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn-add_n-485/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-add_n-485_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-502_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn-add_n-502/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-add_n-502_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k_optimizer(model.blocks.3.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn-add_n-502_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r_optimizer(model.blocks.3.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn-add_n-485_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.3-add_n-201([model.blocks.3.att.output-broadcast_matmul-177/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-200/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3-add_n-201/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.3-add_n-178-out_0-cast_h2f(model.blocks.3.att.output-broadcast_matmul-177/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3-add_n-178-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.3-add_n-201-out_0-cast_h2f(model.blocks.3-add_n-201/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3-add_n-201-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.3_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.4:Block()): (
        (INPUT:_model.blocks.4_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.4.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.4.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.4.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.4.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-layer_norm-202(model.blocks.3-add_n-201/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln1-layer_norm-202/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-202/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-202/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-layer_norm_param_grad-462(model.blocks.4.att.time_shift-add_n-459-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3-add_n-201-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-202/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-202/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln1-layer_norm_param_grad-462/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm_param_grad-462/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-layer_norm_grad-463(model.blocks.4.att.time_shift-add_n-459/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3-add_n-201/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-202/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-202/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln1-layer_norm_grad-463/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias-out-cast_f2h(model.blocks.4.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight-out-cast_f2h(model.blocks.4.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-add_n-464-out_0-cast_h2f(model.blocks.4.ln1-layer_norm_grad-463/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln1-add_n-464-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight_optimizer(model.blocks.4.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm_param_grad-462/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias_optimizer(model.blocks.4.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm_param_grad-462/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.4.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.4.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.4.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.4.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2-layer_norm-229(model.blocks.4.att.output-broadcast_matmul-227/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln2-layer_norm-229/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-229/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-229/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2-layer_norm_param_grad-442(model.blocks.4.ffn.time_shift-add_n-439-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4-add_n-228-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-229/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-229/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln2-layer_norm_param_grad-442/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm_param_grad-442/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2-layer_norm_grad-443(model.blocks.4.ffn.time_shift-add_n-439/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.output-broadcast_matmul-227/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-229/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-229/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln2-layer_norm_grad-443/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias-out-cast_f2h(model.blocks.4.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight-out-cast_f2h(model.blocks.4.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight_optimizer(model.blocks.4.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm_param_grad-442/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias_optimizer(model.blocks.4.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm_param_grad-442/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.4.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.4.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.4.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.4.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.4.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.4.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.4.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.4.att.time_shift-pad-203(model.blocks.4.ln1-layer_norm-202/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.time_shift-pad-203/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.time_shift-pad-458(model.blocks.4.att-broadcast_mul-455/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.time_shift-pad-458/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.time_shift-add_n-459([model.blocks.4.att.time_shift-pad-458/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-452/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att.time_shift-add_n-459/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.time_shift-add_n-459-out_0-cast_h2f(model.blocks.4.att.time_shift-add_n-459/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.time_shift-add_n-459-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.4.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key-broadcast_matmul-220(model.blocks.4.att-add_n-208/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.key-broadcast_matmul-220/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key.weight-out-cast_f2h(model.blocks.4.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value-broadcast_matmul-222(model.blocks.4.att-add_n-213/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.value-broadcast_matmul-222/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value.weight-out-cast_f2h(model.blocks.4.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul-224(model.blocks.4.att-add_n-218/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul-224/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul-450(model.blocks.4.att-sigmoid_v2_grad-449-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul-450/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul_grad_b-451(model.blocks.4.att-sigmoid_v2_grad-449-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-add_n-218/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul_grad_b-451/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight-out-cast_f2h(model.blocks.4.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul-224-out_0-cast_h2f(model.blocks.4.att.receptance-broadcast_matmul-224/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul-224-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul_grad_b-451_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att.receptance-broadcast_matmul_grad_b-451/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul_grad_b-451_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight_optimizer(model.blocks.4.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.receptance-broadcast_matmul_grad_b-451_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.4.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
            (PARAMETER:model.blocks.4.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul-227(model.blocks.4.att-sigmoid_v2-225-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul-227/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul-447(model.blocks.4.ln2-layer_norm_grad-443/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul-447/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul_grad_b-448(model.blocks.4.ln2-layer_norm_grad-443/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-sigmoid_v2-225-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul_grad_b-448/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight-out-cast_f2h(model.blocks.4.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul-447-out_0-cast_h2f(model.blocks.4.att.output-broadcast_matmul-447/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul-447-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul_grad_b-448_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att.output-broadcast_matmul_grad_b-448/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul_grad_b-448_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight_optimizer(model.blocks.4.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.output-broadcast_matmul_grad_b-448_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.4.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-204(model.blocks.4.ln1-layer_norm-202/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-204/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-205(model.blocks.4.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_mul-205/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-206(model.blocks.4.att-scalar_mul-205/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-206/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-207(model.blocks.4.att.time_shift-pad-203/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-206-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-207/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-208([model.blocks.4.att-broadcast_mul-204/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-207/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-208/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-209(model.blocks.4.ln1-layer_norm-202/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-209/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-210(model.blocks.4.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_mul-210/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-211(model.blocks.4.att-scalar_mul-210/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-211/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-212(model.blocks.4.att.time_shift-pad-203/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-211-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-212/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-213([model.blocks.4.att-broadcast_mul-209/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-212/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-213/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-214(model.blocks.4.ln1-layer_norm-202/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-214/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-215(model.blocks.4.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_mul-215/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-216(model.blocks.4.att-scalar_mul-215/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-216/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-217(model.blocks.4.att.time_shift-pad-203/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-216-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-217/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-218([model.blocks.4.att-broadcast_mul-214/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-217/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-218/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-sigmoid_v2-225(model.blocks.4.att.receptance-broadcast_matmul-224-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-sigmoid_v2-225/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-sigmoid_v2_grad-449(model.blocks.4.att.receptance-broadcast_matmul-224-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.output-broadcast_matmul-447-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-sigmoid_v2_grad-449/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-452(model.blocks.4.att.receptance-broadcast_matmul-450/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-452/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-453(model.blocks.4.att.receptance-broadcast_matmul-450/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-202/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-453/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-454(model.blocks.4.att-broadcast_mul-453/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-454/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-455(model.blocks.4.att.receptance-broadcast_matmul-450/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-216-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-455/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-456(model.blocks.4.att.receptance-broadcast_matmul-450/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_shift-pad-203/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-456/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-457(model.blocks.4.att-broadcast_mul-456/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-216-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-457/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-465(model.blocks.4.att-reduce_sum_like-457/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-scalar_mul-465/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-466([model.blocks.4.att-scalar_mul-465/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-reduce_sum_like-454/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-466/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-sigmoid_v2_grad-449-dx_0-cast_f2h(model.blocks.4.att-sigmoid_v2_grad-449/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-sigmoid_v2_grad-449-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r-out-cast_f2h(model.blocks.4.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_k-out-cast_f2h(model.blocks.4.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_v-out-cast_f2h(model.blocks.4.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-211-out_0-cast_f2h(model.blocks.4.att-scalar_add-211/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-211-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-sigmoid_v2-225-y_0-cast_f2h(model.blocks.4.att-sigmoid_v2-225/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-sigmoid_v2-225-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-216-out_0-cast_f2h(model.blocks.4.att-scalar_add-216/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-216-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-206-out_0-cast_f2h(model.blocks.4.att-scalar_add-206/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-206-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-466_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att-add_n-466/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-add_n-466_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r_optimizer(model.blocks.4.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att-add_n-466_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.4.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.4.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.4.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.4.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.4.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.4.ffn.time_shift-pad-230(model.blocks.4.ln2-layer_norm-229/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.time_shift-pad-230/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.time_shift-pad-438(model.blocks.4.ffn-add_n-437/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.time_shift-pad-438/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.time_shift-add_n-439([model.blocks.4.ffn.time_shift-pad-438/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-add_n-433/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn.time_shift-add_n-439/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.time_shift-add_n-439-out_0-cast_h2f(model.blocks.4.ffn.time_shift-add_n-439/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.time_shift-add_n-439-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.4.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul-242(model.blocks.4.ffn-add_n-235/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul-242/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul-426(model.blocks.4.ffn-relu_grad-419/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul-426/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul_grad_b-427(model.blocks.4.ffn-relu_grad-419/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-add_n-235/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul_grad_b-427/out_0:(sbp=(P), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight-out-cast_f2h(model.blocks.4.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul_grad_b-427_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn.key-broadcast_matmul_grad_b-427/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul_grad_b-427_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight_optimizer(model.blocks.4.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.key-broadcast_matmul_grad_b-427_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul-248(model.blocks.4.ffn-add_n-240/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul-248/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul-416(model.blocks.4.ffn-sigmoid_v2_grad-413-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417(model.blocks.4.ffn-sigmoid_v2_grad-413-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-add_n-240/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight-out-cast_f2h(model.blocks.4.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul-248-out_0-cast_h2f(model.blocks.4.ffn.receptance-broadcast_matmul-248/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul-248-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight_optimizer(model.blocks.4.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-417_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.4.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.4.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-246(model.blocks.4.ffn-square-244-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-246/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-414(model.blocks.4.ffn-broadcast_mul-412/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-414/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul_grad_b-415(model.blocks.4.ffn-broadcast_mul-412/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-square-244-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul_grad_b-415/out_0:(sbp=(P), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight-out-cast_f2h(model.blocks.4.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-246-out_0-cast_h2f(model.blocks.4.ffn.value-broadcast_matmul-246/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-246-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-414-out_0-cast_h2f(model.blocks.4.ffn.value-broadcast_matmul-414/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-414-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul_grad_b-415_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn.value-broadcast_matmul_grad_b-415/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul_grad_b-415_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight_optimizer(model.blocks.4.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.4.ffn.value-broadcast_matmul_grad_b-415_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.4.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.4.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-231(model.blocks.4.ln2-layer_norm-229/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-231/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-232(model.blocks.4.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_mul-232/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-233(model.blocks.4.ffn-scalar_mul-232/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-233/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-234(model.blocks.4.ffn.time_shift-pad-230/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-233-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-234/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-235([model.blocks.4.ffn-broadcast_mul-231/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-234/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-235/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-236(model.blocks.4.ln2-layer_norm-229/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-236/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-237(model.blocks.4.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_mul-237/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-238(model.blocks.4.ffn-scalar_mul-237/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-238/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-239(model.blocks.4.ffn.time_shift-pad-230/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-238-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-239/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-240([model.blocks.4.ffn-broadcast_mul-236/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-239/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-240/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-relu-243(model.blocks.4.ffn.key-broadcast_matmul-242/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-relu-243/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square-244(model.blocks.4.ffn-relu-243-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square-244/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2-249(model.blocks.4.ffn.receptance-broadcast_matmul-248-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2-249/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-250(model.blocks.4.ffn-sigmoid_v2-249-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.value-broadcast_matmul-246/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-250/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-411(model.blocks.5.ln1-add_n-408-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.value-broadcast_matmul-246-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-broadcast_mul-411/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-412(model.blocks.5.ln1-layer_norm_grad-407/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-sigmoid_v2-249-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-412/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2_grad-413(model.blocks.4.ffn.receptance-broadcast_matmul-248-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn-broadcast_mul-411/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2_grad-413/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square_grad-418(model.blocks.4.ffn-relu-243-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.4.ffn.value-broadcast_matmul-414-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square_grad-418/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-relu_grad-419(model.blocks.4.ffn-square_grad-418-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-relu-243/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-relu_grad-419/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-420(model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-420/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-421(model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-229/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-421/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-422(model.blocks.4.ffn-broadcast_mul-421/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-422/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-423(model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-238-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-423/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-424(model.blocks.4.ffn.receptance-broadcast_matmul-416/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_shift-pad-230/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-424/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-425(model.blocks.4.ffn-broadcast_mul-424/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-238-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-425/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-428(model.blocks.4.ffn-reduce_sum_like-425/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-scalar_mul-428/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-429([model.blocks.4.ffn-scalar_mul-428/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-reduce_sum_like-422/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-429/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-430(model.blocks.4.ffn.key-broadcast_matmul-426/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-430/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-431(model.blocks.4.ffn.key-broadcast_matmul-426/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-229/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-431/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-432(model.blocks.4.ffn-broadcast_mul-431/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-432/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-433([model.blocks.4.ffn-broadcast_mul-430/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-420/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-433/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-434(model.blocks.4.ffn.key-broadcast_matmul-426/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-233-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-434/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-435(model.blocks.4.ffn.key-broadcast_matmul-426/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_shift-pad-230/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-435/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-436(model.blocks.4.ffn-broadcast_mul-435/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-233-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-436/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-437([model.blocks.4.ffn-broadcast_mul-434/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-423/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-437/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-445(model.blocks.4.ffn-reduce_sum_like-436/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-scalar_mul-445/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-446([model.blocks.4.ffn-scalar_mul-445/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-reduce_sum_like-432/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-446/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2_grad-413-dx_0-cast_f2h(model.blocks.4.ffn-sigmoid_v2_grad-413/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2_grad-413-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-233-out_0-cast_f2h(model.blocks.4.ffn-scalar_add-233/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-233-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2-249-y_0-cast_f2h(model.blocks.4.ffn-sigmoid_v2-249/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2-249-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k-out-cast_f2h(model.blocks.4.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square-244-y_0-cast_f2h(model.blocks.4.ffn-square-244/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square-244-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-238-out_0-cast_f2h(model.blocks.4.ffn-scalar_add-238/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-238-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r-out-cast_f2h(model.blocks.4.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square_grad-418-dx_0-cast_f2h(model.blocks.4.ffn-square_grad-418/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square_grad-418-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-relu-243-y_0-cast_h2f(model.blocks.4.ffn-relu-243/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-relu-243-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-429_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn-add_n-429/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-add_n-429_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-446_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn-add_n-446/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-add_n-446_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k_optimizer(model.blocks.4.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn-add_n-446_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r_optimizer(model.blocks.4.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn-add_n-429_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.4-add_n-251([model.blocks.4.att.output-broadcast_matmul-227/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-250/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4-add_n-251/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.4-add_n-251-out_0-cast_h2f(model.blocks.4-add_n-251/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4-add_n-251-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.4-add_n-228-out_0-cast_h2f(model.blocks.4.att.output-broadcast_matmul-227/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4-add_n-228-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.4_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.5:Block()): (
        (INPUT:_model.blocks.5_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.5.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.5.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.5.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.5.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-layer_norm-252(model.blocks.4-add_n-251/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln1-layer_norm-252/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-252/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-252/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-layer_norm_param_grad-406(model.blocks.5.att.time_shift-add_n-403-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4-add_n-251-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-252/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-252/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln1-layer_norm_param_grad-406/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm_param_grad-406/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-layer_norm_grad-407(model.blocks.5.att.time_shift-add_n-403/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4-add_n-251/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-252/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-252/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln1-layer_norm_grad-407/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight-out-cast_f2h(model.blocks.5.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias-out-cast_f2h(model.blocks.5.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-add_n-408-out_0-cast_h2f(model.blocks.5.ln1-layer_norm_grad-407/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln1-add_n-408-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight_optimizer(model.blocks.5.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm_param_grad-406/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias_optimizer(model.blocks.5.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm_param_grad-406/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.5.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.5.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.5.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.5.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2-layer_norm-279(model.blocks.5.att.output-broadcast_matmul-277/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln2-layer_norm-279/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-279/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-279/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2-layer_norm_param_grad-386(model.blocks.5.ffn.time_shift-add_n-383-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5-add_n-278-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-279/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-279/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln2-layer_norm_param_grad-386/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm_param_grad-386/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2-layer_norm_grad-387(model.blocks.5.ffn.time_shift-add_n-383/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.output-broadcast_matmul-277/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-279/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-279/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln2-layer_norm_grad-387/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias-out-cast_f2h(model.blocks.5.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight-out-cast_f2h(model.blocks.5.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight_optimizer(model.blocks.5.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm_param_grad-386/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias_optimizer(model.blocks.5.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm_param_grad-386/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.5.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.5.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.5.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.5.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 requires_grad=True)): ()
          (PARAMETER:model.blocks.5.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.5.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.5.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.5.att.time_shift-pad-253(model.blocks.5.ln1-layer_norm-252/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.time_shift-pad-253/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.time_shift-pad-402(model.blocks.5.att-broadcast_mul-399/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.time_shift-pad-402/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.time_shift-add_n-403([model.blocks.5.att.time_shift-pad-402/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-396/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att.time_shift-add_n-403/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.time_shift-add_n-403-out_0-cast_h2f(model.blocks.5.att.time_shift-add_n-403/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.time_shift-add_n-403-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.5.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key-broadcast_matmul-270(model.blocks.5.att-add_n-258/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.key-broadcast_matmul-270/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key.weight-out-cast_f2h(model.blocks.5.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value-broadcast_matmul-272(model.blocks.5.att-add_n-263/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.value-broadcast_matmul-272/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value.weight-out-cast_f2h(model.blocks.5.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul-274(model.blocks.5.att-add_n-268/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul-274/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul-394(model.blocks.5.att-sigmoid_v2_grad-393-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul-394/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul_grad_b-395(model.blocks.5.att-sigmoid_v2_grad-393-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-add_n-268/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul_grad_b-395/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight-out-cast_f2h(model.blocks.5.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul-274-out_0-cast_h2f(model.blocks.5.att.receptance-broadcast_matmul-274/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul-274-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul_grad_b-395_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att.receptance-broadcast_matmul_grad_b-395/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul_grad_b-395_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight_optimizer(model.blocks.5.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.receptance-broadcast_matmul_grad_b-395_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.5.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<sigmoid_v2_backward>))
            (PARAMETER:model.blocks.5.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul-277(model.blocks.5.att-sigmoid_v2-275-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul-277/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul-391(model.blocks.5.ln2-layer_norm_grad-387/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul-391/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul_grad_b-392(model.blocks.5.ln2-layer_norm_grad-387/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-sigmoid_v2-275-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul_grad_b-392/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight-out-cast_f2h(model.blocks.5.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul-391-out_0-cast_h2f(model.blocks.5.att.output-broadcast_matmul-391/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul-391-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul_grad_b-392_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att.output-broadcast_matmul_grad_b-392/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul_grad_b-392_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight_optimizer(model.blocks.5.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.output-broadcast_matmul_grad_b-392_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.5.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-254(model.blocks.5.ln1-layer_norm-252/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-254/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-255(model.blocks.5.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_mul-255/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-256(model.blocks.5.att-scalar_mul-255/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-256/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-257(model.blocks.5.att.time_shift-pad-253/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-256-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-257/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-258([model.blocks.5.att-broadcast_mul-254/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-257/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-258/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-259(model.blocks.5.ln1-layer_norm-252/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-259/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-260(model.blocks.5.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_mul-260/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-261(model.blocks.5.att-scalar_mul-260/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-261/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-262(model.blocks.5.att.time_shift-pad-253/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-261-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-262/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-263([model.blocks.5.att-broadcast_mul-259/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-262/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-263/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-264(model.blocks.5.ln1-layer_norm-252/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-264/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-265(model.blocks.5.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_mul-265/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-266(model.blocks.5.att-scalar_mul-265/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-266/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-267(model.blocks.5.att.time_shift-pad-253/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-266-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-267/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-268([model.blocks.5.att-broadcast_mul-264/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-267/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-268/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-sigmoid_v2-275(model.blocks.5.att.receptance-broadcast_matmul-274-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-sigmoid_v2-275/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-sigmoid_v2_grad-393(model.blocks.5.att.receptance-broadcast_matmul-274-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.output-broadcast_matmul-391-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-sigmoid_v2_grad-393/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-396(model.blocks.5.att.receptance-broadcast_matmul-394/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-396/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-397(model.blocks.5.att.receptance-broadcast_matmul-394/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-252/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-397/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-398(model.blocks.5.att-broadcast_mul-397/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-398/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-399(model.blocks.5.att.receptance-broadcast_matmul-394/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-266-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-399/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-400(model.blocks.5.att.receptance-broadcast_matmul-394/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_shift-pad-253/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-400/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-401(model.blocks.5.att-broadcast_mul-400/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-266-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-401/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-409(model.blocks.5.att-reduce_sum_like-401/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-scalar_mul-409/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-410([model.blocks.5.att-scalar_mul-409/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-reduce_sum_like-398/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-410/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r-out-cast_f2h(model.blocks.5.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-sigmoid_v2-275-y_0-cast_f2h(model.blocks.5.att-sigmoid_v2-275/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-sigmoid_v2-275-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_k-out-cast_f2h(model.blocks.5.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-261-out_0-cast_f2h(model.blocks.5.att-scalar_add-261/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-261-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-256-out_0-cast_f2h(model.blocks.5.att-scalar_add-256/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-256-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-266-out_0-cast_f2h(model.blocks.5.att-scalar_add-266/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-266-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-sigmoid_v2_grad-393-dx_0-cast_f2h(model.blocks.5.att-sigmoid_v2_grad-393/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-sigmoid_v2_grad-393-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_v-out-cast_f2h(model.blocks.5.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-410_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att-add_n-410/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-add_n-410_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r_optimizer(model.blocks.5.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att-add_n-410_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.5.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.5.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.5.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.5.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.5.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.5.ffn.time_shift-pad-280(model.blocks.5.ln2-layer_norm-279/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.time_shift-pad-280/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.time_shift-pad-382(model.blocks.5.ffn-add_n-381/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.time_shift-pad-382/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.time_shift-add_n-383([model.blocks.5.ffn.time_shift-pad-382/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-add_n-377/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn.time_shift-add_n-383/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.time_shift-add_n-383-out_0-cast_h2f(model.blocks.5.ffn.time_shift-add_n-383/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.time_shift-add_n-383-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.5.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul-292(model.blocks.5.ffn-add_n-285/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul-292/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul-370(model.blocks.5.ffn-relu_grad-363/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul-370/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul_grad_b-371(model.blocks.5.ffn-relu_grad-363/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-add_n-285/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul_grad_b-371/out_0:(sbp=(P), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight-out-cast_f2h(model.blocks.5.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul_grad_b-371_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn.key-broadcast_matmul_grad_b-371/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul_grad_b-371_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight_optimizer(model.blocks.5.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.key-broadcast_matmul_grad_b-371_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul-298(model.blocks.5.ffn-add_n-290/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul-298/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul-360(model.blocks.5.ffn-sigmoid_v2_grad-357-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361(model.blocks.5.ffn-sigmoid_v2_grad-357-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-add_n-290/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361/out_0:(sbp=(P), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight-out-cast_f2h(model.blocks.5.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul-298-out_0-cast_h2f(model.blocks.5.ffn.receptance-broadcast_matmul-298/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul-298-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight_optimizer(model.blocks.5.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-361_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.5.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.5.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-296(model.blocks.5.ffn-square-294-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-296/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-358(model.blocks.5.ffn-broadcast_mul-356/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-358/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul_grad_b-359(model.blocks.5.ffn-broadcast_mul-356/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-square-294-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul_grad_b-359/out_0:(sbp=(P), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight-out-cast_f2h(model.blocks.5.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-296-out_0-cast_h2f(model.blocks.5.ffn.value-broadcast_matmul-296/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-296-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-358-out_0-cast_h2f(model.blocks.5.ffn.value-broadcast_matmul-358/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-358-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul_grad_b-359_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn.value-broadcast_matmul_grad_b-359/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul_grad_b-359_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight_optimizer(model.blocks.5.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.5.ffn.value-broadcast_matmul_grad_b-359_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.5.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.5.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-281(model.blocks.5.ln2-layer_norm-279/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-281/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-282(model.blocks.5.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_mul-282/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-283(model.blocks.5.ffn-scalar_mul-282/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-283/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-284(model.blocks.5.ffn.time_shift-pad-280/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-283-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-284/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-285([model.blocks.5.ffn-broadcast_mul-281/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-284/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-285/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-286(model.blocks.5.ln2-layer_norm-279/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-286/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-287(model.blocks.5.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_mul-287/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-288(model.blocks.5.ffn-scalar_mul-287/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-288/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-289(model.blocks.5.ffn.time_shift-pad-280/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-288-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-289/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-290([model.blocks.5.ffn-broadcast_mul-286/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-289/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-290/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-relu-293(model.blocks.5.ffn.key-broadcast_matmul-292/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-relu-293/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square-294(model.blocks.5.ffn-relu-293-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square-294/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2-299(model.blocks.5.ffn.receptance-broadcast_matmul-298-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2-299/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-300(model.blocks.5.ffn-sigmoid_v2-299-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.value-broadcast_matmul-296/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-300/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-355(model.ln_out-layer_norm_grad-354-dx_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.value-broadcast_matmul-296-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-broadcast_mul-355/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-356(model.ln_out-layer_norm_grad-354/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-sigmoid_v2-299-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-356/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2_grad-357(model.blocks.5.ffn.receptance-broadcast_matmul-298-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn-broadcast_mul-355/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2_grad-357/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square_grad-362(model.blocks.5.ffn-relu-293-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.5.ffn.value-broadcast_matmul-358-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square_grad-362/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-relu_grad-363(model.blocks.5.ffn-square_grad-362-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-relu-293/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-relu_grad-363/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-364(model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-364/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-365(model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-279/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-365/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-366(model.blocks.5.ffn-broadcast_mul-365/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-366/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-367(model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-288-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-367/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-368(model.blocks.5.ffn.receptance-broadcast_matmul-360/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_shift-pad-280/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-368/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-369(model.blocks.5.ffn-broadcast_mul-368/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-288-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-369/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-372(model.blocks.5.ffn-reduce_sum_like-369/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-scalar_mul-372/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-373([model.blocks.5.ffn-scalar_mul-372/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-reduce_sum_like-366/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-373/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-374(model.blocks.5.ffn.key-broadcast_matmul-370/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-374/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-375(model.blocks.5.ffn.key-broadcast_matmul-370/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-279/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-375/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-376(model.blocks.5.ffn-broadcast_mul-375/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-376/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-377([model.blocks.5.ffn-broadcast_mul-374/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-364/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-377/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-378(model.blocks.5.ffn.key-broadcast_matmul-370/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-283-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-378/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-379(model.blocks.5.ffn.key-broadcast_matmul-370/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_shift-pad-280/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-379/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-380(model.blocks.5.ffn-broadcast_mul-379/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-283-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-380/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-381([model.blocks.5.ffn-broadcast_mul-378/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-367/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-381/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-389(model.blocks.5.ffn-reduce_sum_like-380/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-scalar_mul-389/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-390([model.blocks.5.ffn-scalar_mul-389/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-reduce_sum_like-376/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-390/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square_grad-362-dx_0-cast_f2h(model.blocks.5.ffn-square_grad-362/dx_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square_grad-362-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2-299-y_0-cast_f2h(model.blocks.5.ffn-sigmoid_v2-299/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2-299-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k-out-cast_f2h(model.blocks.5.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square-294-y_0-cast_f2h(model.blocks.5.ffn-square-294/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square-294-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2_grad-357-dx_0-cast_f2h(model.blocks.5.ffn-sigmoid_v2_grad-357/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2_grad-357-dx_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r-out-cast_f2h(model.blocks.5.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-288-out_0-cast_f2h(model.blocks.5.ffn-scalar_add-288/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-288-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-283-out_0-cast_f2h(model.blocks.5.ffn-scalar_add-283/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-283-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-relu-293-y_0-cast_h2f(model.blocks.5.ffn-relu-293/y_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-relu-293-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-390_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn-add_n-390/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-add_n-390_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-373_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn-add_n-373/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-add_n-373_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k_optimizer(model.blocks.5.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn-add_n-390_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r_optimizer(model.blocks.5.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn-add_n-373_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.5-add_n-301([model.blocks.5.att.output-broadcast_matmul-277/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-300/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5-add_n-301/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.5-add_n-301-out_0-cast_h2f(model.blocks.5-add_n-301/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5-add_n-301-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.5-add_n-278-out_0-cast_h2f(model.blocks.5.att.output-broadcast_matmul-277/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5-add_n-278-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.5_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (OUTPUT:_model.blocks_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<add_n_backward>))
    )
    (MODULE:model.ln_out:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
      (INPUT:_model.ln_out_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<add_n_backward>))
      (PARAMETER:model.ln_out.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (PARAMETER:model.ln_out.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.ln_out.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm-302(model.blocks.5-add_n-301/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.ln_out.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.ln_out.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.ln_out-layer_norm-302/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-302/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-302/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm_param_grad-353(model.head_k-add_n-350-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5-add_n-301-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-302/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-302/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.ln_out-layer_norm_param_grad-353/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.ln_out-layer_norm_param_grad-353/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm_grad-354(model.head_k-broadcast_matmul-348/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5-add_n-301/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-302/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-302/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.ln_out-layer_norm_grad-354/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias-out-cast_f2h(model.ln_out.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.ln_out.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight-out-cast_f2h(model.ln_out.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.ln_out.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm_grad-354-dx_0-cast_h2f(model.ln_out-layer_norm_grad-354/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.ln_out-layer_norm_grad-354-dx_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight_optimizer(model.ln_out.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out-layer_norm_param_grad-353/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.ln_out.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias_optimizer(model.ln_out.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out-layer_norm_param_grad-353/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.ln_out.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.ln_out_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
    )
    (MODULE:model.head:Linear1D(in_features=1024, out_features=6064, bias=False, parallel=row)): (
      (INPUT:_model.head_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
      (PARAMETER:model.head.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.head.weight() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul-314(model.ln_out-layer_norm-302/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.head.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul-314/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul-338(model-reshape-337/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16)), model.head.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul-338/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul_grad_b-339(model-reshape-337/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-302/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul_grad_b-339/out_0:(sbp=(P), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight-out-cast_f2h(model.head.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))) -> (model.head.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul_grad_b-339_out_0_pinned_identity-out_0-cast_h2f(model.head-broadcast_matmul_grad_b-339/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul_grad_b-339_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight-m() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight-v() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight_optimizer(model.head.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.head-broadcast_matmul_grad_b-339_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.head.weight-m/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.head.weight-v/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.head_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 6064),
             dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
    )
    (MODULE:model.head_q:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)): (
      (INPUT:_model.head_q_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
      (PARAMETER:model.head_q.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.head_q.weight() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul-304(model.ln_out-layer_norm-302/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.head_q.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul-304/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul-344(model-batch_matmul-342/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.head_q.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul-344/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul_grad_b-345(model-batch_matmul-342/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-302/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul_grad_b-345/out_0:(sbp=(P), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight-out-cast_f2h(model.head_q.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))) -> (model.head_q.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul_grad_b-345_out_0_pinned_identity-out_0-cast_h2f(model.head_q-broadcast_matmul_grad_b-345/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul_grad_b-345_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight-m() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight-v() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight_optimizer(model.head_q.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_q-broadcast_matmul_grad_b-345_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.head_q.weight-m/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_q.weight-v/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.head_q_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 256),
             dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
    )
    (MODULE:model.head_k:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)): (
      (INPUT:_model.head_k_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
      (PARAMETER:model.head_k.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.head_k.weight() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul-306(model.ln_out-layer_norm-302/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.head_k.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul-306/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul-348(model-transpose-346/output_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.head_k.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul-348/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul_grad_b-349(model-transpose-346/output_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-302/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul_grad_b-349/out_0:(sbp=(P), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight-out-cast_f2h(model.head_k.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))) -> (model.head_k.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-add_n-350-out_0-cast_h2f(model.head_k-broadcast_matmul-348/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-add_n-350-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul_grad_b-349_out_0_pinned_identity-out_0-cast_h2f(model.head_k-broadcast_matmul_grad_b-349/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul_grad_b-349_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight-m() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight-v() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight_optimizer(model.head_k.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_k-broadcast_matmul_grad_b-349_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.head_k.weight-m/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_k.weight-v/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.head_k_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 256),
             dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
    )
    (OPERATOR: model.emb.weight() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-307(model.head_k-broadcast_matmul-306/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16))) -> (model-transpose-307/output_0:(sbp=(S(0)), size=(8, 256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-308(model.head_q-broadcast_matmul-304/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model-transpose-307/output_0:(sbp=(S(0)), size=(8, 256, 1024), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-308/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-scalar_mul-309(model-batch_matmul-308/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model-scalar_mul-309/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-one_hot-310(System-Boxing-Identity-222/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64))) -> (model-one_hot-310/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.int64))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-cast-311(model-one_hot-310/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.int64))) -> (model-cast-311/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-312(model-scalar_mul-309/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model-cast-311-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-312/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-316(model.head-broadcast_matmul-314/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-316/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-317(_GraphBase_0_input.1.1_targets/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64))) -> (model-reshape-317/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.int64))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-318(model-reshape-316/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-transpose-318/output_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-319(model-transpose-318/output_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-319/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-log_softmax-320(model-reshape-319/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-log_softmax-320/prob_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-nll-321(model-log_softmax-320-prob_0-cast_h2f/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.float32)), model-reshape-317/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.int64))) -> (model-nll-321/output_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32)), model-nll-321/out_weight_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-322(model-nll-321/output_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))) -> (model-reshape-322/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reduce_sum-323(model-reshape-322/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))) -> (model-reduce_sum-323/output_tensor_0:(sbp=(P), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reduce_sum-324(model-nll-321/out_weight_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))) -> (model-reduce_sum-324/output_tensor_0:(sbp=(P), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_div-325(model-reduce_sum-323/output_tensor_0:(sbp=(P), size=(), dtype=(oneflow.float32)), System-Boxing-Identity-223/out:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (model-broadcast_div-325/z_0:(sbp=(P), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_div-328(ones_like-327/out_0:(sbp=(B), size=(), dtype=(oneflow.float32)), System-Boxing-Identity-223/out:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (model-broadcast_div-328/z_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_div_grad-329(ones_like-327/out_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-broadcast_div-325/z_0:(sbp=(P), size=(), dtype=(oneflow.float32)), System-Boxing-Identity-223/out:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (model-broadcast_div_grad-329/dy_0:(sbp=(P), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_like-330(model-broadcast_div-328/z_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-reshape-322/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))) -> (model-broadcast_like-330/y_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_like-331(model-broadcast_div_grad-329/dy_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-nll-321/out_weight_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))) -> (model-broadcast_like-331/y_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-332(model-broadcast_like-330/y_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))) -> (model-reshape-332/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-nll_grad-333(model-reshape-332/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.float32)), model-log_softmax-320-prob_0-cast_h2f/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.float32)), model-reshape-317/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.int64))) -> (model-nll_grad-333/in_grad_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-log_softmax_grad-334(model-log_softmax-320/prob_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16)), model-nll_grad-333-in_grad_0-cast_f2h/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-log_softmax_grad-334/dx_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-335(model-log_softmax_grad-334/dx_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-335/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-336(model-reshape-335/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-transpose-336/output_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-337(model-transpose-336/output_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-337/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-340(model-reshape-337/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16)), model-cast-311-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-340/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-scalar_mul-341(model-batch_matmul-340/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model-scalar_mul-341/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-342(model-scalar_mul-341/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model-transpose-307/output_0:(sbp=(S(0)), size=(8, 256, 1024), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-342/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-343(model.head_q-broadcast_matmul-304/out_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model-scalar_mul-341/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-343/out_0:(sbp=(S(0)), size=(8, 256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-346(model-batch_matmul-343/out_0:(sbp=(S(0)), size=(8, 256, 1024), dtype=(oneflow.bfloat16))) -> (model-transpose-346/output_0:(sbp=(S(0)), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-nll_grad-333-in_grad_0-cast_f2h(model-nll_grad-333/in_grad_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.float32))) -> (model-nll_grad-333-in_grad_0-cast_f2h/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weight-out-cast_f2h(model.emb.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))) -> (model.emb.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-cast-311-out_0-cast_f2h(model-cast-311/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.float32))) -> (model-cast-311-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-log_softmax-320-prob_0-cast_h2f(model-log_softmax-320/prob_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-log_softmax-320-prob_0-cast_h2f/out_0:(sbp=(S(0)), size=(8192, 6064), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weight-m() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weight-v() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weightadam_bias_correction_factor1(System-Train-TrainStep-Identity/out:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (model.emb.weightadam_bias_correction_factor1/out_0:(sbp=(B), size=(1), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cpu", ranks=[0])))
    (OPERATOR: model.emb.weightadam_bias_correction_factor2(System-Train-TrainStep-Identity/out:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (model.emb.weightadam_bias_correction_factor2/out_0:(sbp=(B), size=(1), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cpu", ranks=[0])))
    (OPERATOR: model.emb.weight_optimizer(model.emb.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.emb-unsorted_segment_sum_like-695_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-221/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.emb.weight-m/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.emb.weight-v/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-225/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-224/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OUTPUT:_model_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
           sbp=(oneflow.sbp.partial_sum,), is_lazy='True', size=(),
           dtype=oneflow.float32, grad_fn=<broadcast_div_backward>))
  )
  (OPERATOR: _GraphBase_0_input.1.0_idx() -> (_GraphBase_0_input.1.0_idx/out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0, 1])))
  (OPERATOR: _GraphBase_0_input.1.1_targets() -> (_GraphBase_0_input.1.1_targets/out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0, 1])))
  (OPERATOR: scalar_add-326(model-broadcast_div-325/z_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (scalar_add-326/out_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: ones_like-327(scalar_add-326/out_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (ones_like-327/out_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: hierarchical_parallel_cast-696(model-broadcast_div-325/z_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (hierarchical_parallel_cast-696/out_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: _GraphBase_0_output.0.0.0_loss(hierarchical_parallel_cast-696/out_0:sbp=(B), size=(), dtype=(oneflow.float32)) -> (_GraphBase_0_output.0.0.0_loss/out:sbp=(B), size=(), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep() -> (out:sbp=(B), size=(1), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep-Identity(System-Train-TrainStep/out:sbp=(B), size=(1), dtype=(oneflow.int64)) -> (out:sbp=(B), size=(1), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep-ScalarAdd(System-Train-TrainStep-Identity/out:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (System-Train-TrainStep-ScalarAdd/out_0:(sbp=(B), size=(1), dtype=(oneflow.int64))), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep-Assign(System-Train-TrainStep/out:(sbp=(B), size=(1), dtype=(oneflow.int64)), System-Train-TrainStep-ScalarAdd/out_0:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Boxing-Identity-221(System-Train-LearningRate-Scheduler_60/out:sbp=(B), size=(1), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(1), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-222(_GraphBase_0_input.1.0_idx/out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)) -> (out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-223(model-reduce_sum-324/output_tensor_0:sbp=(B), size=(), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-224(model.emb.weightadam_bias_correction_factor2/out_0:sbp=(B), size=(1), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(1), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-225(model.emb.weightadam_bias_correction_factor1/out_0:sbp=(B), size=(1), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(1), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OUTPUT:_GraphBase_0_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0]),
         sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(),
         dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
)
[32m[08/24 03:16:20 lb.utils.events]: [0m eta: 0:11:25  iteration: 19/10000  consumed_samples: 160  total_loss: 10.07  time: 0.0688 s/iter  data_time: 0.0023 s/iter total_throughput: 116.27 samples/s lr: 8.00e-04  
[32m[08/24 03:16:22 lb.utils.events]: [0m eta: 0:11:25  iteration: 39/10000  consumed_samples: 320  total_loss: 5.932  time: 0.0689 s/iter  data_time: 0.0022 s/iter total_throughput: 116.19 samples/s lr: 8.00e-04  
[32m[08/24 03:16:23 lb.utils.events]: [0m eta: 0:11:25  iteration: 59/10000  consumed_samples: 480  total_loss: 4.112  time: 0.0689 s/iter  data_time: 0.0027 s/iter total_throughput: 116.17 samples/s lr: 8.00e-04  
[32m[08/24 03:16:24 lb.utils.events]: [0m eta: 0:11:24  iteration: 79/10000  consumed_samples: 640  total_loss: 3.408  time: 0.0689 s/iter  data_time: 0.0029 s/iter total_throughput: 116.08 samples/s lr: 8.00e-04  
[32m[08/24 03:16:26 lb.utils.events]: [0m eta: 0:11:23  iteration: 99/10000  consumed_samples: 800  total_loss: 2.892  time: 0.0689 s/iter  data_time: 0.0025 s/iter total_throughput: 116.04 samples/s lr: 8.00e-04  
[32m[08/24 03:16:27 lb.utils.events]: [0m eta: 0:11:22  iteration: 119/10000  consumed_samples: 960  total_loss: 2.69  time: 0.0690 s/iter  data_time: 0.0027 s/iter total_throughput: 116.02 samples/s lr: 8.00e-04  
[32m[08/24 03:16:29 lb.utils.events]: [0m eta: 0:11:20  iteration: 139/10000  consumed_samples: 1120  total_loss: 2.497  time: 0.0690 s/iter  data_time: 0.0024 s/iter total_throughput: 116.01 samples/s lr: 8.00e-04  
[32m[08/24 03:16:30 lb.utils.events]: [0m eta: 0:11:19  iteration: 159/10000  consumed_samples: 1280  total_loss: 2.252  time: 0.0690 s/iter  data_time: 0.0026 s/iter total_throughput: 115.94 samples/s lr: 8.00e-04  
[32m[08/24 03:16:31 lb.utils.events]: [0m eta: 0:11:18  iteration: 179/10000  consumed_samples: 1440  total_loss: 1.947  time: 0.0690 s/iter  data_time: 0.0027 s/iter total_throughput: 115.87 samples/s lr: 8.00e-04  
[32m[08/24 03:16:33 lb.utils.events]: [0m eta: 0:11:17  iteration: 199/10000  consumed_samples: 1600  total_loss: 1.678  time: 0.0691 s/iter  data_time: 0.0026 s/iter total_throughput: 115.81 samples/s lr: 8.00e-04  
[32m[08/24 03:16:34 lb.utils.events]: [0m eta: 0:11:16  iteration: 219/10000  consumed_samples: 1760  total_loss: 1.106  time: 0.0691 s/iter  data_time: 0.0025 s/iter total_throughput: 115.75 samples/s lr: 8.00e-04  
[32m[08/24 03:16:36 lb.utils.events]: [0m eta: 0:11:15  iteration: 239/10000  consumed_samples: 1920  total_loss: 0.6764  time: 0.0691 s/iter  data_time: 0.0025 s/iter total_throughput: 115.75 samples/s lr: 8.00e-04  
[32m[08/24 03:16:37 lb.utils.events]: [0m eta: 0:11:14  iteration: 259/10000  consumed_samples: 2080  total_loss: 0.4143  time: 0.0691 s/iter  data_time: 0.0022 s/iter total_throughput: 115.76 samples/s lr: 8.00e-04  
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1458836
Killing subprocess 1458837
Main process received SIGINT, exiting
F20220824 03:16:37.804177 1460224 cuda_util.cpp:141] Check failed: cudaGetDevice(&saved_dev_id_) : driver shutting down (4) 
*** Check failure stack trace: ***
    @     0x7fbeebc9c9cc  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fbf1641af70  oneflow::CudaCurrentDeviceGuard::CudaCurrentDeviceGuard()
    @     0x7fbf18a43830  oneflow::boxing::collective::NcclExecutorBackend::ExecuteGroup()
    @     0x7fbf18a50c42  oneflow::boxing::collective::StaticGroupCoordinator::AddRequest()
    @     0x7fbf18a4e6c5  oneflow::boxing::collective::Scheduler::Schedule()
    @     0x7fbf18cfa95a  oneflow::CollectiveBoxingActorContext::Schedule()
    @     0x7fbf18cae906  oneflow::(anonymous namespace)::CollectiveBoxingGenericKernel::ForwardDataContent()
    @     0x7fbf18cc155a  oneflow::Kernel::Forward()
    @     0x7fbf18cc14e2  oneflow::Kernel::Launch()
    @     0x7fbf18d3f37a  oneflow::(anonymous namespace)::LightActor<>::ProcessMsg()
    @     0x7fbf19a5ac12  oneflow::Thread::PollMsgChannel()
    @     0x7fbf19a5b9c5  std::thread::_State_impl<>::_M_run()
    @     0x7fbeeb115de4  (unknown)
    @     0x7fbf6face609  start_thread
    @     0x7fbf6f9f3133  clone

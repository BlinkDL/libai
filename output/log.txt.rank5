[03/09 02:34:53] libai INFO: Rank of current process: 5. World size: 8
[03/09 02:34:53] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 02:34:53] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mimagenet[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/path/to/imagenet[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/path/to/imagenet[39m[38;5;186m"[39m

[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/dataset/extract[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/dataset/extract[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 02:34:53] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 02:34:53] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 02:38:52] libai INFO: Rank of current process: 5. World size: 8
[03/09 02:38:52] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 02:38:52] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mimagenet[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/path/to/imagenet[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/path/to/imagenet[39m[38;5;186m"[39m

[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/dataset/extract[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/dataset/extract[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 02:38:52] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 02:38:52] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 02:40:40] libai.engine.default INFO: Prepare testing set
[03/09 02:40:45] libai.engine.default INFO: Auto-scaling the config to train.train_iter=3002736, train.warmup_iter=200183
[03/09 02:40:52] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 02:40:52] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 02:55:58] libai INFO: Rank of current process: 5. World size: 8
[03/09 02:55:58] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 02:55:58] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 02:55:58] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 02:55:58] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:00:20] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:00:20] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:00:20] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:00:20] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:00:20] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:00:21] libai.engine.default INFO: Prepare testing set
[03/09 03:00:22] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:00:30] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:00:30] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 03:07:58] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:07:58] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:07:58] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:07:58] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:07:58] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:07:59] libai.engine.default INFO: Prepare testing set
[03/09 03:08:00] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:08:07] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:08:07] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 03:10:15] libai.evaluation.evaluator INFO: Start inference on 10000 samples
[03/09 03:10:33] libai.engine.trainer ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/zhangxiaoyu/libai/libai/engine/trainer.py", line 143, in train
    self.after_step()
  File "/home/zhangxiaoyu/libai/libai/engine/trainer.py", line 171, in after_step
    h.after_step()
  File "/home/zhangxiaoyu/libai/libai/engine/hooks.py", line 332, in after_step
    self._do_eval()
  File "/home/zhangxiaoyu/libai/libai/engine/hooks.py", line 304, in _do_eval
    results = self._func()
  File "/home/zhangxiaoyu/libai/libai/engine/default.py", line 339, in test_and_save_results
    self._last_eval_results = self.test(self.cfg, self.test_loader, model)
  File "/home/zhangxiaoyu/libai/libai/engine/default.py", line 658, in test
    model, data_loader, test_batch_size, cls.get_batch, evaluator
  File "/home/zhangxiaoyu/libai/libai/evaluation/evaluator.py", line 187, in inference_on_dataset
    dist.synchronize()
  File "/home/zhangxiaoyu/libai/libai/utils/distributed.py", line 383, in synchronize
    flow._oneflow_internal.eager.multi_client.Sync()
AttributeError: module 'oneflow._oneflow_internal.eager' has no attribute 'multi_client'
[03/09 03:10:33] libai.engine.hooks INFO: Overall training speed: 997 iterations in 0:02:07 (0.1274 s / it)
[03/09 03:10:33] libai.engine.hooks INFO: Total training time: 0:02:24 (0:00:17 on hooks)
[03/09 03:15:00] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:15:00] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:15:01] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:15:01] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:15:01] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:15:02] libai.engine.default INFO: Prepare testing set
[03/09 03:15:02] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:15:09] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:15:10] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 03:17:12] libai.evaluation.evaluator INFO: Start inference on 10000 samples
[03/09 03:17:29] libai.evaluation.evaluator INFO: Inference done 1024/10000. Dataloading: 6.5947 s/iter. Inference: 10.9292 s/iter. Eval: 0.0448 s/iter. Total: 17.5762 s/iter. ETA=0:02:20
[03/09 03:17:30] libai.evaluation.evaluator INFO: Total valid samples: 10000
[03/09 03:17:30] libai.evaluation.evaluator INFO: Total inference time: 0:00:00.659761 (0.000066 s / iter per device, on 8 devices)
[03/09 03:17:30] libai.evaluation.evaluator INFO: Total inference pure compute time: 0:00:00 (0.000051 s / iter per device, on 8 devices)
[03/09 03:17:30] libai.engine.hooks WARNING: Given val metric Acc@1 does not seem to be computed/stored.Will not be checkpointed based on it.
[03/09 03:21:59] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:21:59] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:21:59] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:21:59] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:21:59] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:22:00] libai.engine.default INFO: Prepare testing set
[03/09 03:22:01] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:22:08] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:22:08] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 03:23:06] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:23:06] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:23:06] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:23:06] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:23:06] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:23:07] libai.engine.default INFO: Prepare testing set
[03/09 03:23:08] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:23:15] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:23:15] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 03:23:46] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:23:46] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:23:46] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:23:46] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:23:46] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:23:47] libai.engine.default INFO: Prepare testing set
[03/09 03:23:48] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:23:55] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:23:55] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 03:26:02] libai.evaluation.evaluator INFO: Start inference on 10000 samples
[03/09 03:26:19] libai.evaluation.evaluator INFO: Inference done 1024/10000. Dataloading: 6.3527 s/iter. Inference: 10.9385 s/iter. Eval: 0.0079 s/iter. Total: 17.3076 s/iter. ETA=0:02:18
[03/09 03:26:20] libai.evaluation.evaluator INFO: Total valid samples: 10000
[03/09 03:26:20] libai.evaluation.evaluator INFO: Total inference time: 0:00:00.643277 (0.000064 s / iter per device, on 8 devices)
[03/09 03:26:20] libai.evaluation.evaluator INFO: Total inference pure compute time: 0:00:00 (0.000049 s / iter per device, on 8 devices)
[03/09 03:26:20] libai.engine.hooks WARNING: Given val metric Acc@1 does not seem to be computed/stored.Will not be checkpointed based on it.
[03/09 03:36:51] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:36:51] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:36:51] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:36:51] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:36:51] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:36:52] libai.engine.default INFO: Prepare testing set
[03/09 03:36:53] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:37:00] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:37:00] libai.engine.trainer INFO: Starting training from iteration 0
[03/09 03:38:17] libai INFO: Rank of current process: 5. World size: 8
[03/09 03:38:17] libai INFO: Command line arguments: Namespace(config_file='configs/vit_imagenet.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[03/09 03:38:18] libai INFO: Contents of args.config_file=configs/vit_imagenet.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mvit[39m[38;5;15m.[39m[38;5;15mvit_tiny_patch16_224[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mgraph[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mgraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mcifar100[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mMixup[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mflowvision[39m[38;5;15m.[39m[38;5;15mloss[39m[38;5;15m.[39m[38;5;15mcross_entropy[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mSoftTargetCrossEntropy[39m

[38;5;242m# Refine data path to imagenet[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtest[39m[38;5;15m[[39m[38;5;141m0[39m[38;5;15m][39m[38;5;197m.[39m[38;5;15mdataset[39m[38;5;197m.[39m[38;5;15mroot[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m./[39m[38;5;186m"[39m

[38;5;242m# Add Mixup Func[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmixup_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mMixup[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mmixup_alpha[39m[38;5;197m=[39m[38;5;141m0.8[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mcutmix_alpha[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mprob[39m[38;5;197m=[39m[38;5;141m1.0[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mswitch_prob[39m[38;5;197m=[39m[38;5;141m0.5[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmode[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186mbatch[39m[38;5;186m"[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_classes[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# Refine model cfg for vit training on imagenet[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mnum_classes[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mloss_func[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mSoftTargetCrossEntropy[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m

[38;5;242m# Refine optimizer cfg for vit model[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5e-4[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15meps[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1e-8[39m
[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mweight_decay[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.05[39m

[38;5;242m# Refine train cfg for vit model[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtest_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mwarmup_ratio[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m20[39m[38;5;15m [39m[38;5;197m/[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15meval_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;242m# Scheduler[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_factor[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.001[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15malpha[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_method[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mlinear[39m[38;5;186m"[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[03/09 03:38:18] libai.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[03/09 03:38:18] libai.engine.default INFO: Prepare training, validating, testing set
[03/09 03:38:19] libai.engine.default INFO: Prepare testing set
[03/09 03:38:20] libai.engine.default INFO: Auto-scaling the config to train.train_iter=117188, train.warmup_iter=7813
[03/09 03:38:26] libai.engine.default INFO: Model:
VisionTransformer(
  (patch_embed): PatchEmbedding(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): TransformerLayer(
      (drop_path): Identity()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (1): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (2): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (3): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (4): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (5): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (6): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (7): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (8): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (9): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (10): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (11): TransformerLayer(
      (drop_path): DropPath()
      (input_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (self_attention): MultiheadAttention(
        hidden_size=192, num_heads=3, is_cross_attention=False
        (dropout): Dropout(p=0.0, inplace=False)
        (output_dropout): Dropout(p=0.0, inplace=False)
        (query_key_value): Linear1D(in_features=192, out_features=576, bias=True, parallel=col)
        (dense): Linear1D(in_features=192, out_features=192, bias=True, parallel=row)
      )
      (post_attention_layernorm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        bias_gelu_fusion=False, bias_dropout_fusion=False, dropout=0.0
        (dense_h_to_4h): Linear1D(in_features=192, out_features=768, bias=True, parallel=col)
        (activation_func): GELU()
        (dense_4h_to_h): Linear1D(in_features=768, out_features=192, bias=True, parallel=row)
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=192, out_features=1000, bias=True, parallel=data)
  (loss_func): SoftTargetCrossEntropy()
)
[03/09 03:38:27] libai.engine.trainer INFO: Starting training from iteration 0
